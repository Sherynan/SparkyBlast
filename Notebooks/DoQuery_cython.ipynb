{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations related to Cassandra connector & Cluster\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.1 --conf spark.cassandra.connection.host=192.168.1.4 pyspark-shell '\n",
    "\n",
    "# Creating PySpark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"Blast DoQuery\")\n",
    "\n",
    "# Creating PySpark SQL Context\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1a: Calculate Query's keys & desplazaments (with header line)\n",
      "Test 2: Calculate Query's keys & desplazaments (with header line)\n",
      "Test 1a: Calculate Multipe Querys\n",
      "MultipleQuery(../Datasets/References/MulQuery1.txt, example2_r100, 11).\n",
      "[1566556349.5]----> MultipleQuery ([<SparkContext master=local[*] appName=Blast DoQuery>, <pyspark.sql.context.SQLContext object at 0x274c790>, '../Datasets/References/MulQuery1.txt', 'example2_r100', 11, None]).\n",
      "++++++++++++ INITIAL STATISTICS 08/23/2019 12:32:29 +++++++++++++\n",
      "+ Reference: example2_r100  \tQuery file: ../Datasets/References/MulQuery1.txt.\n",
      "+ Key Size: 11   \tMethod: 1.\n",
      "+ Num Executors: None  \tExecutors/cores: None  \tExecutor Mem: None.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[1566556349.68]----> CalculateMultipleQueryOffset ([<SparkContext master=local[*] appName=Blast DoQuery>, <pyspark.sql.context.SQLContext object at 0x274c790>, '../Datasets/References/MulQuery1.txt', 11]).\n",
      "######################### Query::Input file has 4 sequences and 1 partitions: \n",
      "######################### Query::Input file has 4 sequences and 1 partitions: \n",
      "Query::First 10 records: [Row(_c0=u'JKLMNOPQRSTUVWXYZ', _c1=1, _c2=17)]\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      "\n",
      "######################### Total cores: 1  Max Partition: 3  NQuerys: 4  Calc Partitions: 3  Rdd Partitions: 1\n",
      "######################### Total cores: 1  Max Partition: 3  NQuerys: 4  Calc Partitions: 3  NEW Rdd Partitions: 3\n",
      "######################### Time required for calculate keys in 0.01 seconds.\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 25, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-ec161a38c610>\", line 1108, in cython_calculateMultipleQueryKeysDespl\nImportError: No module named pyx\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-ec161a38c610>\", line 1108, in cython_calculateMultipleQueryKeysDespl\nImportError: No module named pyx\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fc05d3d6e062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mqueryFilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Datasets/References/MulQuery1.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mReferenceName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"example2_r100\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mMultipleQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueryFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReferenceName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ec161a38c610>\u001b[0m in \u001b[0;36mMultipleQuery\u001b[0;34m(sc, sqlContext, queryFilename, referenceName, keySize, hashName)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mkeysdespl_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mkeysdespl_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculateMultipleQueryOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueryFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeySize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mquery_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mkeysdespl_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ec161a38c610>\u001b[0m in \u001b[0;36mCalculateMultipleQueryOffset\u001b[0;34m(sc, sqlContext, queryFilename, keySize)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"######################### Time required for calculate keys in {} seconds.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgt0_bc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"######################### Total time required for processing {} keys using {} partitions in {} seconds.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_keys_despl_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_keys_despl_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"######################### Query data size: {} MBytes.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryFilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 25, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-ec161a38c610>\", line 1108, in cython_calculateMultipleQueryKeysDespl\nImportError: No module named pyx\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-ec161a38c610>\", line 1108, in cython_calculateMultipleQueryKeysDespl\nImportError: No module named pyx\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n"
     ]
    }
   ],
   "source": [
    "DDoTesting = True\n",
    "DDebug = True\n",
    "\n",
    "DCythonLibsPath = 'hdfs://babel.udl.cat/user/nando/cython_libs/'\n",
    "\n",
    "if (DDoTesting):    \n",
    "    \n",
    "    sc.addPyFile(DCythonLibsPath+'do_query.pyx')\n",
    "    \n",
    "    # Test 1: Calculate Query's keys & desplazaments (with header line)\n",
    "    print(\"Test 1a: Calculate Query's keys & desplazaments (with header line)\")\n",
    "    queryFilename = '../Datasets/References/Query3.txt'\n",
    "    ReferenceName = \"example2_r100\"\n",
    "    Method = ECreate2LinesData\n",
    "    #main(sc, sqlContext, queryFilename, ReferenceName)  \n",
    "    \n",
    "    \n",
    "    # Test 2: Calculate Query's keys & desplazaments (with header line)\n",
    "    print(\"Test 2: Calculate Query's keys & desplazaments (with header line)\")\n",
    "    queryFilename = '../Datasets/References/Query_GRCh38.txt'\n",
    "    ReferenceName = \"grch38_1m\"\n",
    "    Method = ECreate2LinesData\n",
    "    #main(sc, sqlContext, queryFilename, ReferenceName.lower())  \n",
    "    \n",
    "\n",
    "    # Test 3: Calculate Multiple Querys \n",
    "    print(\"Test 1a: Calculate Multipe Querys\")\n",
    "    queryFilename = '../Datasets/References/MulQuery1.txt'\n",
    "    ReferenceName = \"example2_r100\"\n",
    "    MultipleQuery(sc, sqlContext, queryFilename, ReferenceName)  \n",
    "\n",
    "    \n",
    "    # Test 4: Calculate Multiple Querys \n",
    "    print(\"Test 1a: Calculate Multipe Querys\")\n",
    "    queryFilename = '../Datasets/References/MulQuery1.txt'\n",
    "    referenceName = \"example2_r100\"\n",
    "    #ultipleQuery(sc, sqlContext, queryFilename, referenceName)  \n",
    "\n",
    "    sc.addPyFile(DCythonLibsPath+'do_query.pyx')\n",
    "    cyt_calculateMultipleQueryKeysDesplR = spark_cython('do_query', 'cython_calculateMultipleQueryKeysDesplR')\n",
    "    print(cyt_calculateMultipleQueryKeysDesplR(\"12345678901234567890\",len(\"12345678901234567890\"), 11))\n",
    "   \n",
    "    error\n",
    "\n",
    "## End Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1a: Calculate Query's keys & desplazaments (with header line)\n",
      "Test 2: Calculate Query's keys & desplazaments (with header line)\n",
      "Test 1a: Calculate Multipe Querys\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ec161a38c610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;31m#MultipleQuery(sc, sqlContext, queryFilename, referenceName)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m     \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[0;31m## End Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "# SparkBlast_DoQuery program to perform blast querys on cassandra using spark\n",
    "# Usage: SparkBlast_DoQuery <Query_Files> <ReferenceName> [Key_size=11].\n",
    "\n",
    "from __future__ import print_function\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import Row, _infer_schema, _has_nulltype, _merge_type, _create_converter\n",
    "from pyspark.sql.types import StringType, ArrayType, LongType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import udf, upper, desc, collect_list, size\n",
    "from cassandra.cluster import Cluster\n",
    "from operator import add\n",
    "import re\n",
    "from time import time\n",
    "import os, shutil, sys, subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "import random\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import cython\n",
    "\n",
    "\n",
    "\n",
    "## Constants\n",
    "DCassandraNodes = ['192.168.1.1', '192.168.1.2', '192.168.1.3', '192.168.1.4', '192.168.1.5', '192.168.1.6']\n",
    "#DCassandraNodes = ['192.168.1.3']\n",
    "DDoTesting = True\n",
    "DDebug = False\n",
    "DTiming = True\n",
    "DShowResult = True\n",
    "APP_NAME = \"SparkBlast_DoQuery\"\n",
    "DKeySize = 11\n",
    "DQueryFilename = '../Datasets/References/Query1.txt'\n",
    "DReferenceName = \"example\"\n",
    "DReferenceHashTableName = \"hash\"\n",
    "DReferenceContentTableName = \"sequences\"\n",
    "DCreateWindowWithPartitions = True\n",
    "DCreateBlocksDataFrame = True\n",
    "DPartitionBlockSize = 128  * 1024 \n",
    "DProcessingByPartitions = False\n",
    "DNumberPartitions = 1\n",
    "DMaxNumberStages = 3\n",
    "DDoMultipleQuery = False\n",
    "DMinAligmentScore = 0.7\n",
    "DBlockCacheSize = 3\n",
    "DCythonLibsPath = 'hdfs://babel.udl.cat/user/nando/cython_libs/'\n",
    "##DCythonLibsPath = '/tmp/cython_libs/'\n",
    "##cyt_calculateMultipleQueryKeysDesplR = None\n",
    "\n",
    "# Statistics \n",
    "DHdfsHomePath = \"hdfs://babel.udl.cat/user/nando/\"\n",
    "DHdfsTmpPath = DHdfsHomePath + \"Tmp/\"\n",
    "DHdfsOutputPath = DHdfsHomePath + \"Output/\"\n",
    "\n",
    "\n",
    "## Types\n",
    "# Enum Methods:\n",
    "ECreate2LinesData = 1\n",
    "ECreate1LineDataWithoutDependencies = 2\n",
    "ECreateBlocksData = 3\n",
    "DDefaultMethod = ECreate2LinesData\n",
    "    \n",
    "\n",
    "## Global Variables\n",
    "Method = DDefaultMethod\n",
    "CreateWindowWithPartitions = DCreateWindowWithPartitions\n",
    "BlockSize = DPartitionBlockSize\n",
    "DoMultipleQuery = DDoMultipleQuery\n",
    "NumberPartitions = DNumberPartitions\n",
    "DKeyMatchingThreshold = 5\n",
    "StatisticsFileName = None\n",
    "YarnJobId = None\n",
    "key_size_bc = 0\n",
    "query_length_bc = 0\n",
    "gt0_bc = time()\n",
    "reference_name_bc = 0\n",
    "query_sequence_bc = 0\n",
    "content_block_size_bc = 0\n",
    "keysdespl_rdd = 0\n",
    "reference_keys = 0\n",
    "\n",
    "\n",
    "\n",
    "## Functions ##\n",
    "\n",
    "def Query(sc, sqlContext, queryFilename, referenceName, keySize=DKeySize, hashName=None):\n",
    "    dfc(\"Query\", sc, sqlContext, queryFilename, referenceName, keySize, hashName)\n",
    "    \n",
    "    # Broadcast Global variables\n",
    "    global DDebug, key_size_bc, gt0_bc, reference_name_bc, hash_name_bc\n",
    "    t0 = time()\n",
    "    gt0_bc = sc.broadcast(t0)\n",
    "    key_size_bc = sc.broadcast(keySize)\n",
    "    reference_name_bc = sc.broadcast(referenceName)\n",
    "    hash_name_bc = sc.broadcast(hashName)\n",
    "    \n",
    "    if (DTiming):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"++++++++++++ INITIAL STATISTICS {} +++++++++++++\".format(date_time))\n",
    "        print(\"+ Reference: {}  \\tQuery file: {}.\".format(ReferenceName, queryFilename))\n",
    "        print(\"+ Key Size: {}   \\tMethod: {}.\".format(KeySize, Method))\n",
    "        print(\"+ Num Executors: {}  \\tExecutors/cores: {}  \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"+ Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Create Cassandra Sesion\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    session = cluster.connect()\n",
    " \n",
    "    # Steps:\n",
    "    #   1. Read and split query in keySize-Segments + Desplazament\n",
    "    #   2. Query Cassandra for the keys\n",
    "    #   3. Calculate Top-matching zones\n",
    "    #   4. Make extension in Top-matching zones.\n",
    "    \n",
    "\n",
    "    # 1. Read and split query in keySize-Segments + Desplazament\n",
    "    global keysdespl_rdd, reference_keys\n",
    "    t1 = time()\n",
    "    keysdespl_rdd, query_sequence = CalculateQueryOffset(sc, queryFilename, keySize)\n",
    "    t_read = time()-t1\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Query::QueryKeys:\")\n",
    "        keysdespl_rdd.persist()\n",
    "        print(keysdespl_rdd.take(1))\n",
    "        \n",
    "    # 2. Query Cassandra for the keys\n",
    "    t2 = time()\n",
    "    if (DProcessingByPartitions):\n",
    "        reference_keys = GetKeysOffsetsInReferenceByPartition(sc, session, referenceName, keysdespl_rdd)\n",
    "    else:\n",
    "        reference_keys = GetKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd)\n",
    "    t_qcass = time()-t2\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"Query::QueryKeys: Matching desplazaments: {}\".format(reference_keys.take(1)))\n",
    "        \n",
    "          \n",
    "    # 3. Calculate Top-matching offsets\n",
    "    t3 = time()\n",
    "    offsets_count = reference_keys.map(lambda off: (off,1)).reduceByKey(add)\n",
    "    top_offsets = offsets_count.sortBy(lambda kv: kv[1], False)\n",
    "    # Get Top-matching zones that exceed the threshold.\n",
    "    top_matching = offsets_count.filter(lambda kv: kv[1]>DKeyMatchingThreshold) \n",
    "    top_matchng.persist()\n",
    "    candidate_offsets = top_matching.count()\n",
    "    t_topmat = time()-t3\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Top-10 offsets: {}\".format(top_offsets.take(1)))\n",
    "        print(\"Top-Matching offsets: {}\".format(top_matching.take(1)))  \n",
    "      \n",
    "    # 4. Make extension in Top-matching zones.\n",
    "    t4 = time()\n",
    "    good_aligments = CalculateAligments(sc, session, query_sequence, referenceName, top_matching)\n",
    "    tc = time()\n",
    "    good_aligments.persist()\n",
    "    n_aligments = good_aligments.count()\n",
    "    t_ext = tc-t4\n",
    "    tt = tc - t0\n",
    "\n",
    "    if (DTiming):\n",
    "        print(\"# Time required for calculate {} aligments: {} seconds.\\n\".format(n_aligments,round(tt,3)))\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Good Aligments:\")\n",
    "        print(good_aligments.take(1))\n",
    "        \n",
    "    if (DShowResult):\n",
    "        map(ShowAligmentResult,good_aligments.collect())\n",
    "\n",
    "    global YarnJobId\n",
    "    ResultFile = DHdfsOutputPath+APP_NAME+\"_\"+ os.path.splitext(os.path.basename(queryFilename))[0] +\"_\"+referenceName+\"_\"+YarnJobId\n",
    "    print(\"Writing matching aligments in file {}\".format(ResultFile))\n",
    "    good_aligments.coalesce(1).saveAsTextFile(ResultFile)\n",
    "\n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "               \n",
    "    if (DTiming):\n",
    "        print(\"# Total time required for processing the query with {} keys: {} seconds.\".format(keysdespl_rdd.count(), round(tt,3)))\n",
    "\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"########################### FINAL STATISTICS Single QUERY {} ###########################\".format(date_time))\n",
    "        print(\"# Reference: {}  \\tFile: {}   \\tQuerys: {}.\".format(ReferenceName, queryFilename, 1))\n",
    "        print(\"# Key Size: {}            \\tMethod: {}.\".format(KeySize, Method))\n",
    "        print(\"# Num Executors: {}      \\tExecutors/cores: {}      \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"# Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"# Total Time: {}         \\tData Read Time: {}       \\tCassandra Read Time: {} .\".format(round(tt,3), round(t_read,3), round(t_qcass,3)))\n",
    "        print(\"# Top matching Time: {}   \\tAlig. Extension Time: {}.\".format(round(t_topmat,3), round(t_ext,3)))\n",
    "        print(\"############################################################################################\")\n",
    "        #result.persist()\n",
    "        #print(\"# Total time required for processing {} keys using {} partitions in {} seconds.\".format(result.count(), result.getNumPartitions(), round(tt,3)))\n",
    "        #print(\"# Reference data size: {} MBytes.\\n\".format(round(get_size(ReferenceFilename)/(1024.0*1024.0),3)))\n",
    "        if (StatisticsFileName):\n",
    "            write_statistics_single_query(StatisticsFileName, queryFilename, ReferenceName, KeySize, date_time, 1, candidate_offsets, n_aligments, offsets_count.getNumPartitions() ,tt, t_read, t_qcass, t_topmat, t_ext)\n",
    "        \n",
    "    print(\"Done.\")\n",
    "        \n",
    "    return good_aligments\n",
    "\n",
    "\n",
    "def pquerykeys(qk):\n",
    "    res = []\n",
    "    for off in qk[1]:\n",
    "        res.append((qk[0], off[0], off[1])) \n",
    "    return ([x for x in res])\n",
    "\n",
    "def addDespl(off_list,despl):\n",
    "    res = [off + despl for off in off_list] \n",
    "    return res\n",
    "    \n",
    "def flatten(off_list):\n",
    "    return ([y for x in off_list for y in x])\n",
    "\n",
    "def countby(off_list):\n",
    "    return(collections.Counter(off_list).items())\n",
    "\n",
    "def countbyfilter(off_list):\n",
    "    return(filter(lambda (k,v): v>DKeyMatchingThreshold,collections.Counter(off_list).items()))\n",
    "\n",
    "def countbyfilterdespl(off_list):\n",
    "    return(map(lambda (k,v): k, filter(lambda (k,v): v>DKeyMatchingThreshold,collections.Counter(off_list).items())))\n",
    "\n",
    "\n",
    "#map(lambda (k,v): k,filter(lambda (k,v): v>2,counter.items()))\n",
    "\n",
    "def MultipleQuery(sc, sqlContext, queryFilename, referenceName, keySize=DKeySize, hashName=None):\n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"MultipleQuery({}, {}, {}).\".format( queryFilename, referenceName, keySize, hashName))\n",
    "    dfc(\"MultipleQuery\",sc, sqlContext, queryFilename, referenceName, keySize, hashName)\n",
    "    \n",
    "    # Broadcast Global variables\n",
    "    global key_size_bc, gt0_bc, reference_name_bc, hash_name_bc\n",
    "    t0 = time()\n",
    "    gt0_bc = sc.broadcast(t0)\n",
    "    key_size_bc = sc.broadcast(keySize)\n",
    "    reference_name_bc = sc.broadcast(referenceName)\n",
    "    hash_name_bc = sc.broadcast(hashName)\n",
    "\n",
    "\n",
    "    if (DTiming):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"++++++++++++ INITIAL STATISTICS {} +++++++++++++\".format(date_time))\n",
    "        print(\"+ Reference: {}  \\tQuery file: {}.\".format(referenceName, queryFilename))\n",
    "        print(\"+ Key Size: {}   \\tMethod: {}.\".format(keySize, Method))\n",
    "        print(\"+ Num Executors: {}  \\tExecutors/cores: {}  \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"+ Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Create Cassandra Sesion\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    session = cluster.connect()\n",
    "\n",
    "  \n",
    "    # 1. Read and split query in keySize-Segments + Desplazament\n",
    "    global keysdespl_rdd, reference_keys\n",
    "    t1 = time()\n",
    "    keysdespl_rdd, query_sequence = CalculateMultipleQueryOffset(sc, sqlContext, queryFilename, keySize)\n",
    "    query_sequence.persist()\n",
    "    keysdespl_rdd.persist()\n",
    "    nquerys = query_sequence.count()\n",
    "    nkeysdespl = keysdespl_rdd.count()\n",
    "    t_read = time()-t1\n",
    "    print(\"@@@@@ nkeysdespl: {}\".format(nkeysdespl))\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"######################### Number of partitions query_sequence: {}\".format(query_sequence.getNumPartitions()))\n",
    "        print(\"######################### Number of partitions keysdespl_rdd: {}\".format(keysdespl_rdd.getNumPartitions()))\n",
    "        keysdespl_rdd.persist()\n",
    "        print(\"Query::QueryKeys:\")\n",
    "        print(keysdespl_rdd.take(1))\n",
    "        print(query_sequence.take(1))\n",
    "   \n",
    "   \n",
    "    # 2. Query Cassandra for the keys\n",
    "    t2 = time()\n",
    "    reference_keys = GetMultipleKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd)\n",
    "    reference_keys.persist()\n",
    "    print(\"@@@@@ Reference_keys: {}\".format(reference_keys.count()))\n",
    "    t_qcass = time()-t2\n",
    "        \n",
    "    if (DDebug):\n",
    "        reference_keys.persist()\n",
    "        print(\"######################### Number of partitions reference_keys: {}\".format(reference_keys.getNumPartitions()))\n",
    "        print(\"Query::QueryKeys: Matching desplazaments: {}\".format(reference_keys.take(1)))\n",
    " \n",
    "\n",
    "    # Joining querys & keys offsets\n",
    "    if (DDebug):\n",
    "        query_sequence.persist()\n",
    "        keysdespl_rdd.persist()\n",
    "        print(\"MultipleQuery:: Querys: {}\".format(query_sequence.take(1)))\n",
    "        print(\"MultipleQuery:: Query Keys: {}\".format(keysdespl_rdd.take(1)))\n",
    "        print(\"MultipleQuery:: Matching desplazaments: {}\".format(reference_keys.take(1)))\n",
    "        \n",
    "    t3 = time()\n",
    "    query_keys_df = keysdespl_rdd.flatMap(pquerykeys).toDF([\"Query\", \"Key\" , \"Despl\"])\n",
    "    reference_keys_df = reference_keys.toDF([\"Key2\" , \"Offsets\"])\n",
    "    query_keys_df.groupBy('Key')\n",
    "    reference_keys_df.groupBy('Key2')\n",
    "    \n",
    "    if (DDebug):\n",
    "        reference_keys_df.persist()\n",
    "        query_keys_df.persist()\n",
    "        print(\"MultipleQuery:: query_keys_df: \")\n",
    "        query_keys_df.show(10)\n",
    "        print(\"MultipleQuery:: reference_keys_df: \")\n",
    "        reference_keys_df.show(10)\n",
    "        \n",
    "    joined_df = query_keys_df.join(reference_keys_df, query_keys_df.Key == reference_keys_df.Key2)\n",
    "    joined_df = joined_df.drop(joined_df.Key2)\n",
    "    #joined_df.printSchema()\n",
    "    t3a = time()\n",
    "    t_join = t3a-t3\n",
    "\n",
    "    if (DDebug):\n",
    "        joined_df.persist()\n",
    "        print(\"######################### Number of partitions joined_df: {}\".format(joined_df.rdd.getNumPartitions()))\n",
    "        print(\"MultipleQuery:: joined_df:\")\n",
    "        joined_df.show(10)\n",
    "         \n",
    "    addDespl_udf = udf(addDespl,  ArrayType(LongType())) \n",
    "    joined_df = joined_df.withColumn(\"Offsets\", addDespl_udf('Offsets','Despl'))\n",
    "    t3b = time()\n",
    "    t_adddespl = t3b-t3a\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"MultipleQuery:: joined_df addind despl:\")\n",
    "        joined_df.show()\n",
    "  \n",
    "     \n",
    "    grouped_df = joined_df.drop(joined_df.Key).groupby('Query').agg(F.collect_list(\"Offsets\")).withColumnRenamed(\"collect_list(Offsets)\", \"Offsets\") \n",
    "    flatten_udf = udf(flatten,  ArrayType(LongType())) \n",
    "    grouped_df = grouped_df.withColumn(\"Offsets\", flatten_udf('Offsets'))\n",
    "    grouped_df.persist()\n",
    "    print(\"@@@@@ grouped_df: {}\".format(grouped_df.count()))\n",
    "    t_joining = time()-t3\n",
    "    \n",
    "    if (DDebug):\n",
    "        grouped_df.persist()\n",
    "        print(\"MultipleQuery:: grouped_df flatten despl:\")\n",
    "        grouped_df.show()\n",
    " \n",
    "    \n",
    "    \n",
    "    # 3. Calculate Top-matching offsets\n",
    "    t4 = time()\n",
    "    offcount_schema = ArrayType(StructType([\n",
    "                                    StructField(\"Offset\", IntegerType(), False),\n",
    "                                    StructField(\"count\", IntegerType(), False)\n",
    "                                ]))\n",
    "    countby_udf = udf(countbyfilter,  offcount_schema) \n",
    "    counted_df = grouped_df.withColumn(\"Offsets\", countby_udf('Offsets'))\n",
    "    \n",
    "    if (DDebug):\n",
    "        counted_df.persist()\n",
    "        print(\"MultipleQuery:: countbyfilter_udf despl:\")\n",
    "        print(counted_df.take(1))\n",
    "    \n",
    "    \n",
    "    countby_udf = udf(countbyfilterdespl, ArrayType(LongType())) \n",
    "    counted_df = grouped_df.withColumn(\"Offsets\", countby_udf('Offsets'))\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"MultipleQuery:: countbyfilterdespl despl:\")\n",
    "        counted_df.persist()\n",
    "        print(counted_df.take(1))\n",
    "        \n",
    "    filter_df = counted_df.filter(size('Offsets')>0)\n",
    "    \n",
    "    query_sequence_df = query_sequence.toDF()\n",
    "    query_offset_df = filter_df.join(query_sequence_df, query_sequence_df._c1 == filter_df.Query)\n",
    "    query_offset_df = query_offset_df.drop(query_offset_df._c1).drop(query_offset_df._c2).withColumnRenamed(\"_c0\", \"QuerySeq\")\n",
    "    query_offset_df.persist()\n",
    "    candidates_offsets = query_offset_df.select(F.sum(F.size('Offsets'))).collect()[0][0]\n",
    "    print(\"@@@@@ candidates_offsets: {}\".format(candidates_offsets))\n",
    "    t_topmat = time()-t4\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"######################### Number of partitions query_offset_df: {}\".format(query_offset_df.rdd.getNumPartitions()))\n",
    "        print(\"MultipleQuery:: Candidate Offsets: {}\".format(candidates_offsets ))\n",
    "        print(\"MultipleQuery:: query_offset_df:\")\n",
    "        print(query_offset_df.take(1))\n",
    "\n",
    "      \n",
    "    # 4. Make extension in Top-matching zones.\n",
    "    t5 = time()\n",
    "    good_aligments = CalculateAligmentsMultipleQuery(sc, session, query_offset_df, referenceName)\n",
    "    good_aligments.persist()\n",
    "    n_aligments = good_aligments.count()\n",
    "    print(\"@@@@@ N good aligments: {}\".format(n_aligments))\n",
    "    t6 = time()\n",
    "    t_ext = t6-t5\n",
    "    tt = t6-t0\n",
    "\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"Good Aligments:\")       \n",
    "        print(good_aligments.collect())\n",
    "        \n",
    "    if (DShowResult):\n",
    "        map(ShowMultipleQueryAligmentResult,good_aligments.collect()) \n",
    "    \n",
    "    global YarnJobId\n",
    "    ResultFile = DHdfsOutputPath+APP_NAME+\"_\"+ os.path.splitext(os.path.basename(queryFilename))[0] +\"_\"+referenceName+\"_\"+YarnJobId\n",
    "    print(\"Writing matching aligments in file {}\".format(ResultFile))\n",
    "    good_aligments.coalesce(1).saveAsTextFile(ResultFile)\n",
    "        \n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "        \n",
    "    if (DTiming):\n",
    "        print(\"# Total time required for processing the {} query with {} keys: {} seconds.\".format(nquerys, keysdespl_rdd.count(), round(tt,3)))\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"########################### FINAL STATISTICS Multiple QUERY {} ###########################\".format(date_time))\n",
    "        print(\"# Reference: {}  \\tFile: {}   \\tQuerys: {}.\".format(ReferenceName, queryFilename, nquerys))\n",
    "        print(\"# Key Size: {}            \\tMethod: {}.\".format(KeySize, Method))\n",
    "        print(\"# Num Executors: {}      \\tExecutors/cores: {}      \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"# Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"# Total Time: {}         \\tData Read Time: {}       \\tCassandra Read Time: {} .\".format(round(tt,3), round(t_read,3), round(t_qcass,3)))\n",
    "        print(\"# DF Joining Time: {}    \\tTop matching Time: {}   \\tAlig. Extension Time: {}.\".format(round(t_joining,3), round(t_topmat,3), round(t_ext,3)))\n",
    "        print(\"############################################################################################\")\n",
    "        #result.persist()\n",
    "        #print(\"# Total time required for processing {} keys using {} partitions in {} seconds.\".format(result.count(), result.getNumPartitions(), round(tt,3)))\n",
    "        #print(\"# Reference data size: {} MBytes.\\n\".format(round(get_size(ReferenceFilename)/(1024.0*1024.0),3)))\n",
    "        if (StatisticsFileName):\n",
    "            write_statistics_multiple_query(StatisticsFileName, queryFilename, ReferenceName, KeySize, date_time, nquerys, candidates_offsets, n_aligments, query_offset_df.rdd.getNumPartitions(), tt, t_read, t_qcass, t_joining, t_topmat, t_ext)\n",
    "\n",
    "            \n",
    "    print(\"Done.\")\n",
    "            \n",
    "    return good_aligments\n",
    "    \n",
    "    \n",
    "def ShowAligmentResult(aligment):\n",
    "    global reference_name_bc\n",
    "    print(\"Find in \"+reference_name_bc.value+\": \"+str(aligment[0])+' ---> '+str(aligment[1])+\", align score: \"+str(aligment[2]))\n",
    "    Display(aligment[3], aligment[4])\n",
    "    \n",
    "    \n",
    "def ShowMultipleQueryAligmentResult(aligment):\n",
    "    global reference_name_bc\n",
    "    print(\"Query \" +str(aligment[0])+ \" find in \"+reference_name_bc.value+\": \"+str(aligment[1])+' ---> '+str(aligment[2])+\", align score: \"+str(aligment[3]))\n",
    "    Display(aligment[4], aligment[5])\n",
    "\n",
    "    \n",
    "def CalculateAligments(sc, session, querySequence, referenceName, top_matching):\n",
    "    dfc(\"CalculateAligments\", sc, session, querySequence, referenceName, top_matching)\n",
    "    \n",
    "    # Get Content block size\n",
    "    querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=0\"\n",
    "    resultSelect = session.execute(querySelect)\n",
    "    if resultSelect:\n",
    "        contentBlockSize = resultSelect[0].size\n",
    "        if (DDebug):\n",
    "            print(\"CalculateAligments::Content block size: {}.\".format(contentBlockSize))\n",
    "    else:\n",
    "        print(\"ERROR: Refererence contante table {} do not exist.\".format(referenceName + \".\" + DReferenceContentTableName))     \n",
    "            \n",
    "    aligments = DistributeAligments(sc, querySequence, referenceName, top_matching, contentBlockSize)\n",
    "    \n",
    "    quality_aligments = aligments.filter(lambda algn: algn[2]>DMinAligmentScore) \n",
    "           \n",
    "    return quality_aligments\n",
    "    \n",
    "\n",
    "def DistributeAligments(sc, querySequence, referenceName, offsets_count, contentBlockSize):\n",
    "    dfc(\"DistributeAligments\", sc, querySequence, referenceName, offsets_count, contentBlockSize)\n",
    "    \n",
    "    global query_sequence_bc, content_block_size_bc\n",
    "    query_sequence_bc = sc.broadcast(querySequence)\n",
    "    content_block_size_bc = sc.broadcast(contentBlockSize)\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"DistributeAligments: {} {}\".format(querySequence, referenceName))\n",
    "        print(\"offsets_count: \",)\n",
    "        print(offsets_count.take(1))\n",
    "    \n",
    "    #CassandraAligmentR(offsets_count.collect()[0][0])\n",
    "    aligments = offsets_count.map(lambda off: CassandraAligmentR(off[0]))\n",
    "    \n",
    "    if (DDebug and not aligments.isEmpty()):\n",
    "        print(\"ResultAligments: {}\".format(aligments.take(1)))\n",
    "    \n",
    "    return aligments\n",
    "\n",
    "\n",
    "def CassandraAligment(record):\n",
    "    return CassandraAligmentR(record[0])\n",
    "\n",
    "def CassandraAligmentR(offset):\n",
    "    global reference_name_bc, query_sequence_bc, content_block_size_bc, key_size_bc\n",
    "    querySequence = query_sequence_bc.value\n",
    "    queryLength = len(querySequence)\n",
    "    referenceName = reference_name_bc.value\n",
    "    contentBlockSize = content_block_size_bc.value\n",
    "    keySize  = key_size_bc.value\n",
    "    \n",
    "    # calculate reference begin and end\n",
    "    reference_seq_begin = offset - (queryLength + keySize - 5)\n",
    "    reference_seq_end = reference_seq_begin + queryLength + keySize\n",
    "    \n",
    "    # Calculate reference content start and end block\n",
    "    bbegin = reference_seq_begin/contentBlockSize\n",
    "    bend = reference_seq_end/contentBlockSize\n",
    "    \n",
    "    # Get Reference Content Sequence\n",
    "    end, contentSequence = GetRefereceContentBlocks(referenceName, bbegin, bend)\n",
    "    if (contentSequence is None):\n",
    "        return (-1,-1, 0, \"Error in GetRefereceContentBlocks\",\"Block Id %d-%d not exist inf table %s\" % (bbegin, bend, referenceName))\n",
    "    #print(contentSequence.first())\n",
    "    if (DDebug):\n",
    "        print(\"Reference Content Sequence {}-{}: {}\".format(bbegin, bend, contentSequence))\n",
    "        \n",
    "    if (reference_seq_end>end):\n",
    "        contentSequence = contentSequence + \" \" * (reference_seq_end-end+1)\n",
    "    \n",
    "    # Calculate Alignment\n",
    "    boffset = reference_seq_begin-(contentBlockSize * bbegin) \n",
    "    if (DDebug):\n",
    "        print(\"Short Reference Content Sequence {}-{}: {}\".format(bbegin, bend, contentSequence[boffset:reference_seq_end]))\n",
    "    align_seq1, align_seq2, align_score, align_off = DoAligment(querySequence, contentSequence[boffset:reference_seq_end])\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"find in \"+referenceName+\": \"+str(reference_seq_begin+align_off)+' ---> '+str(reference_seq_begin+align_off+queryLength-1)+\", align score: \"+str(align_score))\n",
    "        Display(align_seq1, align_seq2)\n",
    "    \n",
    "    return (offset+align_off, offset+align_off+queryLength-1, align_score, align_seq1, align_seq2)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "def CalculateAligmentsMultipleQuery(sc, session, querySequences, referenceName):\n",
    "    dfc(\"CalculateAligmentsMultipleQuery\", sc, session, querySequences, referenceName)\n",
    "    \n",
    "    # Get Content block size\n",
    "    querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=0\"\n",
    "    resultSelect = session.execute(querySelect)\n",
    "    if resultSelect:\n",
    "        contentBlockSize = resultSelect[0].size\n",
    "        if (DDebug):\n",
    "            print(\"CalculateAligmentsMultipleQuery::Content block size: {}.\".format(contentBlockSize))\n",
    "    else:\n",
    "        print(\"ERROR: Refererence contante table {} do not exist.\".format(referenceName + \".\" + DReferenceContentTableName))\n",
    "    \n",
    "    aligments = DistributeAligmentsMultipleQuery(sc, querySequences, referenceName, contentBlockSize)\n",
    "    aligments.persist()\n",
    "    \n",
    "    if not aligments.isEmpty():\n",
    "        #print(\"CalculateAligmentsMultipleQuery::aligments: {}\".format(aligments.take(10)))\n",
    "        quality_aligments = aligments.filter(lambda algn: algn[3]>DMinAligmentScore)    \n",
    "        return quality_aligments\n",
    "    else:\n",
    "        return sc.emptyRDD()\n",
    "    \n",
    "\n",
    "def DistributeAligmentsMultipleQuery(sc, querySequences, referenceName, contentBlockSize):\n",
    "    dfc(\"DistributeAligmentsMultipleQuery\", sc, querySequences, referenceName, contentBlockSize)\n",
    "    \n",
    "    global content_block_size_bc\n",
    "    content_block_size_bc = sc.broadcast(contentBlockSize)\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"DistributeAligmentsMultipleQuery: {} \".format(referenceName))\n",
    "        print(\"querySequences: \",)\n",
    "        print(querySequences.take(1))\n",
    "    \n",
    "    #CassandraAligmentR(offsets_count.collect()[0][0])\n",
    "    aligments = querySequences.rdd.flatMap(CassandraAligmentMultipleQuery)\n",
    "    \n",
    "    if (DDebug and aligments.count()>0):\n",
    "        print(\"ResultAligments: {}\".format(aligments.take(1)))\n",
    "    \n",
    "    return aligments\n",
    "\n",
    "\n",
    "def CassandraAligmentMultipleQuery(record):\n",
    "    #if (DDebug and record[0]!=4):\n",
    "    #    return ([(record[0], -1, -1, 0, \"Not Processed\",\"%s\" % (record[2]))])\n",
    "    return CassandraAligmentMultipleQueryR(record[0], record[1], record[2])\n",
    "\n",
    "def CassandraAligmentMultipleQueryR(queryId, offsetList, querySequence):\n",
    "    dfc(\"CassandraAligmentMultipleQueryR\", queryId, offsetList, querySequence)\n",
    "    \n",
    "    global reference_name_bc, content_block_size_bc, key_size_bc\n",
    "    referenceName = reference_name_bc.value\n",
    "    contentBlockSize = content_block_size_bc.value\n",
    "    keySize  = key_size_bc.value\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"CassandraAligmentMultipleQueryR ({}, {}, {}).\".format(queryId, offsetList, querySequence))\n",
    "    \n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    session = cluster.connect()  \n",
    "    queryLength = len(querySequence)\n",
    "    \n",
    "    aligments = []\n",
    "    contentSequence = \"\"\n",
    "    #print(\"Offsets {} sort: {}\".format(offsetList, offsetList.sort()))\n",
    "    offsetList.sort()\n",
    "    for offset in offsetList:\n",
    "        if (DDebug):\n",
    "            print(\"CassandraAligmentMultipleQueryR:: Processing offset {}.\".format(offset))\n",
    "            \n",
    "        # calculate reference begin and end\n",
    "        reference_seq_begin = offset - (queryLength + keySize - 5)\n",
    "        reference_seq_end = reference_seq_begin + queryLength + keySize\n",
    "        # Calculate reference content start and end block\n",
    "        bbegin = reference_seq_begin/contentBlockSize\n",
    "        bend = reference_seq_end/contentBlockSize\n",
    "        \n",
    "        # Get Reference Content Sequence\n",
    "        end, contentSequence = GetRefereceContentBlocksCache(session, referenceName, bbegin, bend)\n",
    "        if (contentSequence is None):\n",
    "            continue\n",
    "        if (DDebug):\n",
    "            print(\"Reference Content Sequence {}-{}: {}\".format(bbegin, bend, contentSequence))\n",
    "\n",
    "        if (reference_seq_end>end):\n",
    "            contentSequence = contentSequence + \" \" * (reference_seq_end-end+1)\n",
    "            \n",
    "        # Calculate Alignment\n",
    "        boffset = reference_seq_begin-(contentBlockSize * bbegin) \n",
    "        if (DDebug):\n",
    "            print(\"Short Reference Content Sequence {}-{}, {}-{}: {}\".format(bbegin, bend, reference_seq_begin, reference_seq_end, contentSequence[boffset:reference_seq_end]))\n",
    "        align_seq1, align_seq2, align_score, align_off = DoAligment(querySequence, contentSequence[boffset:reference_seq_end])\n",
    "\n",
    "        if (DDebug):\n",
    "            print(\"find in \"+referenceName+\": \"+str(reference_seq_begin+align_off)+' ---> '+str(reference_seq_begin+align_off+queryLength-1)+\", align score: \"+str(align_score))\n",
    "            Display(align_seq1, align_seq2)\n",
    "            \n",
    "        aligments.append((queryId, offset+align_off, offset+align_off+queryLength-1, align_score, align_seq1, align_seq2))\n",
    "        \n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    if (len(aligments)==0):\n",
    "        print(\"CassandraAligmentMultipleQueryR {} {} {}\".format(queryId, offsetList, querySequence))\n",
    "        aligments.append((queryId, -1, -1, 0, \"Error in GetRefereceContentBlocks\",\"Block Id %d-%d not exist inf table %s - offsets %s\" % (bbegin, bend, referenceName,offsetList)))\n",
    "\n",
    "    return(aligments)\n",
    "    \n",
    "   \n",
    "   \n",
    "def GetRefereceContentBlocks(referenceName, bbegin, bend):\n",
    "    dfc(\"GetRefereceContentBlocks\", referenceName, bbegin, bend)\n",
    "    \n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()  \n",
    "    \n",
    "    contentSequence = \"\"\n",
    "    blocks_read = 0\n",
    "    end = 0\n",
    "    for block in range(bbegin, bend+1):\n",
    "        # Get Content block\n",
    "        if (DDebug):\n",
    "            print(\"Reading block {} froma {} table.\".format(block,referenceName))\n",
    "        querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=%s\"\n",
    "        resultSelect = ses.execute(querySelect, [block])\n",
    "        if resultSelect:\n",
    "            contentSequence = contentSequence + resultSelect[0].value\n",
    "            blocks_read +=1\n",
    "            end = resultSelect[0].offset + resultSelect[0].size\n",
    "            \n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    if (blocks_read>0):\n",
    "        return (end, contentSequence)\n",
    "    else:\n",
    "        print(\"ERROR: Refererence Content block id {} do not exist in table {} .\".format(end+1,referenceName + \".\" + DReferenceContentTableName))\n",
    "        return (0,None)\n",
    "      \n",
    "        \n",
    "        \n",
    "def static_vars(**kwargs):\n",
    "    def decorate(func):\n",
    "        for k in kwargs:\n",
    "            setattr(func, k, kwargs[k])\n",
    "        return func\n",
    "    return decorate\n",
    "\n",
    "@static_vars(BlockCache = collections.defaultdict(list))\n",
    "def GetRefereceContentBlocksCache(ses, referenceName, bbegin, bend):\n",
    "    \n",
    "    contentSequence = \"\"\n",
    "    blocks_read = 0\n",
    "    end = 0\n",
    "    for block in range(bbegin, bend+1):\n",
    "        if block in GetRefereceContentBlocksCache.BlockCache:\n",
    "            if (DDebug):\n",
    "                print(\"HIT block cache {} -> {}.\".format(block,GetRefereceContentBlocksCache.BlockCache[block][0]))\n",
    "            contentSequence = contentSequence + GetRefereceContentBlocksCache.BlockCache[block][1]\n",
    "            blocks_read +=1\n",
    "            end = GetRefereceContentBlocksCache.BlockCache[block][0] + len(GetRefereceContentBlocksCache.BlockCache[block][1]) \n",
    "        else:\n",
    "            # Get Content block\n",
    "            if (DDebug):\n",
    "                print(\"MISS block {} Reading froma {} table.\".format(block,referenceName))\n",
    "            querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=%s\"\n",
    "            resultSelect = ses.execute(querySelect, [block])\n",
    "            if resultSelect:\n",
    "                contentSequence = contentSequence + resultSelect[0].value\n",
    "                blocks_read +=1\n",
    "                end = resultSelect[0].offset + resultSelect[0].size\n",
    "                #Put Block in cache.\n",
    "                GetRefereceContentBlocksCache.BlockCache[block]=(resultSelect[0].offset, resultSelect[0].value)\n",
    "                #Pop Blcok\n",
    "                if block-DBlockCacheSize in GetRefereceContentBlocksCache.BlockCache:\n",
    "                    GetRefereceContentBlocksCache.BlockCache.pop(block-DBlockCacheSize)\n",
    "            \n",
    "    if (blocks_read>0):\n",
    "        return (end, contentSequence)\n",
    "    else:\n",
    "        print(\"ERROR: Refererence Content block id {} do not exist in table {} .\".format(end+1,referenceName + \".\" + DReferenceContentTableName))\n",
    "        return ( 0, None)\n",
    "    \n",
    "\n",
    "        \n",
    "def ProcessCassandraQuery(tuple):\n",
    "\n",
    "    global reference_name_bc, hash_name_bc\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()\n",
    "            \n",
    "    res = ProcessCassandraQueryR(tuple[0], tuple[1], ses, reference_name_bc.value, hash_name_bc.value)\n",
    "\n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    return (res)\n",
    "    \n",
    "    \n",
    "def ProcessCassandraQueryR(key, despl, session, referenceName, hashName):\n",
    "    dfc(\"ProcessCassandraQueryR\", key, despl, session, referenceName, hashName)\n",
    "        \n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"Processing Query Record {} with despl {}.\".format(key, despl))\n",
    "   \n",
    "    if hashName is None:\n",
    "        querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "    else:\n",
    "        querySelect = \"SELECT * FROM \" + hashName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryR::Select: {} {}\".format(querySelect,key))\n",
    "        \n",
    "    resultSelect = session.execute(querySelect, [key] )\n",
    "\n",
    "    res = []\n",
    "    for result_row in resultSelect:\n",
    "        if (DDebug):\n",
    "            print(\"{}.{}->{}\".format(result_row.seq, result_row.block, list(map(lambda offset: offset, result_row.value))))\n",
    "        res.append(map(lambda offset: offset+int(despl), result_row.value))        \n",
    "        #res.append(map(lambda offset: int(offset)+int(despl), result_row.value))        \n",
    "    print(res)\n",
    "    res = flattened_list = [y for x in res for y in x]           \n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"Processing Query Record result: {}.\".format(res))\n",
    "    \n",
    "    return res\n",
    "    \n",
    "      \n",
    "def ProcessMultipleCassandraQuery(tuple):\n",
    "\n",
    "    global reference_name_bc\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()\n",
    "            \n",
    "    res = ProcessMultipleCassandraQueryR(tuple, ses, reference_name_bc.value, hash_name_bc.value)\n",
    "\n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    return (res)\n",
    "    \n",
    "    \n",
    "def ProcessMultipleCassandraQueryR(key, session, referenceName, hashName):\n",
    "    dfc(\"ProcessMultipleCassandraQueryR\", key, session, referenceName, hashName)\n",
    "    \n",
    "    global DDebug\n",
    "    if (DDebug and False):\n",
    "        print(\"Processing Multilple Query Record {}.\".format(key))\n",
    "        \n",
    "    if hashName is None:\n",
    "        querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "    else:\n",
    "        querySelect = \"SELECT * FROM \" + hashName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryR::Select: {} {}\".format(querySelect,key))\n",
    "        \n",
    "    resultSelect = session.execute(querySelect, [key] )\n",
    "\n",
    "    res = []\n",
    "    for result_row in resultSelect:\n",
    "        if (DDebug):\n",
    "            print(\"{}.{}->{}\".format(result_row.seq, result_row.block, list(map(lambda offset: offset, result_row.value))))\n",
    "        res.append(map(lambda offset: offset, result_row.value))        \n",
    "        #res.append(result_row.value)        \n",
    "        #res.append(map(lambda offset: int(offset)+int(despl), result_row.value))        \n",
    "    #print(res)\n",
    "    res = flattened_list = [y for x in res for y in x]           \n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"Processing Multilple Query Record result: {}.\".format(res))\n",
    "    \n",
    "    return ((key, res))\n",
    "        \n",
    "# Loads and returns data frame for a table including key space given\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df\n",
    "    \n",
    "\n",
    "## \n",
    "## Receives Keys+Desplazament tuples (keysdespl_rdd) and select the keys in cassandra reference table and\n",
    "## sums the corresponding query Desplazament to the reference Offset\n",
    "## \n",
    "def GetKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd):\n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference\")\n",
    "        \n",
    "    if (DDebug & False):\n",
    "        # Show Reference Table\n",
    "        GenRef = load_and_get_table_df(referenceName, DReferenceContentTableName)\n",
    "        print(\"Reference Table:\")\n",
    "        GenRef.show()\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::QueryKeys:\")\n",
    "        print(keysdespl_rdd.take(1))\n",
    "    \n",
    "    #print(\"GetKeysOffsetsInReference::Process first element\")\n",
    "    #first = keysdespl_rdd.first()\n",
    "    #ProcessCassandraQuery(first[0], first[1], session, referenceName)\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Process all elements\")\n",
    "    matching_despl_rdd = keysdespl_rdd.flatMap(lambda kv: ProcessCassandraQuery(kv))\n",
    "          \n",
    "    if (DDebug & matching_despl_rdd.isEmpty()==False):\n",
    "        print(\"GetKeysOffsetsInReference::Result: {}\".format(matching_despl_rdd.isEmpty()))\n",
    "        print(matching_despl_rdd.take(1))\n",
    "       \n",
    "    return matching_despl_rdd\n",
    "\n",
    "\n",
    "## \n",
    "## Receives Keys+Desplazament tuples (keysdespl_rdd) and select the keys in cassandra reference table and\n",
    "## sums the corresponding query Desplazament to the reference Offset\n",
    "## \n",
    "def GetMultipleKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd):\n",
    "    dfc(\"GetMultipleKeysOffsetsInReference\", sc, session, referenceName, keysdespl_rdd)\n",
    "        \n",
    "    #offsets_rdd = keysdespl_rdd.flatMap(lambda res: off[0] for off in res[1])\n",
    "    offsets_rdd = keysdespl_rdd.flatMap(lambda res: map(lambda off: off[0], res[1]))\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Print all {} offsets: {}\".format(offsets_rdd.count(), offsets_rdd.sortBy(lambda r:r[0]).collect()))\n",
    "    \n",
    "    offsets_rdd = offsets_rdd.distinct()\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Print distintct {} offsets: {}\".format(offsets_rdd.count(), offsets_rdd.sortBy(lambda r:r[0]).collect()))\n",
    "               \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Process all elements\")\n",
    "    matching_despl_rdd = offsets_rdd.map(lambda kv: ProcessMultipleCassandraQuery(kv))\n",
    "    \n",
    "    # Filter emtpy offsets\n",
    "    matching_despl_rdd =  matching_despl_rdd.filter(lambda off: len(off[1])>0)\n",
    "          \n",
    "    if (DDebug and matching_despl_rdd.isEmpty()==False):\n",
    "        print(\"GetKeysOffsetsInReference::Result: {}\".format(matching_despl_rdd.isEmpty()))\n",
    "        print(matching_despl_rdd.take(1))\n",
    "       \n",
    "    return matching_despl_rdd\n",
    "\n",
    "\n",
    "def ProcessCassandraQueryByPartition(list):\n",
    "\n",
    "    global reference_name_bc, DDebug\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(list)\n",
    "        \n",
    "    if (DDebug):\n",
    "        for tuple in list:       \n",
    "            print(\"ProcessCassandraQueryByPartition: Key: {}, value: {}.\".format(tuple[0], tuple[1]))\n",
    "    \n",
    "    res = map(lambda tuple : ProcessCassandraQueryR(tuple[0], tuple[1], ses, reference_name_bc.value), list)\n",
    "    res = flattened_list = [y for x in res for y in x]     \n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryByPartition::Result: {}.\".format(res))\n",
    "        \n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "## \n",
    "## Receives Keys+Desplazament tuples (keysdespl_rdd) and select the keys in cassandra reference table and\n",
    "## sums the corresponding query Desplazament to the reference Offset\n",
    "## The procesing is done at high-level one-partition one task, in order to reducer the cassandra connections.\n",
    "\n",
    "def GetKeysOffsetsInReferenceByPartition(sc, session, referenceName, keysdespl_rdd):\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition\")\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition::QueryKeys:\")\n",
    "        print(keysdespl_rdd.take(1))\n",
    "    \n",
    "    #print(\"GetKeysOffsetsInReference::Process first element\")\n",
    "    #first = keysdespl_rdd.first()\n",
    "    #ProcessCassandraQuery(first[0], first[1], session, referenceName)\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition::Process all elements\")\n",
    "    matching_despl_rdd = keysdespl_rdd.glom().flatMap(lambda kv: ProcessCassandraQueryByPartition(kv) if len(kv) > 0 else \"\")\n",
    "          \n",
    "    if (DDebug & matching_despl_rdd.isEmpty()==False):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition::Result: {}\".format(matching_despl_rdd.isEmpty()))\n",
    "        print(matching_despl_rdd.take(1))\n",
    "       \n",
    "    return matching_despl_rdd\n",
    "\n",
    "\n",
    "    \n",
    "def CalculateQueryOffset(sc, queryFilename, keySize):\n",
    "    if (DDebug):\n",
    "        print(\"CalculateQueryOffset\")\n",
    "\n",
    "    # Read query file (offset,line) from hdfs\n",
    "    query_rdd = sc.newAPIHadoopFile(\n",
    "        queryFilename,\n",
    "        'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "        'org.apache.hadoop.io.LongWritable',\n",
    "        'org.apache.hadoop.io.Text',\n",
    "    )\n",
    "    if (DDebug):\n",
    "        print(\"Query::Input file has {} lines and {} partitions: \".format(query_rdd.count(),query_rdd.getNumPartitions()))\n",
    "        print(\"Query::First 10 records: \"+format(query_rdd.take(1)))\n",
    "        #query_rdd.count()\n",
    "    \n",
    "    # Calculate Dataframe\n",
    "    query_df, query_sequence = CreateDataFrame(query_rdd)\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Query Data Frame: \")\n",
    "        query_df.show(10)\n",
    "    \n",
    "    # Calculate Query length\n",
    "    queryLength =len(query_sequence)\n",
    "#    if CreateWindowWithPartitions:\n",
    "#        row = query_df.rdd.reduce(lambda x, y: x if int(x[3]) > int(y[3]) else y)\n",
    "#    else:\n",
    "#        row = query_df.rdd.reduce(lambda x, y: x if int(x[2]) > int(y[2]) else y)\n",
    "#       \n",
    "#    queryLength =  int(row['offset']) + int(row['size'])\n",
    "    global query_length_bc\n",
    "    query_length_bc = sc.broadcast(queryLength)\n",
    "    \n",
    "    # Calculate keys & offsets\n",
    "    t1 = time()\n",
    "    query_keys_despl_rdd = query_df.rdd.flatMap(calculateQueryKeysDespl)\n",
    "    query_keys_despl_rdd.persist()\n",
    "        \n",
    "    if (DTiming):\n",
    "        print(\"Time required for calculate keys in {} seconds.\\n\".format(round(time() - t1,3)))\n",
    "        tt = time() - gt0_bc.value\n",
    "        print(\"Total time required for processing {} keys using {} partitions in {} seconds.\".format(query_keys_despl_rdd.count(), query_keys_despl_rdd.getNumPartitions(), round(tt,3)))\n",
    "        print(\"Query data size: {} MBytes.\\n\".format(round(get_size(queryFilename)/(1024.0*1024.0),3)))\n",
    "\n",
    "    return query_keys_despl_rdd, query_sequence\n",
    "\n",
    "\n",
    "\n",
    "def CalculateMultipleQueryOffset(sc, sqlContext, queryFilename, keySize):\n",
    "    dfc(\"CalculateMultipleQueryOffset\", sc, sqlContext, queryFilename, keySize)\n",
    "    \n",
    "    # Read querys csv files: (query, id, length)\n",
    "    query_df = sqlContext.read.option(\"delimiter\", \"\\\\t\").csv(queryFilename, inferSchema=True)\n",
    "    if (DDebug):\n",
    "        print(\"######################### Query::Input file has {} sequences and {} partitions: \".format(query_df.count(),query_df.rdd.getNumPartitions()))\n",
    "        # Calculate Number of Partitions           \n",
    "    convertedudf = udf(nonasciitoascii)\n",
    "    query_df = query_df.withColumn(\"_c0\", convertedudf(upper(query_df._c0)))\n",
    "    query_rdd = query_df.rdd\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"######################### Query::Input file has {} sequences and {} partitions: \".format(query_df.count(),query_df.rdd.getNumPartitions()))\n",
    "        print(\"Query::First 10 records: \"+format(query_df.take(1)))\n",
    "        query_df.printSchema()\n",
    "    \n",
    "    # Calculate Number of Partitions    \n",
    "    executors = sc._conf.get(\"spark.executor.instances\")\n",
    "    cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "    memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "    if executors is None or cores is None:\n",
    "        total_cores = 1\n",
    "    else:\n",
    "        total_cores = int(executors) * int(cores)\n",
    "    max_partitions = DMaxNumberStages * total_cores\n",
    "    nquerys = query_df.count()\n",
    "    if (nquerys>max_partitions):\n",
    "        NumberPartitions = max_partitions\n",
    "    else:\n",
    "        NumberPartitions = nquerys\n",
    "    \n",
    "    # Repartition.\n",
    "    print(\"######################### Total cores: {}  Max Partition: {}  NQuerys: {}  Calc Partitions: {}  Rdd Partitions: {}\".format(total_cores, max_partitions, nquerys, NumberPartitions, query_rdd.getNumPartitions()))\n",
    "      \n",
    "    if (NumberPartitions>query_rdd.getNumPartitions()):\n",
    "        query_rdd = query_rdd.repartition(NumberPartitions)\n",
    "    else:\n",
    "        query_rdd = query_rdd.coalesce(NumberPartitions)\n",
    "    \n",
    "    print(\"######################### Total cores: {}  Max Partition: {}  NQuerys: {}  Calc Partitions: {}  NEW Rdd Partitions: {}\".format(total_cores, max_partitions, nquerys, NumberPartitions, query_rdd.getNumPartitions()))\n",
    "    \n",
    "    # Calculate keys & offsets\n",
    "    t1 = time()\n",
    "    query_keys_despl_rdd = query_rdd.map(cython_calculateMultipleQueryKeysDespl)\n",
    "    #query_keys_despl_rdd = query_rdd.map(calculateMultipleQueryKeysDespl)\n",
    "    \n",
    "    # Repartition.\n",
    "    if (NumberPartitions>query_rdd.getNumPartitions()):\n",
    "        query_keys_despl_rdd = query_keys_despl_rdd.repartition(NumberPartitions)\n",
    "    else:\n",
    "        query_keys_despl_rdd = query_keys_despl_rdd.coalesce(NumberPartitions)\n",
    "            \n",
    "    if (DTiming and DDebug):\n",
    "        print(\"######################### Time required for calculate keys in {} seconds.\\n\".format(round(time() - t1,3)))\n",
    "        tt = time() - gt0_bc.value\n",
    "        print(\"######################### Total time required for processing {} keys using {} partitions in {} seconds.\".format(query_keys_despl_rdd.count(), query_keys_despl_rdd.getNumPartitions(), round(tt,3)))\n",
    "        print(\"######################### Query data size: {} MBytes.\\n\".format(round(get_size(queryFilename)/(1024.0*1024.0),3)))\n",
    "\n",
    "    return query_keys_despl_rdd, query_rdd\n",
    "\n",
    "\n",
    "\n",
    "def calculateQueryKeysDespl(record):\n",
    "    return calculateQueryKeysDesplR(record.offset, record.size, record.lines, len(record.lines))\n",
    "    \n",
    "    \n",
    "# Calculate query keys with the following tuples {key,offset}\n",
    "def calculateQueryKeysDesplR(offset, size, lines, lines_size):\n",
    "    dfc(\"calculateQueryKeysDesplR\", offset, size, lines, lines_size)\n",
    "        \n",
    "    global key_size_bc, query_length_bc\n",
    "    KeySize  = key_size_bc.value\n",
    "    QueryLength = query_length_bc.value\n",
    "    KeysDespl = []\n",
    "    \n",
    "    #tg1 = time()\n",
    "    # Calculate the first and last keys desplazaments.\n",
    "    first_key = 0\n",
    "    if (size!=lines_size):    \n",
    "        # Internal lines.\n",
    "        last_key = size\n",
    "    else:\n",
    "        # Last file line.\n",
    "        last_key = size - KeySize + 1\n",
    "        \n",
    "    offset = QueryLength-(offset+first_key+1)\n",
    "    for k in range (first_key, last_key):       \n",
    "        # Add key to python list\n",
    "        KeysDespl.append((lines[k:k+KeySize],int(offset)))\n",
    "        offset -= 1\n",
    "        \n",
    "    #print(\"\\rProcessing {} keys from offset {} in {} secs\".format(len(Keys),offset, round(time() - gt0.value,3), end =\" \"))\n",
    "    \n",
    "    return KeysDespl\n",
    "  \n",
    "    \n",
    "def cython_calculateMultipleQueryKeysDespl(record):\n",
    "    #print(\"calculateMultipleQueryKeysDespl {}.\".format(record))\n",
    "    global key_size_bc\n",
    "    \n",
    "    import pyximport\n",
    "    pyximport.install(build_dir='/tmp/cython')\n",
    "    import do_query.pyx\n",
    "    cyt_calculateMultipleQueryKeysDesplR = spark_cython('do_query.pyx', 'cython_calculateMultipleQueryKeysDesplR')\n",
    "        \n",
    "    #cyt_calculateMultipleQueryKeysDesplR = spark_cython('do_query', 'cython_calculateMultipleQueryKeysDesplR')\n",
    "    return (record[\"_c1\"], cyt_calculateMultipleQueryKeysDesplR(record._c0.upper().encode('ascii','ignore'), len(record._c0), key_size_bc))\n",
    "\n",
    "\n",
    "def calculateMultipleQueryKeysDespl(record):\n",
    "    #print(\"calculateMultipleQueryKeysDespl {}.\".format(record))\n",
    "    return (record[\"_c1\"], calculateMultipleQueryKeysDesplR(record._c0.upper().encode('ascii','ignore'), len(record._c0)))\n",
    "\n",
    "       \n",
    "# Calculate multiple query keys with the following tuples {key,offset}\n",
    "def calculateMultipleQueryKeysDesplR(query, size):\n",
    "    dfc(\"calculateMultipleQueryKeysDesplR\", query, size)\n",
    "        \n",
    "    global key_size_bc\n",
    "    KeySize  = key_size_bc.value\n",
    "    KeysDespl = []\n",
    "    \n",
    "    #tg1 = time()\n",
    "    # Calculate the first and last keys desplazaments.\n",
    "    first_key = 0\n",
    "    last_key = size - KeySize + 1\n",
    "        \n",
    "    offset = size-(first_key+1)\n",
    "    for k in range (first_key, last_key):       \n",
    "        # Add key to python list\n",
    "        KeysDespl.append((query[k:k+KeySize],int(offset)))\n",
    "        offset -= 1\n",
    "        \n",
    "    #print(\"\\rProcessing {} keys from offset {} in {} secs\".format(len(Keys),offset, round(time() - gt0.value,3), end =\" \"))\n",
    "    \n",
    "    return KeysDespl\n",
    "    \n",
    "    \n",
    "def CreateDataFrame(reference_rdd):\n",
    "\n",
    "    # Create DataFrame  \n",
    "    reference_df = sqlContext.createDataFrame(reference_rdd,[\"file_offset\",\"line\"])\n",
    "\n",
    "    # Delete first line header if exist\n",
    "    header = reference_df.first()\n",
    "    header_size = 0\n",
    "    if (header.line[0]=='>'):\n",
    "        header_size = len(header.line)+1\n",
    "        reference_df = reference_df.filter(reference_df.file_offset!=0)\n",
    "               \n",
    "    if (Method==ECreate2LinesData):\n",
    "        df = Create2LinesDataFrame(reference_df, header_size, BlockSize)\n",
    "    elif (Method==ECreate1LineDataWithoutDependencies):\n",
    "        df = Create1LineDataFrameWithoutDependencies(reference_df, header_size, BlockSize)\n",
    "    elif (Method==ECreateBlocksData):\n",
    "        df = CreateBlocksDataFrame(reference_df, BlockSize)\n",
    "    \n",
    "    # Calculate Query String\n",
    "    query_string = ''\n",
    "    query_lines = df.select('line').collect()\n",
    "    for row in query_lines:\n",
    "        query_string = query_string + row[0]\n",
    "        \n",
    "    df.drop(df.line)\n",
    "    \n",
    "    return df, query_string.upper()\n",
    "    \n",
    "def nonasciitoascii(unicodestring):\n",
    "    return unicodestring.encode(\"ascii\",\"ignore\")\n",
    "    \n",
    "def Create2LinesDataFrame(df, header_size, blocksize):\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Method: ECreate2LinesData\")\n",
    "        \n",
    "    if (CreateWindowWithPartitions and blocksize>0):\n",
    "        df = df.withColumn(\"block\", (df.file_offset / blocksize).cast(\"int\"))\n",
    "        my_window = Window.partitionBy(\"block\").orderBy(\"file_offset\")\n",
    "        if (DDebug):\n",
    "            print(\"Spark shuffle partitions:\".format(sqlContext.getConf(\"spark.sql.shuffle.partitions\")))\n",
    "    else: # Without partitions\n",
    "        my_window = Window.partitionBy().orderBy(\"file_offset\")\n",
    "    \n",
    "    convertedudf = udf(nonasciitoascii)\n",
    "    df1 = df.withColumn(\"line\", convertedudf(upper(df.line)))\n",
    "    df2 = df1.withColumn(\"next_line\", F.lag(df1.line,-1).over(my_window))\n",
    "    df3 = df2.withColumn(\"size\", F.length(df2.line)) \\\n",
    "             .withColumn(\"lines\", F.when(F.isnull(df2.next_line), df2.line) \\\n",
    "                                   .otherwise(F.concat(df2.line, df2.next_line))) \\\n",
    "             .withColumn(\"offset\", df2.file_offset-header_size) \\\n",
    "             .drop(df2.next_line).drop(df2.file_offset) \n",
    "    df3.persist()\n",
    "        \n",
    "    #print(\"Query String: \")\n",
    "    #print(\"Number of total rows: {} with {} partitions\".format(df3.count(),df3.rdd.glom().count()))\n",
    "    #print(\"DF3:\")\n",
    "    #df3.show(20) \n",
    "    #val rdd = sc.cassandraTable(\"test\", \"words\")\n",
    "    if (DDebug):\n",
    "        print(\"Time required for read and prepare dataframe with {} rows using {} partitions in {} seconds.\\n\".format(df3.count(), df3.rdd.getNumPartitions(), round(time() - gt0_bc.value,3)))\n",
    "    #print(\"DF3:\")\n",
    "    #df3.show(10)\n",
    "    #print(\"Time required for read and prepare dataframe in {} seconds.\\n\".format(round(time() - gt0_bc.value,3)))\n",
    "\n",
    "    return df3\n",
    "\n",
    "\n",
    "def Create1LineDataFrameWithoutDependencies(df, header_size, blocksize):\n",
    "    # Create Blocks of lines to avoid dependencies with the previous line.\n",
    "    # Ref: https://stackoverflow.com/questions/49468362/combine-text-from-multiple-rows-in-pyspark\n",
    "  \n",
    "    if (DDebug):\n",
    "        print(\"Method: ECreate1LineDataWithoutDependencies\")\n",
    "    if (CreateWindowWithPartitions and blocksize>0):\n",
    "        df = df.withColumn(\"block\", (df.file_offset / blocksize).cast(\"int\"))\n",
    "        my_window = Window.partitionBy(\"block\").orderBy(\"file_offset\")\n",
    "        if (DDebug):\n",
    "            print(\"Spark shuffle partitions:\".format(sqlContext.getConf(\"spark.sql.shuffle.partitions\")))\n",
    "    else: # Without partitions\n",
    "        my_window = Window.partitionBy().orderBy(\"file_offset\")\n",
    "    \n",
    "    convertedudf = udf(nonasciitoascii)\n",
    "    df = df.withColumn(\"lines\", convertedudf(upper(df.line)))\n",
    "    df3 = df.withColumn(\"size\", F.length(df.lines)-DKeySize) \\\n",
    "             .withColumn(\"offset\", df.file_offset-header_size) \\\n",
    "             .drop(df.file_offset) \n",
    "    #df3.persist()\n",
    "    #df3.rdd.getNumPartitions() \n",
    "    #print(\"Number of total rows: {} with {} partitions\".format(df3.count(),df3.rdd.glom().count()))\n",
    "    #print(\"DF3:\")\n",
    "    #df3.show(20) \n",
    "    #val rdd = sc.cassandraTable(\"test\", \"words\")\n",
    "    #print(\"Time required for read and prepare dataframe with {} rows using {} partitions in {} seconds.\\n\".format(df3.count(), df3.rdd.getNumPartitions(), round(time() - gt0_bc.value,3)))\n",
    "    if (DTiming):\n",
    "        print(\"Time required for read and prepare dataframe in {} seconds.\\n\".format(round(time() - gt0_bc.value,3)))\n",
    "\n",
    "    return df4\n",
    "\n",
    "    \n",
    "def CreateBlocksDataFrame(dfc, blocksize):\n",
    "    if (DDebug):\n",
    "        print(\"Method: ECreateBlocksData\")\n",
    "\n",
    "    #return CreateBlocksDataFrame2(dfc)   \n",
    "    t1 = time()\n",
    "    \n",
    "    global key_size_bc\n",
    "    keySize = key_size_bc.value\n",
    "    \n",
    "    if (CreateWindowWithPartitions and blocksize>0):\n",
    "        dfc = dfc.withColumn(\"block\", (dfc.offset / blocksize).cast(\"int\"))\n",
    "        my_window = Window.partitionBy(\"block\").orderBy(\"offset\")\n",
    "        if (DDebug):\n",
    "            print(\"Spark shuffle partitions:\".format(sqlContext.getConf(\"spark.sql.shuffle.partitions\")))\n",
    "    else: # Without partitions\n",
    "        my_window = Window.partitionBy().orderBy(\"offset\")\n",
    "       \n",
    "    #dfc.show(20)\n",
    "    #my_window = Window.partitionBy().orderBy(\"offset\")     \n",
    "    #df1 = dfc.select(dfc.value.substr(0,keySize).alias(\"prefix\"))\n",
    "    df0 = dfc.withColumn(\"lines\", upper(dfc.line))\n",
    "    df1 = df0.withColumn(\"prefix\",df0.value.substr(0,keySize-1))\n",
    "    df2 = df1.withColumn(\"next_line\", F.lag(df1.prefix,-1).over(my_window))\n",
    "    df3 = df2.withColumn(\"lines\", F.when(F.isnull(df2.next_line), df2.value) \\\n",
    "                                   .otherwise(F.concat(df2.value, df2.next_line))) \n",
    "    df3 = df3.withColumn(\"size\", F.length(df3.lines)) \n",
    "    #df3.sort(col(\"offset\").asc()).show(10)\n",
    "    df3 = df3.drop(\"prefix\").drop(\"next_line\").drop(\"value\")\n",
    "    #df3.sort(col(\"offset\").asc()).show(10)\n",
    "      \n",
    "    if (DDebug):\n",
    "        print(\"Dataframe done with {} rows using {} partitions in {} seconds.\\n\".format(df3.count(), df3.rdd.getNumPartitions()))\n",
    "    if (DTiming):\n",
    "        print(\"Time required for read and prepare dataframe in {} seconds.\\n\".format(round(time() - t1,3)))\n",
    "    \n",
    "    return df3\n",
    "\n",
    "\n",
    "\n",
    "def write_statistics_single_query(statisticsFileName, queryFilename, referenceName, keySize, date, nquerys, candidates_offsets, n_aligments, partitions, totalTime, readTime, qcassTime, topmTime, extTime):\n",
    "\n",
    "    DStatisticsFileHeader = \"Procedure ; Query Method ; Query File Name ; Reference Name ; Date  ; Number of executors ; Cores/Executor ; Memory/Executor ; Total Time (sec) ; Query Read Time (sec) ; Cassandra query Time (sec) ; Top Matching Time (sec) ; Alig. Extension Time (sec) ; Key Size ; Number of Querys ; Candidate Regions ; Number of Good Aligments ; Number of partitions ; Method ;  ; Block Size ; \"\n",
    "    \n",
    "    executors = sc._conf.get(\"spark.executor.instances\")\n",
    "    cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "    memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "    \n",
    "    new_stats = \"SparkBlast DoQuery ; Single Query ; %s ; %s ; %s ; %s ; %s ; %s ; %f ; %f ; %f ; %f ; %f ; %d ; %d ; %d ; %d ; %d ; %d ; %d ;\" % (queryFilename, referenceName, date, executors, cores, memory, totalTime, readTime, qcassTime, topmTime, extTime, keySize, nquerys, candidates_offsets, n_aligments, partitions, Method, BlockSize)\n",
    "   \n",
    "    print(\"File {} exists? {}\".format(DHdfsHomePath+statisticsFileName, check_file(DHdfsHomePath+statisticsFileName)))\n",
    " \n",
    "    # Generate statistics in hdfs using rdd.\n",
    "    # Read statstics file\n",
    "    if (not check_file(DHdfsHomePath+statisticsFileName)):\n",
    "        new_stats_rdd = sc.parallelize([new_stats],1)\n",
    "    else:\n",
    "        new_stats_rdd = sc.parallelize([DStatisticsFileHeader, new_stats],1)\n",
    "    \n",
    "    global YarnJobId   \n",
    "    OutputFile = DHdfsTmpPath+\"tmp\"+\"_\"+YarnJobId\n",
    "    new_stats_rdd.saveAsTextFile(OutputFile)\n",
    "  \n",
    "    cmd = ['hdfs', 'dfs', '-getmerge',OutputFile+\"/part-*\", \"/tmp/prueba\"]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error getmerge\")\n",
    "    cmd = ['hdfs', 'dfs', '-appendToFile',\"/tmp/prueba\", DHdfsHomePath+statisticsFileName]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error appendToFile\")\n",
    "    cmd = ['hdfs', 'dfs', '-rm -R',OutputFile]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error remove output tmp file\")\n",
    "\n",
    "            \n",
    "    \n",
    "def write_statistics_multiple_query(statisticsFileName, queryFilename, referenceName, keySize, date, nquerys, candidates_offsets, n_aligments, partitions, totalTime, readTime, qcassTime, joinTime, topmTime, extTime):\n",
    "\n",
    "    DStatisticsFileHeader = \"Procedure ; Query Method ; Query File Name ; Reference Name ; Date  ; Number of executors ; Cores/Executor ; Memory/Executor ; Total Time (sec) ; Query Read Time (sec) ; Cassandra query Time (sec) ;  DF Joining Time (sec) ; Top Matching Time (sec) ; Alig. Extension Time (sec) ; Key Size ; Number of Querys ; Candidate Regions ; Number of Good Aligments ; Number of partitions ; Method ;  ; Block Size ; \"\n",
    "    \n",
    "    executors = sc._conf.get(\"spark.executor.instances\")\n",
    "    cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "    memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "    \n",
    "    new_stats = \"SparkBlast DoQuery ; Multiple Query ; %s ; %s ; %s ; %s ; %s ; %s ; %f ; %f ; %f ; %f ; %f ; %f ; %d ; %d ; %d ; %d ; %d ; %d ; %d ;\" % (queryFilename, referenceName, date, executors, cores, memory, totalTime, readTime, qcassTime, joinTime, topmTime, extTime, keySize, nquerys, candidates_offsets, n_aligments, partitions, Method, BlockSize)\n",
    "   \n",
    "    print(\"File {} exists? {}\".format(DHdfsHomePath+statisticsFileName, check_file(DHdfsHomePath+statisticsFileName)))\n",
    " \n",
    "    # Generate statistics in hdfs using rdd.\n",
    "    # Read statstics file\n",
    "    if (not check_file(DHdfsHomePath+statisticsFileName)):\n",
    "        new_stats_rdd = sc.parallelize([new_stats],1)\n",
    "    else:\n",
    "        new_stats_rdd = sc.parallelize([DStatisticsFileHeader, new_stats],1)\n",
    "        \n",
    "    global YarnJobId\n",
    "    OutputFile = DHdfsTmpPath+\"tmp\"+\"_\"+YarnJobId\n",
    "    new_stats_rdd.saveAsTextFile(OutputFile)\n",
    "  \n",
    "    cmd = ['hdfs', 'dfs', '-getmerge',OutputFile+\"/part-*\", \"/tmp/prueba\"]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error getmerge\")\n",
    "    cmd = ['hdfs', 'dfs', '-appendToFile',\"/tmp/prueba\", DHdfsHomePath+statisticsFileName]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error appendToFile\")  \n",
    "    cmd = ['hdfs', 'dfs', '-rm -R',OutputFile]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error remove output tmp file\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def spark_cython(module, method):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        print(\"Entered function with: {}\".format(args))\n",
    "        global cython_function_\n",
    "        try:\n",
    "            return cython_function_(*args, **kwargs)\n",
    "        except:\n",
    "            import pyximport\n",
    "            pyximport.install()\n",
    "            #pyximport.install(build_dir=DCythonLibsPath)\n",
    "            print(\"Cython compilation complete\")\n",
    "            cython_function_ = getattr(__import__(module), method)\n",
    "        print(\"Defined function: {}\".format(cython_function_))\n",
    "        return cython_function_(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def spark_cython2(*args,**kwargs):\n",
    "    global cython_function_\n",
    "    module='do_query'\n",
    "    method='cython_calculateMultipleQueryKeysDesplR'\n",
    "    try:\n",
    "          return cython_function_(*args, **kwargs)\n",
    "    except:\n",
    "        import pyximport\n",
    "        pyximport.install(build_dir=DCythonLibsPath)\n",
    "        cython_function_ = getattr(__import__(module), method)\n",
    "    return cython_function_(*args, **kwargs)\n",
    "\n",
    "    \n",
    "def dfc(functionName, *args):\n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"[{}]----> {} ({}).\".format(time(),functionName,list(args)))\n",
    "    \n",
    "\n",
    "    \n",
    "def run_cmd(args_list):\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE)\n",
    "    proc.communicate()\n",
    "    print(\"Return code: {}\".format(proc.returncode))\n",
    "    return proc.returncode\n",
    "   \n",
    "\n",
    "def check_file(hdfs_file_path):\n",
    "    cmd = ['hdfs', 'dfs', '-test', '-e', hdfs_file_path]\n",
    "    code = run_cmd(cmd)\n",
    "    return code\n",
    "\n",
    "  \n",
    "def remove_file(hdfs_file_path):\n",
    "    cmd = ['hdfs', 'dfs', '-rm', '-R', hdfs_file_path]\n",
    "    code = run_cmd(cmd)\n",
    "    return code\n",
    "\n",
    "\n",
    "def get_size(start_path = '.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def main(sc, sqlContext, queryFilename, referenceName, keySize=DKeySize, DoMultipleQuery=DDoMultipleQuery, HashName=None):\n",
    "    if (DDebug):\n",
    "        print(\"main\")\n",
    "        \n",
    "    global YarnJobId, cyt_calculateMultipleQueryKeysDesplR\n",
    "    try:\n",
    "        a,b,YarnJobId = sc._jsc.sc().applicationId().split('_')\n",
    "    except: \n",
    "        a,YarnJobId = sc._jsc.sc().applicationId().split('-')\n",
    "        \n",
    "    sc.addPyFile(DCythonLibsPath+'do_query.pyx')\n",
    "    \n",
    "    #import pyximport\n",
    "    #pyximport.install(build_dir=DCythonLibsPath)\n",
    "    #import do_query\n",
    "    #cyt_calculateMultipleQueryKeysDesplR = spark_cython('do_query', 'cython_calculateMultipleQueryKeysDesplR')\n",
    "    #print(\"@@@@ cyt_calculateMultipleQueryKeysDesplR result: {}\".format( cyt_calculateMultipleQueryKeysDesplR(\"12345678901234567890\",len(\"12345678901234567890\"), 11)))\n",
    "    \n",
    "    if (not DoMultipleQuery):\n",
    "        Query(sc, sqlContext, queryFilename, referenceName, keySize, HashName)\n",
    "    else:\n",
    "        MultipleQuery(sc, sqlContext, queryFilename, referenceName, keySize, HashName)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#######################\n",
    "## Sequences alignments\n",
    "#######################\n",
    "\n",
    "def DoAligment(querySequence, referenceSequence):\n",
    "    align_seq1,align_seq2,align_score,i_start = Aligment(querySequence, referenceSequence)\n",
    "    return align_seq1,align_seq2,align_score, i_start\n",
    "\n",
    "def Aligment(query_seq, candidate_sequence):\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"Alignment\")\n",
    "        print(\"Cand Seg: {}\".format(candidate_sequence))\n",
    "        print(\"Query Seg: {}\".format(query_seq))\n",
    "    #candidate_seq_pos = finded_postion - query_seq_length + 11 - 5\n",
    "    #candidate_seq_length = query_seq_length + 11\n",
    "    #candidate_sequence = ExtractSeq(chr_index,candidate_seq_pos,candidate_seq_length)\n",
    "    \n",
    "    #query_seq = querySequence\n",
    "    #candidate_sequence = referenceSequence\n",
    "    #query_seq_length = len(query_seq)\n",
    "    #candidate_seq_length = query_seq_length + KeySize\n",
    "    #candidate_seq_pos  = finded_position - query_seq_length + keySize - 5\n",
    "    \n",
    "    i_start_indexs = []\n",
    "    for i_start in range(15):\n",
    "        _,_,score = SMalignment(candidate_sequence[i_start:],query_seq)\n",
    "        i_start_indexs.append(score)\n",
    "    #i_start = np.array(i_start_indexs).argmax()\n",
    "    i_start = i_start_indexs.index(max(i_start_indexs))\n",
    "\n",
    "    i_end_indexs = []\n",
    "    for i_end in range(1,16):\n",
    "        _,_,score = SMalignment(candidate_sequence[:-i_end],query_seq)\n",
    "        i_end_indexs.append(score)\n",
    "    #i_end = np.array(i_end_indexs).argmax()+1\n",
    "    i_end = i_end_indexs.index(max(i_end_indexs))+1\n",
    "    \n",
    "    candidate_sequence = candidate_sequence[i_start:-i_end]\n",
    "    if (DDebug):\n",
    "        print(\"Best aligment {}-{}: {}\".format(i_start, i_end, candidate_sequence))\n",
    "    align_seq1,align_seq2,align_score = SMalignment(candidate_sequence,query_seq)\n",
    "    \n",
    "    return align_seq1, align_seq2, align_score, i_start\n",
    "\n",
    "\n",
    "# compare single base\n",
    "def SingleBaseCompare(seq1,seq2,i,j):\n",
    "    if seq1[i] == seq2[j]:\n",
    "        return 2\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#\n",
    "# Smith-Waterman Alignment\n",
    "#\n",
    "def SMalignment(seq1, seq2):\n",
    "    if (False and DDebug):\n",
    "        print(\"SMalignment\")\n",
    "        print(\"Seg1: {}\".format(seq1))\n",
    "        print(\"Seg2: {}\".format(seq2))\n",
    "    m = len(seq1)\n",
    "    n = len(seq2)\n",
    "    g = -3\n",
    "    matrix = []\n",
    "    for i in range(0, m):\n",
    "        tmp = []\n",
    "        for j in range(0, n):\n",
    "            tmp.append(0)\n",
    "        matrix.append(tmp)\n",
    "    for sii in range(0, m):\n",
    "        matrix[sii][0] = sii*g\n",
    "    for sjj in range(0, n):\n",
    "        matrix[0][sjj] = sjj*g\n",
    "    for siii in range(1, m):\n",
    "        for sjjj in range(1, n):\n",
    "            matrix[siii][sjjj] = max(matrix[siii-1][sjjj] + g, matrix[siii - 1][sjjj - 1] + SingleBaseCompare(seq1,seq2,siii, sjjj), matrix[siii][sjjj-1] + g)\n",
    "    sequ1 = [seq1[m-1]]\n",
    "    sequ2 = [seq2[n-1]]\n",
    "    while m > 1 and n > 1:\n",
    "        if max(matrix[m-1][n-2], matrix[m-2][n-2], matrix[m-2][n-1]) == matrix[m-2][n-2]:\n",
    "            m -= 1\n",
    "            n -= 1\n",
    "            sequ1.append(seq1[m-1])\n",
    "            sequ2.append(seq2[n-1])\n",
    "        elif max(matrix[m-1][n-2], matrix[m-2][n-2], matrix[m-2][n-1]) == matrix[m-1][n-2]:\n",
    "            n -= 1\n",
    "            sequ1.append('-')\n",
    "            sequ2.append(seq2[n-1])\n",
    "        else:\n",
    "            m -= 1\n",
    "            sequ1.append(seq1[m-1])\n",
    "            sequ2.append('-')\n",
    "    sequ1.reverse()\n",
    "    sequ2.reverse()\n",
    "    align_seq1 = ''.join(sequ1)\n",
    "    align_seq2 = ''.join(sequ2)\n",
    "    align_score = 0.\n",
    "    for k in range(0, len(align_seq1)):\n",
    "        if align_seq1[k] == align_seq2[k]:\n",
    "            align_score += 1\n",
    "    align_score = float(align_score)/len(align_seq1)\n",
    "    return align_seq1, align_seq2, align_score\n",
    "\n",
    "\n",
    "# Display BlAST result\n",
    "def Display(seque1, seque2):\n",
    "    le = 40\n",
    "    while len(seque1)-le >= 0:\n",
    "        print('sequence1: ',end='')\n",
    "        for a in list(seque1)[le-40:le]:\n",
    "            print(a,end='')\n",
    "        print(\"\\n\")\n",
    "        print('           ',end='')\n",
    "        for k in range(le-40, le):\n",
    "            if seque1[k] == seque2[k]:\n",
    "                print('|',end='')\n",
    "            else:\n",
    "                print(' ',end='')\n",
    "        print(\"\\n\")\n",
    "        print('sequence2: ',end='')\n",
    "        for b in list(seque2)[le-40:le]:\n",
    "            print(b,end='')\n",
    "        print(\"\\n\")\n",
    "        le += 40\n",
    "    if len(seque1) > le-40:\n",
    "        print('sequence1: ',end='')\n",
    "        for a in list(seque1)[le-40:len(seque1)]:\n",
    "            print(a,end='')\n",
    "        print(\"\\n\")\n",
    "        print('           ',end='')\n",
    "        for k in range(le-40, len(seque1)):\n",
    "            if seque1[k] == seque2[k]:\n",
    "                print('|',end='')\n",
    "            else:\n",
    "                print(' ',end='')\n",
    "        print(\"\\n\")\n",
    "        print('sequence2: ',end='')\n",
    "        for b in list(seque2)[le-40:len(seque2)]:\n",
    "            print(b,end='')\n",
    "        print(\"\\n\")\n",
    "   \n",
    "\n",
    "## Testing \n",
    "\n",
    "if (DDoTesting):    \n",
    "       \n",
    "    # Test 1: Calculate Query's keys & desplazaments (with header line)\n",
    "    print(\"Test 1a: Calculate Query's keys & desplazaments (with header line)\")\n",
    "    queryFilename = '../Datasets/References/Query3.txt'\n",
    "    referenceName = \"example2_r100\"\n",
    "    Method = ECreate2LinesData\n",
    "    #main(sc, sqlContext, queryFilename, referenceName)  \n",
    "    \n",
    "    \n",
    "    # Test 2: Calculate Query's keys & desplazaments (with header line)\n",
    "    print(\"Test 2: Calculate Query's keys & desplazaments (with header line)\")\n",
    "    queryFilename = '../Datasets/References/Query_GRCh38.txt'\n",
    "    referenceName = \"grch38_1m\"\n",
    "    Method = ECreate2LinesData\n",
    "    #main(sc, sqlContext, queryFilename, referenceName.lower())  \n",
    "    \n",
    "\n",
    "    # Test 3: Calculate Multiple Querys \n",
    "    print(\"Test 1a: Calculate Multipe Querys\")\n",
    "    queryFilename = '../Datasets/References/MulQuery1.txt'\n",
    "    referenceName = \"example2_r100\"\n",
    "    #MultipleQuery(sc, sqlContext, queryFilename, referenceName)  \n",
    "    \n",
    "    error\n",
    "\n",
    "## End Testing \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"__Main__\")\n",
    "    \n",
    "    ## Process parameters.\n",
    "    ## SparkBlast_DoQuery <Query_Files> <ReferenceName> [Key_size=11]\n",
    "    if (len(sys.argv)<2):\n",
    "        print(\"Error parametes. Usage: DoQuery [--MQuery] <Query_Files> <ReferenceName> [Key_size=11] [StadisticsFile] [HashName] .\\n\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    KeySize = DKeySize\n",
    "    Method = DDefaultMethod\n",
    "  \n",
    "    if (len(sys.argv)>1 and sys.argv[1].upper()==\"--MQUERY\"):\n",
    "        DoMultipleQuery = True\n",
    "        args=2\n",
    "    else: \n",
    "        DoMultipleQuery = False\n",
    "        args=1\n",
    "        \n",
    "    QueryFilename = sys.argv[args]\n",
    "    ReferenceName = sys.argv[args+1].lower()\n",
    "    HashName=None\n",
    "    if (len(sys.argv)>(args+2)):\n",
    "        KeySize = int(sys.argv[args+2])\n",
    "    if (len(sys.argv)>args+3):\n",
    "        StatisticsFileName = sys.argv[args+3]\n",
    "    if (len(sys.argv)>(args+4)):\n",
    "        HashName = sys.argv[args+4].lower()\n",
    "\n",
    "    ## Configure Spark\n",
    "    conf = SparkConf().setAppName(APP_NAME+ReferenceName)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "    sqlContext = SQLContext(sc)\n",
    "    random.seed()\n",
    "    \n",
    "    t0 = time()\n",
    "    gt0_bc = sc.broadcast(t0)\n",
    "        \n",
    "    # Execute Main functionality\n",
    "    print(\"{}({}, {}, {}, {}).\".format(sys.argv[0], MultipleQuery, QueryFilename, ReferenceName, KeySize, HashName))\n",
    "    main(sc, sqlContext, QueryFilename, ReferenceName, KeySize, DoMultipleQuery, HashName)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
