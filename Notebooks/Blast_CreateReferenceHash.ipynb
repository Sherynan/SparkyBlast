{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,232 [INFO] cassandra.policies: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '192.168.1.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,245 [INFO] cassandra.cluster: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,258 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.6\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,270 [INFO] cassandra.cluster: New Cassandra host <Host: 192.168.1.4 dc1> discovered\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,281 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.5\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n",
      "2019-06-24 10:23:38,293 [WARNING] cassandra.cluster: Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1a: Create reference Hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n",
      "2019-06-24 10:23:42,311 [INFO] root: example2.hash Table Created !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert row 12345678901 0\n",
      "Insert row 23456789012 1\n",
      "Insert row 34567890123 2\n",
      "Insert row 45678901234 3\n",
      "Insert row 56789012345 4\n",
      "Insert row 67890123456 5\n",
      "Insert row 78901234567 6\n",
      "Insert row 89012345678 7\n",
      "Insert row 90123456789 8\n",
      "Insert row 01234567890 9\n",
      "Insert row 12345678901 10\n",
      "Insert row 23456789012 11\n",
      "Insert row 34567890123 12\n",
      "Insert row 45678901234 13\n",
      "Insert row 56789012345 14\n",
      "Insert row 67890123456 15\n",
      "Insert row 78901234567 16\n",
      "Insert row 89012345678 17\n",
      "Insert row 90123456789 18\n",
      "Insert row 01234567890 19\n",
      "Insert row 1234567890A 20\n",
      "Insert row 234567890AB 21\n",
      "Insert row 34567890ABC 22\n",
      "Insert row 4567890ABCD 23\n",
      "Insert row 567890ABCDE 24\n",
      "Insert row 67890ABCDEF 25\n",
      "Insert row 7890ABCDEFG 26\n",
      "Insert row 890ABCDEFGH 27\n",
      "Insert row 90ABCDEFGHI 28\n",
      "Insert row 0ABCDEFGHIJ 29\n",
      "Insert row ABCDEFGHIJK 30\n",
      "Insert row BCDEFGHIJKL 31\n",
      "Insert row CDEFGHIJKLM 32\n",
      "Insert row DEFGHIJKLMN 33\n",
      "Insert row EFGHIJKLMNO 34\n",
      "Insert row FGHIJKLMNOP 35\n",
      "Insert row GHIJKLMNOPQ 36\n",
      "Insert row HIJKLMNOPQR 37\n",
      "Insert row IJKLMNOPQRS 38\n",
      "Insert row JKLMNOPQRST 39\n",
      "Insert row KLMNOPQRSTU 40\n",
      "Insert row LMNOPQRSTUV 41\n",
      "Insert row MNOPQRSTUVW 42\n",
      "Insert row NOPQRSTUVWX 43\n",
      "Insert row OPQRSTUVWXY 44\n",
      "Insert row PQRSTUVWXYZ 45\n",
      "Insert row QRSTUVWXYZA 46\n",
      "Insert row RSTUVWXYZAB 47\n",
      "Insert row STUVWXYZABC 48\n",
      "Insert row TUVWXYZABCD 49\n",
      "Insert row UVWXYZABCDE 50\n",
      "Insert row VWXYZABCDEF 51\n",
      "Insert row WXYZABCDEFG 52\n",
      "Insert row XYZABCDEFGH 53\n",
      "Insert row YZABCDEFGHI 54\n",
      "Insert row ZABCDEFGHIJ 55\n",
      "Insert row ABCDEFGHIJK 56\n",
      "Insert row BCDEFGHIJKL 57\n",
      "Insert row CDEFGHIJKLM 58\n",
      "Insert row DEFGHIJKLMN 59\n",
      "Insert row EFGHIJKLMNO 60\n",
      "Insert row FGHIJKLMNOP 61\n",
      "Insert row GHIJKLMNOPQ 62\n",
      "Insert row HIJKLMNOPQR 63\n",
      "Insert row IJKLMNOPQRS 64\n",
      "Insert row JKLMNOPQRST 65\n",
      "Insert row KLMNOPQRSTU 66\n",
      "Insert row LMNOPQRSTUV 67\n",
      "Insert row MNOPQRSTUVW 68\n",
      "Insert row NOPQRSTUVWX 69\n",
      "Insert row OPQRSTUVWXY 70\n",
      "Insert row PQRSTUVWXYZ 71\n",
      "Insert row QRSTUVWXYZZ 72\n",
      "Insert row RSTUVWXYZZY 73\n",
      "Insert row STUVWXYZZYX 74\n",
      "Insert row TUVWXYZZYXW 75\n",
      "Insert row UVWXYZZYXWV 76\n",
      "Insert row VWXYZZYXWVU 77\n",
      "Insert row WXYZZYXWVUT 78\n",
      "Insert row XYZZYXWVUTS 79\n",
      "Insert row YZZYXWVUTSR 80\n",
      "Insert row ZZYXWVUTSRQ 81\n",
      "Insert row ZYXWVUTSRQP 82\n",
      "Insert row YXWVUTSRQPO 83\n",
      "Insert row XWVUTSRQPON 84\n",
      "Insert row WVUTSRQPONM 85\n",
      "Insert row VUTSRQPONML 86\n",
      "Insert row UTSRQPONMLK 87\n",
      "Insert row TSRQPONMLKJ 88\n",
      "Insert row SRQPONMLKJI 89\n",
      "Insert row RQPONMLKJIH 90\n",
      "Insert row QPONMLKJIHG 91\n",
      "Insert row PONMLKJIHGF 92\n",
      "Insert row ONMLKJIHGFE 93\n",
      "Insert row NMLKJIHGFED 94\n",
      "Insert row MLKJIHGFEDC 95\n",
      "Insert row LKJIHGFEDCB 96\n",
      "Insert row KJIHGFEDCBA 97\n",
      "Insert row JIHGFEDCBAZ 98\n",
      "Insert row IHGFEDCBAZY 99\n",
      "Insert row HGFEDCBAZYX 100\n",
      "Insert row GFEDCBAZYXW 101\n",
      "Insert row FEDCBAZYXWV 102\n",
      "Insert row EDCBAZYXWVU 103\n",
      "Insert row DCBAZYXWVUT 104\n",
      "Insert row CBAZYXWVUTS 105\n",
      "Insert row BAZYXWVUTSR 106\n",
      "Insert row AZYXWVUTSRQ 107\n",
      "Insert row ZYXWVUTSRQP 108\n",
      "Insert row YXWVUTSRQPO 109\n",
      "Insert row XWVUTSRQPON 110\n",
      "Insert row WVUTSRQPONM 111\n",
      "Insert row VUTSRQPONML 112\n",
      "Insert row UTSRQPONMLK 113\n",
      "Insert row TSRQPONMLKJ 114\n",
      "Insert row SRQPONMLKJI 115\n",
      "Insert row RQPONMLKJIH 116\n",
      "Insert row QPONMLKJIHG 117\n",
      "Insert row PONMLKJIHGF 118\n",
      "Insert row ONMLKJIHGFE 119\n",
      "Insert row NMLKJIHGFED 120\n",
      "Insert row MLKJIHGFEDC 121\n",
      "Insert row LKJIHGFEDCB 122\n",
      "Insert row KJIHGFEDCBA 123\n",
      "Insert row JIHGFEDCBA0 124\n",
      "Insert row IHGFEDCBA09 125\n",
      "Insert row HGFEDCBA098 126\n",
      "Insert row GFEDCBA0987 127\n",
      "Insert row FEDCBA09876 128\n",
      "Insert row EDCBA098765 129\n",
      "Insert row DCBA0987654 130\n",
      "Insert row CBA09876543 131\n",
      "Insert row BA098765432 132\n",
      "Insert row A0987654321 133\n",
      "Insert row 09876543210 134\n",
      "Insert row 98765432109 135\n",
      "Insert row 87654321098 136\n",
      "Insert row 76543210987 137\n",
      "Insert row 65432109876 138\n",
      "Insert row 54321098765 139\n",
      "Insert row 43210987654 140\n",
      "Insert row 32109876543 141\n",
      "Insert row 21098765432 142\n",
      "Insert row 10987654321 143\n",
      "Insert row 09876543210 144\n",
      "Insert row 98765432109 145\n",
      "Insert row 87654321098 146\n",
      "Insert row 76543210987 147\n",
      "Insert row 65432109876 148\n",
      "Insert row 54321098765 149\n",
      "Insert row 43210987654 150\n",
      "Insert row 32109876543 151\n",
      "Insert row 21098765432 152\n",
      "Insert row 10987654321 153\n",
      "Insert row 0987654321X 154\n",
      "Insert row 987654321XY 155\n",
      "Insert row 87654321XYZ 156\n",
      "Insert row 7654321XYZA 157\n",
      "Insert row 654321XYZAB 158\n",
      "Insert row 54321XYZABC 159\n",
      "Insert row 4321XYZABCD 160\n",
      "Insert row 321XYZABCDE 161\n",
      "Insert row 21XYZABCDEF 162\n",
      "Insert row 1XYZABCDEFG 163\n",
      "Insert row XYZABCDEFGH 164\n",
      "Insert row YZABCDEFGHI 165\n",
      "Insert row ZABCDEFGHIJ 166\n",
      "Insert row ABCDEFGHIJK 167\n",
      "Insert row BCDEFGHIJKL 168\n",
      "Insert row CDEFGHIJKLM 169\n",
      "Insert row DEFGHIJKLMN 170\n",
      "Insert row EFGHIJKLMNO 171\n",
      "Insert row FGHIJKLMNOP 172\n",
      "Insert row GHIJKLMNOPQ 173\n",
      "Insert row HIJKLMNOPQR 174\n",
      "Insert row IJKLMNOPQRS 175\n",
      "Insert row JKLMNOPQRST 176\n",
      "Insert row KLMNOPQRSTU 177\n",
      "Insert row LMNOPQRSTUV 178\n",
      "Insert row MNOPQRSTUVW 179\n",
      "Insert row NOPQRSTUVWX 180\n",
      "Insert row OPQRSTUVWXY 181\n",
      "Insert row PQRSTUVWXYZ 182\n",
      "Insert row QRSTUVWXYZA 183\n",
      "Insert row RSTUVWXYZAB 184\n",
      "Insert row STUVWXYZABC 185\n",
      "Insert row TUVWXYZABCD 186\n",
      "Insert row UVWXYZABCDE 187\n",
      "Insert row VWXYZABCDEF 188\n",
      "Insert row WXYZABCDEFG 189\n",
      "Insert row XYZABCDEFGH 190\n",
      "Insert row YZABCDEFGHI 191\n",
      "Insert row ZABCDEFGHIJ 192\n",
      "Insert row ABCDEFGHIJK 193\n",
      "Insert row BCDEFGHIJKL 194\n",
      "Insert row CDEFGHIJKLM 195\n",
      "Insert row DEFGHIJKLMN 196\n",
      "Insert row EFGHIJKLMNO 197\n",
      "Insert row FGHIJKLMNOP 198\n",
      "Insert row GHIJKLMNOPQ 199\n",
      "Insert row HIJKLMNOPQR 200\n",
      "Insert row IJKLMNOPQRS 201\n",
      "Insert row JKLMNOPQRST 202\n",
      "Insert row KLMNOPQRSTU 203\n",
      "Insert row LMNOPQRSTUV 204\n",
      "Done\n",
      "Created 205 Keys in 4.239 seconds.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-03bd0a9d4456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateReferenceHashTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "import os, sys, logging\n",
    "from time import time\n",
    "from itertools import cycle\n",
    "from cassandra import ConsistencyLevel\n",
    "from cassandra.concurrent import execute_concurrent\n",
    "from cassandra.cluster import Cluster, BatchStatement\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "\n",
    "DDoTesting = True\n",
    "DDebug = True\n",
    "DReferenceHashTableName = \"hash\"\n",
    "DReferenceContentTableName = \"sequences\"\n",
    "DFileChunkSize = 1000\n",
    "DKeySize = 11\n",
    "DFutureCheck = False\n",
    "DConcurrentRequest = True\n",
    "DNumberConcurrentRequest = 200\n",
    "read_chrs=0\n",
    "read_data=''\n",
    "\n",
    "class SparkBlast_CreateReferenceHash:\n",
    "\n",
    "    def __new__(self):\n",
    "        self.cluster = None\n",
    "        self.session = None\n",
    "        self.keyspace = None\n",
    "        self.log = None\n",
    "        self.future = None\n",
    "        self.Futures = None\n",
    "        self.ReferenceFilename = None\n",
    "        self.ReferenceName = None\n",
    "        self.File = None\n",
    "        self.FileChunkSize = None\n",
    "        self.KeySize = None\n",
    "        self.NumberKeys = None\n",
    "        self.StartTime = None\n",
    "\n",
    "    # parameterized constructor \n",
    "    def __init__(self, referenceFilename, referenceName, keySize): \n",
    "        self.ReferenceFilename = referenceFilename\n",
    "        self.ReferenceName = referenceName\n",
    "        self.KeySize = keySize\n",
    "        self.FileChunkSize = DFileChunkSize\n",
    "        self.StartTime = time()\n",
    "        self.NumberKeys = 0  \n",
    "        self.Futures = []\n",
    "\n",
    "    def __del__(self):\n",
    "        self.session.shutdown()\n",
    "        self.cluster.shutdown()\n",
    "\n",
    "    def CreateCassandraSession(self):\n",
    "        #self.cluster = Cluster(['192.168.1.1', '192.168.1.2', '192.168.1.3', '192.168.1.4', '192.168.1.5', '192.168.1.6'])\n",
    "        self.cluster = Cluster(['192.168.1.1'])\n",
    "        self.session = self.cluster.connect()\n",
    "        #self.session = self.cluster.connect(self.keyspace)\n",
    "\n",
    "    def GetCassandraSession(self):\n",
    "        return self.session\n",
    "\n",
    "    # How about Adding some log info to see what went wrong\n",
    "    def SetLogger(self):\n",
    "        log = logging.getLogger()\n",
    "        log.setLevel('INFO')\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"))\n",
    "        log.addHandler(handler)\n",
    "        self.log = log\n",
    "\n",
    "    def CreateHashTable(self):\n",
    "        self.session.execute(\"CREATE KEYSPACE IF NOT EXISTS \"+ self.ReferenceName +\" WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };\")\n",
    "        #self.session.execute(\"DROP TABLE IF EXISTS \"+ self.ReferenceName +\".\"+ DReferenceHashTableName +\";\")\n",
    "        #self.session.execute(\"CREATE TABLE \"+ self.ReferenceName + \".\"+ DReferenceHashTableName +\" (blockid int, offset int, size int, value text, PRIMARY KEY(blockID));\")\n",
    "        #if (DDebug):\n",
    "        #    self.log.info(self.ReferenceName + \".\"+ DReferenceContentTableName +\" Table Created !!!\")\n",
    "        self.session.execute(\"DROP TABLE IF EXISTS \"+ self.ReferenceName +\".\"+ DReferenceHashTableName +\";\")\n",
    "        self.session.execute(\"CREATE TABLE \"+ self.ReferenceName + \".\"+ DReferenceHashTableName +\" (seq text, value list<text>, PRIMARY KEY(seq));\")\n",
    "        if (DDebug):\n",
    "            self.log.info(self.ReferenceName + \".\"+ DReferenceHashTableName +\" Table Created !!!\")\n",
    "\n",
    "\n",
    "    def ReadFileChunk(self):\n",
    "        \"\"\"Lazy function (generator) to read a file piece by piece.\n",
    "        Default chunk size: 1k.\"\"\"\n",
    "        while True:\n",
    "            data = self.File.read(self.FileChunkSize)\n",
    "            if not data:\n",
    "                break\n",
    "            yield data.replace('\\n','').upper()\n",
    "               \n",
    "\n",
    "    def WriteReferenceHashes(self):\n",
    "        self.File = open(self.ReferenceFilename, 'rt')\n",
    "\n",
    "        # Check for header line\n",
    "        header = self.File.readline()        \n",
    "        if (header and header[0]!='>'):\n",
    "            self.File.seek(0)\n",
    "\n",
    "        offset = 0\n",
    "        rest_prev_chunk = ''\n",
    "        self.InitStatement()\n",
    "        for chunk in self.ReadFileChunk():\n",
    "            chunk_data = rest_prev_chunk+chunk\n",
    "            if (DConcurrentRequest):\n",
    "                self.InsertChunkHashesConcurrent(chunk_data, offset)\n",
    "            else:\n",
    "                self.InsertChunkHashes(chunk_data, offset)\n",
    "            offset = offset + len(chunk_data) - self.KeySize\n",
    "            rest_prev_chunk = chunk[-self.KeySize:]\n",
    "            #print(offset),\n",
    "            \n",
    "        self.File.close()\n",
    "\n",
    "\n",
    "    def InsertChunkHashes(self, chunk_data, offset):\n",
    "        for key in range(0,len(chunk_data)-self.KeySize):\n",
    "            self.InsertHashRow(chunk_data[key:key+self.KeySize],offset+key)\n",
    "            \n",
    "        \n",
    "    def InitStatement(self):\n",
    "        #self.prepared_sql = self.session.prepare(\"UPDATE \"+ self.ReferenceName + \".\"+ DReferenceHashTableName +\" set value = value + [%s] where seq=%s\")\n",
    "        #self.prepared_sql = self.session.prepare('UPDATE example.hash set value = value + [?] where seq=?')\n",
    "        #session.execute(\"update blast.sequences set value = value + [%s] where seq =%s;\", (listText,key))\n",
    "        self.future = None\n",
    "\n",
    "        \n",
    "    def InsertHashRow(self, key, offset):\n",
    "        if (DFutureCheck):\n",
    "            if self.future:\n",
    "                try:\n",
    "                    results = self.future.result()\n",
    "                    self.future = None\n",
    "                except Exception:\n",
    "                    log.exeception()\n",
    "                    print(\"InsertHashRow::Error Checing asincronous insert\")\n",
    "        elif (len(self.Futures)>100):\n",
    "            for query in self.Futures:\n",
    "                try:\n",
    "                    results = query.result()\n",
    "                except Exception:\n",
    "                    log.exeception()\n",
    "                    print(\"InsertHashRow::Error Checing asincronous insert\")            \n",
    "            self.Futures=[]\n",
    "        \n",
    "        #print(\"Insert row {} {}\".format(key,offset))\n",
    "        #self.future = self.session.execute_async(self.prepared_sql.bind(offset, key))\n",
    "        future = self.session.execute_async(\"UPDATE \"+ self.ReferenceName + \".\"+ DReferenceHashTableName + \" set value = value + [%s] where seq=%s\",[str(offset), key])      \n",
    "        if DFutureCheck:\n",
    "            self.future = future\n",
    "        else:\n",
    "            self.Futures.append(future)\n",
    "        if (DDebug):\n",
    "            print(\"Insert row {} {}\".format(key,offset))\n",
    "        self.NumberKeys += 1 \n",
    "        if (self.NumberKeys%1000)==0:\n",
    "            print(\"Processed {} Keys in {} seconds.\".format(self.NumberKeys,round(time() - self.StartTime,3)))\n",
    "\n",
    "\n",
    "    def InsertChunkHashesConcurrent(self, chunk_data, offset):\n",
    "        parameters = [ (chunk_data[key:key+self.KeySize], str(offset+key)) for key in range(0,len(chunk_data)-self.KeySize) ]\n",
    "        self.InsertHashRowConcurrent(parameters)\n",
    "\n",
    "\n",
    "    def InsertHashRowConcurrent(self, query_parameters):                       \n",
    "        update_tatement = SimpleStatement( \"UPDATE \"+ self.ReferenceName + \".\"+ DReferenceHashTableName + \" set value = value + [%s] where seq=%s\",\n",
    "                                            consistency_level=ConsistencyLevel.QUORUM)\n",
    "        statements = cycle((update_tatement, ))\n",
    "\n",
    "        results = execute_concurrent(self.session, list(zip(statements, query_parameters)), concurrency=DNumberConcurrentRequest,  raise_on_first_error=True)\n",
    "        for (success, result) in results:\n",
    "            if (not success):\n",
    "                print(\"InsertHashRowConcurrent::Error in concurrent insert\")\n",
    "\n",
    "        if (DDebug):\n",
    "            for (key,offset) in query_parameters:\n",
    "                print(\"Insert row {} {}\".format(key,offset))\n",
    "\n",
    "        self.NumberKeys += len(query_parameters)\n",
    "        if (self.NumberKeys%1000)==0:\n",
    "            print(\"Processed {} Keys in {} seconds.\".format(self.NumberKeys,round(time() - self.StartTime,3)))\n",
    "\n",
    "\n",
    "    def CreateReferenceHashTable(self):\n",
    "        self.CreateCassandraSession()\n",
    "        self.SetLogger()\n",
    "        self.CreateHashTable()\n",
    "        self.WriteReferenceHashes()\n",
    "        print(\"Done\")\n",
    "        print(\"Created {} Keys in {} seconds.\".format(self.NumberKeys,round(time() - self.StartTime,3)))\n",
    "    \n",
    "        \n",
    "## Testing \n",
    "\n",
    "if (DDoTesting):    \n",
    "       \n",
    "    # Test 1a: Calculate Query's keys & desplazaments (with header line)\n",
    "    print(\"Test 1a: Create reference Hash\")\n",
    "    referenceFilename = '../Datasets/References/Example.txt'\n",
    "    referenceName = str.lower(\"Example2\")\n",
    "    keySize = DKeySize\n",
    "    obj = SparkBlast_CreateReferenceHash(referenceFilename, referenceName, keySize)\n",
    "    obj.CreateReferenceHashTable()\n",
    "    \n",
    "    error\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "      ## Process parameters. (https://docs.python.org/2/library/argparse.html)\n",
    "    ## SparkBlast_CreateReferenceHash <Reference_Files> [Key_size=11] [ReferenceName] [ContentBlockSize=1000]\n",
    "    if (len(sys.argv)<2):\n",
    "        print(\"Error parametes. Usage: SparkBlast_CreateReferenceHash <Reference_Files> [Key_size=11] [ReferenceName=ReferenceFileName].\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    referenceFilename = sys.argv[1]\n",
    "    base = os.path.basename(referenceFilename).lower()\n",
    "    referenceName, ext = os.path.splitext(base)    \n",
    "    keySize = DKeySize\n",
    "    if (len(sys.argv)>2):\n",
    "        keySize = int(sys.argv[2])\n",
    "    if (len(sys.argv)>3):\n",
    "        referenceName = sys.argv[3]\n",
    "    \n",
    "\n",
    "    # Execute Main functionality\n",
    "    print(\"{}({}, {}, {}).\".format(sys.argv[0], referenceFilename, keySize, referenceName))\n",
    "    \n",
    "    obj = SparkBlast_CreateReferenceHash(referenceFilename, referenceName, keySize)\n",
    "    obj.CreateReferenceHashTable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
