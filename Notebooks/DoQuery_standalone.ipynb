{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=SparkBlast_DoQuery/run/user/1010/jupyter/kernel-62caf0ca-9559-42b5-abd3-5049634475b2.json, master=local[*]) created by __init__ at <ipython-input-1-457d45c0023c>:2166 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-86519a5dbb3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Creating PySpark Context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Blast DoQuery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Creating PySpark SQL Context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 314\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=SparkBlast_DoQuery/run/user/1010/jupyter/kernel-62caf0ca-9559-42b5-abd3-5049634475b2.json, master=local[*]) created by __init__ at <ipython-input-1-457d45c0023c>:2166 "
     ]
    }
   ],
   "source": [
    "# Configurations related to Cassandra connector & Cluster\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.1 --conf spark.cassandra.connection.host=192.168.1.4 pyspark-shell '\n",
    "\n",
    "# Creating PySpark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(\"local[*]\", \"Blast DoQuery\")\n",
    "\n",
    "# Creating PySpark SQL Context\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: Calculate Multipe Querys\n",
      "Test 4B: Calculate Multipe Querys\n",
      "MultipleQuery(hdfs://babel.udl.cat///user/nando/Datasets/Sequences/NM_018073.csv.gz, grch38F, 21).\n",
      "%%%%% [1582717831.61]----> MultipleQuery.\n",
      "++++++++++++ INITIAL STATISTICS 02/26/2020 12:50:31 +++++++++++++\n",
      "+ Reference: grch38F  \tQuery file: hdfs://babel.udl.cat///user/nando/Datasets/Sequences/NM_018073.csv.gz.\n",
      "+ Key Size: 21   \tMethod: 1.\n",
      "+ Num Executors: None  \tExecutors/cores: None  \tExecutor Mem: None.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "%%%%% [1582717831.85]----> CalculateMultipleQueryOffset.\n",
      "######################### Query::Input file has 42 sequences and 1 partitions: \n",
      "KeySize used 1: 21\n",
      "@@@@@ Average query size: 80.0714285714\n",
      "@@@@@ Key Matching Threshold: 20\n",
      "######################### Query::Input file has 42 sequences and 1 partitions: \n",
      "KeySize used 2: 21\n",
      "Query::First 10 records: [Row(_c0=u'GCTGCGGGCTACTGGGCCTGCGCTGCCGGGCTTTGGGTTCTGGGCCTCTGCCGCTCTCTGGCCCTAAGTGCTGAGCTGCC ', _c1=1.0, _c2=80.0)]\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      "\n",
      "@@@@@ Number of Partitions: 192\n",
      "######################### Time required for calculate keys in 0.018 seconds.\n",
      "\n",
      "######################### Total time required for processing 42 keys using 192 partitions in 1.08 seconds.\n",
      "######################### Query data size: 0.0 MBytes.\n",
      "\n",
      "@@@@@ 1-Read and split query: 1.617\n",
      "######################### Number of partitions query_sequence: 1\n",
      "######################### Number of partitions keysdespl_rdd: 192\n",
      "Query::QueryKeys:\n",
      "[(1.0, [('GCTGCGGGCTACTGGGCCTGC', 80), ('CTGCGGGCTACTGGGCCTGCG', 79), ('TGCGGGCTACTGGGCCTGCGC', 78), ('GCGGGCTACTGGGCCTGCGCT', 77), ('CGGGCTACTGGGCCTGCGCTG', 76), ('GGGCTACTGGGCCTGCGCTGC', 75), ('GGCTACTGGGCCTGCGCTGCC', 74), ('GCTACTGGGCCTGCGCTGCCG', 73), ('CTACTGGGCCTGCGCTGCCGG', 72), ('TACTGGGCCTGCGCTGCCGGG', 71), ('ACTGGGCCTGCGCTGCCGGGC', 70), ('CTGGGCCTGCGCTGCCGGGCT', 69), ('TGGGCCTGCGCTGCCGGGCTT', 68), ('GGGCCTGCGCTGCCGGGCTTT', 67), ('GGCCTGCGCTGCCGGGCTTTG', 66), ('GCCTGCGCTGCCGGGCTTTGG', 65), ('CCTGCGCTGCCGGGCTTTGGG', 64), ('CTGCGCTGCCGGGCTTTGGGT', 63), ('TGCGCTGCCGGGCTTTGGGTT', 62), ('GCGCTGCCGGGCTTTGGGTTC', 61), ('CGCTGCCGGGCTTTGGGTTCT', 60), ('GCTGCCGGGCTTTGGGTTCTG', 59), ('CTGCCGGGCTTTGGGTTCTGG', 58), ('TGCCGGGCTTTGGGTTCTGGG', 57), ('GCCGGGCTTTGGGTTCTGGGC', 56), ('CCGGGCTTTGGGTTCTGGGCC', 55), ('CGGGCTTTGGGTTCTGGGCCT', 54), ('GGGCTTTGGGTTCTGGGCCTC', 53), ('GGCTTTGGGTTCTGGGCCTCT', 52), ('GCTTTGGGTTCTGGGCCTCTG', 51), ('CTTTGGGTTCTGGGCCTCTGC', 50), ('TTTGGGTTCTGGGCCTCTGCC', 49), ('TTGGGTTCTGGGCCTCTGCCG', 48), ('TGGGTTCTGGGCCTCTGCCGC', 47), ('GGGTTCTGGGCCTCTGCCGCT', 46), ('GGTTCTGGGCCTCTGCCGCTC', 45), ('GTTCTGGGCCTCTGCCGCTCT', 44), ('TTCTGGGCCTCTGCCGCTCTC', 43), ('TCTGGGCCTCTGCCGCTCTCT', 42), ('CTGGGCCTCTGCCGCTCTCTG', 41), ('TGGGCCTCTGCCGCTCTCTGG', 40), ('GGGCCTCTGCCGCTCTCTGGC', 39), ('GGCCTCTGCCGCTCTCTGGCC', 38), ('GCCTCTGCCGCTCTCTGGCCC', 37), ('CCTCTGCCGCTCTCTGGCCCT', 36), ('CTCTGCCGCTCTCTGGCCCTA', 35), ('TCTGCCGCTCTCTGGCCCTAA', 34), ('CTGCCGCTCTCTGGCCCTAAG', 33), ('TGCCGCTCTCTGGCCCTAAGT', 32), ('GCCGCTCTCTGGCCCTAAGTG', 31), ('CCGCTCTCTGGCCCTAAGTGC', 30), ('CGCTCTCTGGCCCTAAGTGCT', 29), ('GCTCTCTGGCCCTAAGTGCTG', 28), ('CTCTCTGGCCCTAAGTGCTGA', 27), ('TCTCTGGCCCTAAGTGCTGAG', 26), ('CTCTGGCCCTAAGTGCTGAGC', 25), ('TCTGGCCCTAAGTGCTGAGCT', 24), ('CTGGCCCTAAGTGCTGAGCTG', 23), ('TGGCCCTAAGTGCTGAGCTGC', 22), ('GGCCCTAAGTGCTGAGCTGCC', 21), ('GCCCTAAGTGCTGAGCTGCC ', 20)])]\n",
      "[Row(_c0=u'GCTGCGGGCTACTGGGCCTGCGCTGCCGGGCTTTGGGTTCTGGGCCTCTGCCGCTCTCTGGCCCTAAGTGCTGAGCTGCC ', _c1=1.0, _c2=80.0)]\n",
      "%%%%% [1582717834.17]----> GetMultipleKeysOffsetsInReference.\n",
      "GetKeysOffsetsInReference::Print all 2523 offsets: ['ACTGGGCCTGCGCTGCCGGGC', 'AACGGCAGCTTCTGACGCTGG', 'ACGGCAGCTTCTGACGCTGGG', 'AGCTTCTGACGCTGGGCCATT', 'ACGCTGGGCCATTGGACGCTG', 'ATTGGACGCTGCGGAACCAGG', 'ACGCTGCGGAACCAGGCTTCT', 'AACCAGGCTTCTTCACTTTGA', 'ACCAGGCTTCTTCACTTTGAG', 'AGGCTTCTTCACTTTGAGTTT', 'ACTTTGAGTTTCCGCCGCGAA', 'AGTCCGGGCCGAGGAGGGAGC', 'AGGAGGGAGCCTTTACTACTT', 'AGGGAGCCTTTACTACTTCTC', 'AGCCTTTACTACTTCTCCCTG', 'ACTACTTCTCCCTGGTTTCAT', 'ACTTCTCCCTGGTTTCATTCA', 'ATTCATGTTCTGAGGAGGGTG', 'ATGTTCTGAGGAGGGTGTGAG', 'AGGAGGGTGTGAGAAGGAACC', 'AGGGTGTGAGAAGGAACCAT ', 'ATCCCACAGCCTTGGTGGAAG', 'ACAGCCTTGGTGGAAGCCATT', 'AGCCTTGGTGGAAGCCATTGT', 'AAGCCATTGTGGAAGAAGTGG', 'AGCCATTGTGGAAGAAGTGGC', 'ATTGTGGAAGAAGTGGCCTGT', 'AAGAAGTGGCCTGTCCCATCT', 'AGAAGTGGCCTGTCCCATCTG', 'AAGTGGCCTGTCCCATCTGTA', 'AGTGGCCTGTCCCATCTGTAT', 'ATCTGTATGACCTTCCTGAGG', 'ATGACCTTCCTGAGGGAGCCC', 'ACCTTCCTGAGGGAGCCCATG', 'ATTGACTGTGGCCACAGCTTC', 'ACTGTGGCCACAGCTTCTGCC', 'ACAGCTTCTGCCACAGCTGTC', 'AGCTTCTGCCACAGCTGTCTC', 'ACAGCTGTCTCTCTGGACTCT', 'AGCTGTCTCTCTGGACTCTGG', 'ACTCTGGGAGATCCCAGGAGA', 'AGATCCCAGGAGAATCCCAGA', 'ATCCCAGGAGAATCCCAGAAC', 'AGGAGAATCCCAGAACTGGGG', 'ACACCTGTCCCCTCTGTCGAG', 'ACCTGTCCCCTCTGTCGAGCT', 'AGCTCCTGTCCAGCCAAGGAA', 'AGCCAAGGAACCTGCGGCCTA', 'AAGGAACCTGCGGCCTAATTG', 'AGGAACCTGCGGCCTAATTGG', 'AACCTGCGGCCTAATTGGCAG', 'ACCTGCGGCCTAATTGGCAGC', 'AATTGGCAGCTGGCCAATGTT', 'ATTGGCAGCTGGCCAATGTTG', 'AGCTGGCCAATGTTGTAGAAA', 'AGTCCGTCTGCTAAGGCTACA', 'AAGGCTACATCCAGGAATGGG', 'AGGCTACATCCAGGAATGGGG', 'ACATCCAGGAATGGGGCTGAA', 'ATCCAGGAATGGGGCTGAAGG', 'AGGAATGGGGCTGAAGGGTGA', 'AATGGGGCTGAAGGGTGACCT', 'ATGGGGCTGAAGGGTGACCTG', 'AAGGGTGACCTGTGTGAGCGC', 'AGGGTGACCTGTGTGAGCGCC', 'ACCTGTGTGAGCGCCATGGGG', 'AGCGCCATGGGGAAAAGCTGA', 'ATGGGGAAAAGCTGAAGATGT', 'AAAGAGGATGTCTTGATAATG', 'AAGAGGATGTCTTGATAATGT', 'AGAGGATGTCTTGATAATGTG', 'AGGATGTCTTGATAATGTGTG', 'ATGTCTTGATAATGTGTGAGG', 'ATAATGTGTGAGGCCTGCAGC', 'AATGTGTGAGGCCTGCAGCCA', 'ATGTGTGAGGCCTGCAGCCAG', 'AGGCCTGCAGCCAGTCCCCAG', 'AGCCAGTCCCCAGAGCATGAG', 'AGTCCCCAGAGCATGAGGCCC', 'AGAGCATGAGGCCCACAGTGT', 'AGCATGAGGCCCACAGTGTTG', 'ATGAGGCCCACAGTGTTGTGC', 'AGGCCCACAGTGTTGTGCCAA', 'AGGATGTTGCCTGGGAGTACA', 'ATGTTGCCTGGGAGTACAAGT', 'AGTACAAGTGGGAACTTCATG', 'ACAAGTGGGAACTTCATGAGG', 'AAGTGGGAACTTCATGAGGCC', 'AGTGGGAACTTCATGAGGCCC', 'AACTTCATGAGGCCCTCGAAC', 'ACTTCATGAGGCCCTCGAACA', 'ATGAGGCCCTCGAACATCTGA', 'AGGCCCTCGAACATCTGAAGA', 'AACATCTGAAGAAAGAGCAAG', 'ACATCTGAAGAAAGAGCAAGA', 'ATCTGAAGAAAGAGCAAGAAG', 'AAGAAAGAGCAAGAAGAGGCC', 'AGAAAGAGCAAGAAGAGGCCT', 'AAAGAGCAAGAAGAGGCCTGG', 'AAGAGCAAGAAGAGGCCTGGA']\n",
      "GetKeysOffsetsInReference::Print distintct 2523 offsets: ['AGGCCCACAGTGTTGTGCCAA', 'ACTACTTCTCCCTGGTTTCAT', 'ATGATTGTCCTGCCTCATGGA', 'AAGGTTGACAAACTCACTTCG', 'AGCAGGCACCGATGAGTACCC', 'ACCTGCGGCCTAATTGGCAGC', 'AAAAAATGCTGCGCATACCCA', 'AGCTGTCTCTCTGGACTCTGG', 'AGGCACCGATGAGTACCCAAT', 'ACCAACAACACTGCTCCTCTG', 'AATTATAAAGCATTTTTCACA', 'ATTGTGGAAGAAGTGGCCTGT', 'ATGGGGAAAAGCTGAAGATGT', 'ACAGGCGCCCACCACGACGCC', 'ACATCTTCACTTTCCCCCGCT', 'ACTGCCAGACAATCCTGAGAG', 'ATGCTGCGCATACCCAGAGCT', 'AATGTGACTGACTGTGGCTCC', 'AATCTTGGAGCTTGCAGCAGC', 'AAGAAAGAGCAAGAAGAGGCC', 'ATTCATGTTCTGAGGAGGGTG', 'AGCTGGCCAATGTTGTAGAAA', 'ATCTCCTGACCTCATGATCCG', 'AACCAACAACACTGCTCCTCT', 'AGATCCCAGATCCATCCCATG', 'AAAGAGCAAGAAGAGGCCTGG', 'AGTGCAGCAGGGGGACACAGA', 'AGCTCCTGTCCAGCCAAGGAA', 'ATGTTAGCTAGGATGGTCTCG', 'ACCAGCGATTACTAGAGAAAA', 'ATCAGAGCAGTTCTAAGGTGA', 'AATGCTGCGCATACCCAGAGC', 'ATTAGATGGGCTTTCCCAATA', 'ACACCAACCAGAAACTGCCAG', 'ATGACCTTCCTGAGGGAGCCC', 'AGGGCTGATGGTGAGGATCAG', 'AGTACAAGTGGGAACTTCATG', 'AGACAGATTGCCGTGTGCTGG', 'AGTTGAAAGAGAGGTCGCAGA', 'AAGCATTTTTCACACTCAAGT', 'AGGATGTCTTGATAATGTGTG', 'ACTTATGCAGCTGATGTGCGC', 'AGCCCAGTATCCAAGGATTCC', 'AAAAATTGTGTTTCTACTTTA', 'ATGGGGCTGAAGGGTGACCTG', 'AGCCGTGGATTTTGCACATGG', 'AAGGAGCTTCAAATGGCTGTG', 'ATGTAGACCGGAAGGAGGTGG', 'ACTTTCCCCCGCTATCCCTTC', 'AGCCATTGTGGAAGAAGTGGC', 'ACCACCCTAACCACAGAGGCT', 'AACTCACTTCGCAGGTCACAT', 'ACTGCAGGCTCTGCCTCCTGG', 'ACAGGTGGAAACCCGAAAACA', 'AGAGGCCTGTCCGCTGGATGT', 'AAGTCTGTCATGAAACCACTT', 'AAATAGGCCAAACTGGAGAAA', 'ACTTTCCCACTGTGCCATGAT', 'AGAAAACGTGTGCACTATGGA', 'AAGCACGACAGGGCTGATGGT', 'AGCCAGTGCATCTCCTCAGGC', 'AAGAGAGGTCGCAGAGGCCTG', 'AACCACAGAGGCTTGGAATTG', 'AGTATCCAAGGATTCCTCTGT', 'ACTTCTCCCTGGTTTCATTCA', 'ACCACCCTCTTCCCTGATTCC', 'AGTACCCAATCCTGTCCTTGC', 'ATCAAAATTATAAAGCATTTT', 'ACCACTTATTTTAAAAAGCAG', 'ATGTGCGCTTGGATCCAGATA', 'AGAGGCTTGGAATTGGGCCTG', 'AGACCATGCAGAAACTGGAGT', 'ACCCTCTTCCCTGATTCCCGT', 'ATTGTGTTTCTACTTTAATTG', 'ATATGATTGTCCTGCCTCATG', 'AGGGAGCAAGTTATTTCTTTT', 'AGTTCAGTCAGTGAGGATGAT', 'AGCTTGCAGCAGCCAGAACCA', 'AGCCTTTACTACTTCTCCCTG', 'ACACTGCTCCTCTGGCCATCT', 'AGCCAACCACCCTCTTCCCTG', 'ACTTTGAGTTTCCGCCGCGAA', 'AGTGTCCTAGTCTTGCCTTTT', 'AATTGTATTGGCTGTTCATGT', 'ACTCACTTCGCAGGTCACATG', 'ACCTCATATGATTGTCCTGCC', 'AGGTGACTCGTTGGGGTAAGG', 'ATAATGTGTGAGGCCTGCAGC', 'AGAGATTTTACCGCTATAATA', 'AGAAGTGGCCTGTCCCATCTG', 'AAGGGAGCAAGTTATTTCTTT', 'AGCTGGGACTACAGGCGCCCA', 'AAATACCAGCGATTACTAGAG', 'AAACGTGTGCACTATGGAGAC', 'ACTGGGAGGTGGAGGTGGGAG', 'AGGCTGAAGTCTGTCATGAAA', 'ACAGCTGTCTCTCTGGACTCT', 'AACACTGCTCCTCTGGCCATC', 'AACTGCCAGACAATCCTGAGA', 'AGGCATAGAGGCAGGCAAGCC', 'AGGAGTTAAAATAGGCCAAAC', 'AGGATCAGAGCAGTTCTAAGG', 'AATCTCCCTGGAGTTGAAGAC', 'ATTGCCGTGTGCTGGGGCTA ', 'ACTGCTCCTCTGGCCATCTGC', 'AAATTGTGTTTCTACTTTAAT', 'AGACCGGAAGGAGGTGGTCTA', 'AACCAGGCTTCTTCACTTTGA', 'ATTCTGGGTGATAAGGCTGAG', 'ATACAGGTGGAAACCCGAAAA', 'ATCCTGAAGACTTATGCAGCT', 'AGTTGAACCATAGCGAGCTCA', 'AGCCTACAGCGGGAGGCAGCG', 'AGACCTGCCCAAGCCACCCCA', 'ACCTGGAAGATACAGGTGGAA', 'ACATCTGAAGAAAGAGCAAGA', 'ACGGAGTTTCACCATGTTAGC', 'AAAAAGCAGCCACCACATCGG', 'AGAGAGGTCGCAGAGGCCTGT', 'AATGAGGAAACTGAAATGGCC', 'AGGTCGCAGAGGCCTGTCCGC', 'AAACAAGGTTGACAAACTCAC', 'AAAGCATTTTTCACACTCAAG', 'AGGATGGTCTCGATCTCCTGA', 'AGTGCATCTCCTCAGGCCGGC', 'AATCCTGTCCTTGCCGGTCCC', 'ATAAAACCTCATATGATTGTC', 'AGTTCTAAGGTGACTCGTTGG', 'ATGTATGTAGGAGTTAAAATA', 'AAAAATGCTGCGCATACCCAG', 'AACAGAGTATTGTATGGGAGT', 'AGCCACCACATCGGCAGCTGG', 'AACGGCAGCTTCTGACGCTGG', 'AACATCTGAAGAAAGAGCAAG', 'ACTGACAGGTATCCCCTGAAA', 'ACCAGGCTTCTTCACTTTGAG', 'AAAAAAATGCTGCGCATACCC', 'ACAGAGTATTGTATGGGAGT ', 'AGGCAGCGGAGACCATGCAGA', 'ATTTGGGTCTGAAACTTCTCA', 'ACGATCTCGGCTCACTGCAGG', 'AGGTATCCCCTGAAACTGAGC', 'AGACGGAGTTTCACCATGTTA', 'AGTCCCCAGAGCATGAGGCCC', 'AGAGGATGTCTTGATAATGTG', 'AGAGACGGAGTTTCACCATGT', 'ACCACGACGCCTGGCTCATTT', 'AGTCAAATGAGCATTGCATCC', 'AGCTGAGCCCAGTATCCAAGG', 'AAATTATAAAGCATTTTTCAC', 'ATCCCAGGAGAATCCCAGAAC', 'AGCCACTGCGCCCGGCCCCTG', 'ATCTCCCTGGAGTTGAAGACA', 'ACGTGTGCACTATGGAGACAC', 'ACAGGGCTGATGGTGAGGATC', 'AGCAACCTTTATGTGACTCTC', 'ACAGCTTCTGCCACAGCTGTC', 'ACCAGGCTGAAGTCTGTCATG', 'ACTCTGGGAGATCCCAGGAGA', 'AAAATTGTGTTTCTACTTTAA', 'AGTGGGAACTTCATGAGGCCC', 'ACCTTCCTGAGGGAGCCCATG', 'AGAAACTGGAGTTGAACCATA', 'AGGATCAGGACTTTGTCCATG', 'AGCGGAGACCATGCAGAAACT', 'AGACAGGTCTGAGTGGGGCCT', 'AGACAATCCTGAGAGATTTTA', 'AATACCAGCGATTACTAGAGA', 'ACTACTGGGAGGTGGAGGTGG', 'AGTCTGTCATGAAACCACTTA', 'AAAGGAGCTTCAAATGGCTGT', 'AGAGAGATCCTGAAGACTTAT', 'ATTGTCCTGCCTCATGGAGCT', 'ACCAATCTCCCTGGAGTTGAA', 'AACAAGGTTGACAAACTCACT', 'AAACTGAAATGGCCTTAAGGG', 'ATCCAGGAATGGGGCTGAAGG', 'ACCATAGCGAGCTCATCCAGC', 'AACCACCCTCTTCCCTGATTC', 'ACTATGGAGACACCAACCAGA', 'ATTAGAGGCATGAGCCACTGC', 'AGAAAGCTACCACCCTAACCA', 'ATTGGCAGCTGGCCAATGTTG', 'AGATCCCAGGAGAATCCCAGA', 'AGTGCAGTGGCACGATCTCGG', 'AAATGCTGCGCATACCCAGAG', 'ACGAACTGCCACCTGGAAGAT', 'ATCCCAGATCCATCCCATGAT', 'AAGGAACCTGCGGCCTAATTG', 'ATGTTTGGGAGGTTTTATGTG', 'ACAGGTCTGAGTGGGGCCTGG', 'AAGCAGCCACCACATCGGCAG', 'ATGATCCGCCCTCCTCAGCCT', 'AGTGAGGATGATGAAGTAGAT', 'AATAGGCCAAAAAAATGCTGC', 'ATAGGCCAAACTGGAGAAATA', 'ACATTTCTTTCTACAATGTGA', 'AGGCCTGTCCGCTGGATGTTG', 'AACTTCATGAGGCCCTCGAAC', 'ATTAAAAATTGTGTTTCTACT', 'ATCCATCCCATGATTTGTTCC', 'AGGAACCTGCGGCCTAATTGG', 'AAAAGCAGAGGCCCAGTCAAA', 'AGGCTACATCCAGGAATGGGG', 'AAAGAGGATGTCTTGATAATG', 'ACAAACTCACTTCGCAGGTCA', 'ACCAGTGTCCTAGTCTTGCCT', 'AGGGGGACACAGACCTGCCCA', 'AGTAGCTGGGACTACAGGCGC', 'ACATCGGCAGCTGGGGGCAGA', 'ACCAGAAACTGCCAGACAATC', 'ACTCGTTGGGGTAAGGATCAG', 'ACTACAGGCGCCCACCACGAC', 'ATTATAAAGCATTTTTCACAC', 'AAAATACCAGCGATTACTAGA', 'AGCAGCCACCACATCGGCAGC', 'ATGGGCTTTCCCAATAGGCCA', 'AGAAACTGCCAGACAATCCTG', 'ACTGACTGTGGCTCCCACATC', 'ACTTATCCCCCCACTATGGAT', 'ACAATGTGACTGACTGTGGCT', 'ACAGATTGCCGTGTGCTGGGG', 'AGACACCAACCAGAAACTGCC', 'AAAATGCTGCGCATACCCAGA', 'AGCATGAGGCCCACAGTGTTG', 'AGTTAAAATAGGCCAAACTGG', 'AGCTGTTGTATAGATCCCAGA', 'AGTCTCACTCTGTTGCCCAGG', 'AACTGAAATGGCCTTAAGGGA', 'ATAACTAAGCAACCTTTATGT', 'ACCCTAACCACAGAGGCTTGG', 'AGTGAAACAAGGTTGACAAAC', 'AGCCAAGGAACCTGCGGCCTA', 'ATCCAGCAGAGCCAGGTCCT ', 'ACAGAGGCTTGGAATTGGGCC', 'ACACAGACCTGCCCAAGCCAC', 'AGGATCAGAAAACCTGCGAG ', 'ACCTGCCCAAGCCACCCCACT', 'ACCTTTATGTGACTCTCACCT', 'AGGACTAAGAAAGCTACCACC', 'ACAATCCTGAGAGATTTTACC', 'AAGGATCAGGACTTTGTCCAT', 'AACAACACTGCTCCTCTGGCC', 'ACCTGTGTGAGCGCCATGGGG', 'ACAGCGGGAGGCAGCGGAGAC', 'AGGATGATGAAGTAGATACAG', 'ATAGCGAGCTCATCCAGCAGA', 'AATGAGCATTGCATCCCATGA', 'AACGCATTCTGTCCACCATGA', 'AACCAATCTCCCTGGAGTTGA', 'AAGGAGGTGGTCTACTTATCC', 'AAGATACAGGTGGAAACCCGA', 'AGGCCCATGACATTTCTTTCT', 'AGCCACTGACAGGTATCCCCT', 'ACGCTGGGCCATTGGACGCTG', 'ACCAACCAGAAACTGCCAGAC', 'AGATCCATCCCATGATTTGTT', 'AGCGGGAGGCAGCGGAGACCA', 'AAACTGGAGTTGAACCATAGC', 'ATTTTGCACATGGTGACTTTC', 'ATGATTGCAGAGTTGAAAGAG', 'ATTTTCACTTTATGAATGAGG', 'ATTTCTTACAAAGCTGCTGAA', 'AAACGAACTGCCACCTGGAAG', 'AGCAAAATGTAGACCGGAAGG', 'AGTACCGAGCAGGCACCGATG', 'AGCTTCAAATGGCTGTGATAA', 'AGTGGCACGATCTCGGCTCAC', 'ATTTTAGTCCTTGCTACAGCA', 'ATTTTACCGCTATAATATCGT', 'AAGGCTACATCCAGGAATGGG', 'AAAGCTGCTGAAGGTAAGATT', 'AAACTCACTTCGCAGGTCACA', 'ATGCAGCTGATGTGCGCTTGG', 'AGGTGGGAGACAGGTCTGAGT', 'AGATGGGCTTTCCCAATAGGC', 'AACCATAGCGAGCTCATCCAG', 'AAAGCAGCCACCACATCGGCA', 'ACAGACCTGCCCAAGCCACCC', 'ACCATGAAAAAAAAAAAAAAA', 'AGCGAGCTCATCCAGCAGAGC', 'AGAAAGAGCAAGAAGAGGCCT', 'AGGCATGAGCCACTGCGCCCG', 'AATGGGGCTGAAGGGTGACCT', 'AAGCTGCTGAAGGTAAGATT ', 'AGGCTCTGCCTCCTGGGTTCA', 'ATGGCCTTAAGGGAGCAAGTT', 'AAGAGGATGTCTTGATAATGT', 'ATCCCCCCACTATGGATTCTG', 'AGGCCCAGTCAAATGAGCATT', 'ACTTCATGAGGCCCTCGAACA', 'AACGTGTGCACTATGGAGACA', 'AGAGCATGAGGCCCACAGTGT', 'ACCCCACTGCCGTTCCCTCTC', 'AAAAAAAAAAAAAAAAAAAA ', 'AGGAATGGGGCTGAAGGGTGA', 'AGTTATTTCTTACAAAGCTGC', 'ATCTCCTCAGGCCGGCACTAC', 'ATCCCACAGCCTTGGTGGAAG', 'ACGACGCCTGGCTCATTTTTT', 'AATGTGTGAGGCCTGCAGCCA', 'ATTGTATTGGCTGTTCATGTA', 'AAAAATACCAGCGATTACTAG', 'ATTTCTTTCTACAATGTGACT', 'AGGAAACGAACTGCCACCTGG', 'AGATCCTGAAGACTTATGCAG', 'ACTCCCGTCTCATCGTGTCTG', 'AGCTTCTGACGCTGGGCCATT', 'AGCAAGTTATTTCTTACAAAG', 'ACCGAGCAGGCACCGATGAGT', 'ATTGGAACCAACAACACTGCT', 'AAAGGAAACGAACTGCCACCT', 'AAGTGGCCTGTCCCATCTGTA', 'AGGAGCAAATCTTGGAGCTTG', 'AAGGATTCCTCTGTCTGATCC', 'ATATTTGGGTCTGAAACTTCT', 'AATGAGTACCGAGCAGGCACC', 'ATGAGCCACTGCGCCCGGCCC', 'ATAGAGGCAGGCAAGCCGTGG', 'ATGAGTACCGAGCAGGCACCG', 'AATGGCCTTAAGGGAGCAAGT', 'AGTCCTTGCTACAGCATTGGA', 'ACTTTATGAATGAGGAAACTG', 'AAAACCTCATATGATTGTCCT', 'ATGGAGACACCAACCAGAAAC', 'ACTGTGGCCACAGCTTCTGCC', 'AGTCCGTCTGCTAAGGCTACA', 'ACCATGCAGAAACTGGAGTTG', 'AGATTTTACCGCTATAATATC', 'AGGATATTCAGGAAGTGTTAA', 'AAGCAGAGGCCCAGTCAAATG', 'AGCTTCTGCCACAGCTGTCTC', 'AGGGCCTCTGCTGGCAGTCTA', 'AGGCGCCCACCACGACGCCTG', 'AAAACAGAGTATTGTATGGGA', 'ATGAGTACCCAATCCTGTCCT', 'AGCAGCCAGAACCAATCTCCC', 'ACACTCAAGTGAAACAAGGTT', 'AGGCTTCTTCACTTTGAGTTT', 'ATTTTAAAAAGCAGAGGCCCA', 'AGCCAGTCCCCAGAGCATGAG', 'ACTGCTTACTCCCGTCTCATC', 'AGGATGCTTCCCCTGAGGTTT', 'ACTTATTATATTTGGGTCTGA', 'ACCATGTTAGCTAGGATGGTC', 'ATTGGGCCTGGCCCCCATGGG', 'AGCCTTGGTGGAAGCCATTGT', 'ATTGCAGAGTTGAAAGAGAGG', 'AGCATTGCATCCCATGAGGG ', 'ATCCGCCCTCCTCAGCCTCCC', 'ACGCCTGGCTCATTTTTTTGT', 'ACCGATGAGTACCCAATCCTG', 'AGTTGGTGAAAGGAAACGAAC', 'ATCCTGAGAGATTTTACCGCT', 'AGTCCGGGCCGAGGAGGGAGC', 'AAATTCAGTGCAGCAGGGGGA', 'AAATGAGCATTGCATCCCATG', 'ACCCAATCCTGTCCTTGCCGG', 'ACCATTAGATGGGCTTTCCCA', 'AGACTTATGCAGCTGATGTGC', 'ATGCTTCCCCTGAGGTTTGCT', 'ACGGCAGCTTCTGACGCTGGG', 'ATCTTGGAGCTTGCAGCAGCC', 'ATCTGAAGAAAGAGCAAGAAG', 'AAGCCATTGTGGAAGAAGTGG', 'ACAGGTATCCCCTGAAACTGA', 'ATGAAAAAAAAAAAAAAAAAA', 'ACATGTTTGGGAGGTTTTATG', 'AGAGGTAGCAGCAGCTCTGGC', 'AGGCCAAAAAAATGCTGCGCA', 'AGCATTTTTCACACTCAAGTG', 'ATTATGAGGCCCATGACATTT', 'AAATCTTGGAGCTTGCAGCAG', 'ACTGTCCCTTGGACTCCAGGA', 'AAGCCACCCCACTGCCGTTCC', 'AGAGTTGAAAGAGAGGTCGCA', 'ATTGGGAAAATGGGTGTAATT', 'AGCAAGTTATTTCTTTTTTTT', 'ACTTTAATTGTATTGGCTGTT', 'ACTAAGCAACCTTTATGTGAC', 'AGAGGCATGAGCCACTGCGCC', 'AAGAAAGCTACCACCCTAACC', 'ACGCATTCTGTCCACCATGAA', 'AAATGGCCTTAAGGGAGCAAG', 'AGCAAATCTTGGAGCTTGCAG', 'ATGGTGAGGATCAGAGCAGTT', 'AACCTTTATGTGACTCTCACC', 'ATGCAGAAACTGGAGTTGAAC', 'ATGGATTCTGGGTGATAAGGC', 'AATTGGCAGCTGGCCAATGTT', 'ATACTGCTTACTCCCGTCTCA', 'AGAGCAGTTCTAAGGTGACTC', 'AGCACGACAGGGCTGATGGTG', 'ATGTTGCAGGATATTCAGGAA', 'AGAAAGGAGCTTCAAATGGCT', 'AAAACGTGTGCACTATGGAGA', 'AGGGTGTGAGAAGGAACCAT ', 'AGGAGAATCCCAGAACTGGGG', 'ATCGGCAGCTGGGGGCAGAGG', 'AAGCCGTGGATTTTGCACATG', 'ACACCTGTCCCCTCTGTCGAG', 'AAAATGTAGACCGGAAGGAGG', 'AGTAGCCAACCACCCTCTTCC', 'ACAGGAGCAAATCTTGGAGCT', 'AAAATAGGCCAAACTGGAGAA', 'AGGTGGTCTACTTATCCCCCC', 'AACCACTTATTTTAAAAAGCA', 'ATGTGTGAGGCCTGCAGCCAG', 'AGCTGGTTGTTGTGCTGAGGC', 'AGCCAGAACCAATCTCCCTGG', 'ACTGGGCCTGCGCTGCCGGGC', 'AATGTAGACCGGAAGGAGGTG', 'ACCACATCGGCAGCTGGGGGC', 'ATGTTCTGAGGAGGGTGTGAG', 'AAGCAACCTTTATGTGACTCT', 'AAACCCGAAAACAGAGTATTG', 'AGAGATCCTGAAGACTTATGC', 'ATGGTGACTTTCCCACTGTGC', 'ACAGCCTTGGTGGAAGCCATT', 'AACTGGAGTTGAACCATAGCG', 'ACTCCAGGAGACCAGTGTCCT', 'ACAGCATTGGAACCAACAACA', 'ATGACATTTCTTTCTACAATG', 'ATGTAGGAGTTAAAATAGGCC', 'ACCCAGAGCTGGTTGTTGTGC', 'AACTAAGCAACCTTTATGTGA', 'ATAAAGCATTTTTCACACTCA', 'AAGGTGACTCGTTGGGGTAAG', 'AAGACTTATGCAGCTGATGTG', 'ACTCAAGTGAAACAAGGTTGA', 'ACATCCAGGAATGGGGCTGAA', 'ACTGTGGCTCCCACATCTTCA', 'AGAGGTCGCAGAGGCCTGTCC', 'ATCCTGTCCTTGCCGGTCCCT', 'ATGAGGCCCATGACATTTCTT', 'AGACCTTTTGGGCAGAAAGGA', 'AACCCGAAAACAGAGTATTGT', 'ATAGATCCCAGATCCATCCCA', 'AGGTCACATGCCTATACATC ', 'ATCCAAGGATTCCTCTGTCTG', 'ACCTTTTGGGCAGAAAGGAGC', 'AGACCAGTGTCCTAGTCTTGC', 'AGGTCTGAGTGGGGCCTGGGA', 'ATAGGCCAAAAAAATGCTGCG', 'ATTGGACGCTGCGGAACCAGG', 'ACTTCTCACATGTTTGGGAGG', 'AGAGCTGGTTGTTGTGCTGAG', 'ACCCGAAAACAGAGTATTGTA', 'AACCTGCGGCCTAATTGGCAG', 'AAGTTATTTCTTACAAAGCTG', 'ATGTGTCCTCATTGGGAAAAT', 'AGAGGATGCTTCCCCTGAGGT', 'ATGAAACCACTTATTTTAAAA', 'AAGCCAGTGCATCTCCTCAGG', 'AGCAGTTCTAAGGTGACTCGT', 'ACCACAGAGGCTTGGAATTGG', 'AACCTCATATGATTGTCCTGC', 'ATCCAGATACTGCTTACTCCC', 'ACTGGAGTTGAACCATAGCGA', 'ACGACAGGGCTGATGGTGAGG', 'AAGAGCAAGAAGAGGCCTGGA', 'AGGCAGGCAAGCCGTGGATTT', 'AGGGTGACCTGTGTGAGCGCC', 'ATCTTCACTTTCCCCCGCTAT', 'AGTTTGTATTTTCACTTTATG', 'AGCCACCCCACTGCCGTTCCC', 'AGGAGCTTCAAATGGCTGTGA', 'AAGACAGATTGCCGTGTGCTG', 'ATGTGACTGACTGTGGCTCCC', 'AGGTTTTATGTGTCCTCATTG', 'AGGAAACTGAAATGGCCTTAA', 'AGATACAGGTGGAAACCCGAA', 'AGGTTGACAAACTCACTTCGC', 'ATTCTCCTGCCTCAGCTTCCC', 'AGTTTCACCATGTTAGCTAGG', 'AGAGGCAGGCAAGCCGTGGAT', 'ATCCCCTGAAACTGAGCTGAG', 'AGAGAAAAAGCAGCCACCACA', 'AATCCTGAGAGATTTTACCGC', 'AGATTGCCGTGTGCTGGGGCT', 'ACATGGTGACTTTCCCACTGT', 'ACAAAGCTGCTGAAGGTAAGA', 'AGCGATTACTAGAGAAAAAGC', 'AAGCAAAATGTAGACCGGAAG', 'AGGAGGGAGCCTTTACTACTT', 'ATGAGGCCCTCGAACATCTGA', 'ACCTGTCCCCTCTGTCGAGCT', 'ATGAGGAAACTGAAATGGCCT', 'ATCTGTATGACCTTCCTGAGG', 'AAGTGAAACAAGGTTGACAAA', 'ATTCTGTCCACCATGAAAAAA', 'AGGCCCTCGAACATCTGAAGA', 'AAACCTCATATGATTGTCCTG', 'AGCTAGGATGGTCTCGATCTC', 'AAATGTAGACCGGAAGGAGGT', 'ATGGAGCTGTTGTATAGATCC', 'AGGATGATTGCAGAGTTGAAA', 'ATTATATTTGGGTCTGAAACT', 'AGTGGCAGGATCAGAAAACCT', 'AGCAGGGGGACACAGACCTGC', 'ACTAGAGAAAAAGCAGCCACC', 'ACCTTCTGACCTCCTGGCAAG', 'ACTCTCACCTTCTGACCTCCT', 'AGATACTGCTTACTCCCGTCT', 'ATTCAGTGCAGCAGGGGGACA', 'AACTTCTCACATGTTTGGGAG', 'AAGAAGTGGCCTGTCCCATCT', 'AAACCACTTATTTTAAAAAGC', 'ATTCCCGTCCGGTGTCACAGT', 'AGCTTCCCGAGTAGCTGGGAC', 'AGCTGATGTGCGCTTGGATCC', 'ATGATACAGTCTGCATCTTAT', 'AAGGAAACGAACTGCCACCTG', 'AGTTATTTCTTTTTTTTTGAC', 'AGGGAGCCTTTACTACTTCTC', 'ACCTCATGATCCGCCCTCCTC', 'AGGACACCATTAGATGGGCTT', 'ACTCTGTTGCCCAGGCTGGAG', 'AGGAGACCAGTGTCCTAGTCT', 'ACTAAGAAAGCTACCACCCTA', 'AGTTGAAGACAGATTGCCGTG', 'AGAGCAAGAAGAGGCCTGGAA', 'AGTGCCTTTGTCTCAGGGCCT', 'ACAAGTGGGAACTTCATGAGG', 'ACTGAAATGGCCTTAAGGGAG', 'ACCGGAAGGAGGTGGTCTACT', 'ACTGCCGTTCCCTCTCTGAGC', 'AGCTGGGGGCAGAGGTAGCAG', 'AGAGGCCCAGTCAAATGAGCA', 'AGGAGGGTGTGAGAAGGAACC', 'ATGAATGAGGAAACTGAAATG', 'AGCAGAGGCCCAGTCAAATGA', 'AAACTGAGCTGAGCCCAGTAT', 'AAATGAGTACCGAGCAGGCAC', 'AAGTGGGAACTTCATGAGGCC', 'AGGCTGGAGTGCAGTGGCACG', 'AGTCTTGCCTTTTTTCTCTAA', 'ACGCTGCGGAACCAGGCTTCT', 'AGGATTCCTCTGTCTGATCCT', 'AAGGGTGACCTGTGTGAGCGC', 'ATACCAGCGATTACTAGAGAA', 'ACAGTTCAGTCAGTGAGGATG', 'ATTACTAGAGAAAAAGCAGCC', 'ATTGGCTGTTCATGTATGTAG', 'ACTGAGCTGAGCCCAGTATCC', 'ACACCATTAGATGGGCTTTCC', 'AGCGCCATGGGGAAAAGCTGA', 'AGGTGGAAACCCGAAAACAGA', 'AGTGGCCTGTCCCATCTGTAT', 'AACGAACTGCCACCTGGAAGA', 'ACTGCGCCCGGCCCCTGGAGC', 'ATGGGGAGGACTAAGAAAGCT', 'ACTTATTTTAAAAAGCAGAGG', 'AGGTGGAGGTGGGAGACAGGT', 'AAACAGAGTATTGTATGGGAG', 'ATGAGGCCCACAGTGTTGTGC', 'AAACTGCCAGACAATCCTGAG', 'ATTCCTCTGTCTGATCCTTTG', 'AATAGGCCAAACTGGAGAAAT', 'ATACCCAGAGCTGGTTGTTGT', 'AAGTTGGTGAAAGGAAACGAA', 'AAAAGCAGCCACCACATCGGC', 'AGTCAGTGAGGATGATGAAGT', 'AACCAGAAACTGCCAGACAAT', 'ATGAGCATTGCATCCCATGAG', 'ACAAGGTTGACAAACTCACTT', 'ATGGTCTCGATCTCCTGACCT', 'AACTGAGCTGAGCCCAGTATC', 'AGGTAGCAGCAGCTCTGGCC ', 'AGCTCATCCAGCAGAGCCAGG', 'AATTGGGCCTGGCCCCCATGG', 'AATTCAGTGCAGCAGGGGGAC', 'AGGATGTTGCCTGGGAGTACA', 'AGCATTGGAACCAACAACACT', 'ATGTGACTCTCACCTTCTGAC', 'AGCTACCACCCTAACCACAGA', 'AAAGAGAGGTCGCAGAGGCCT', 'AAAATTATAAAGCATTTTTCA', 'ACAACACTGCTCCTCTGGCCA', 'AAGTTATTTCTTTTTTTTTGA', 'AATTGTGTTTCTACTTTAATT', 'ATCCCATGATTTGTTCCTGTC', 'AGAACCAATCTCCCTGGAGTT', 'ACTATGGATTCTGGGTGATAA', 'AGGCAAGCCGTGGATTTTGCA', 'AAAAAGCAGAGGCCCAGTCAA', 'AGTCTAGACCTTTTGGGCAGA', 'ATACAGTCTGCATCTTATAG ', 'AAAAAAAAAAAAAAAAAAAAA', 'ATGATGAAGTAGATACAGTCT', 'ATGTCTTGATAATGTGTGAGG', 'AGGCCTGCAGCCAGTCCCCAG', 'AACTGCCACCTGGAAGATACA', 'ATTTTTCACACTCAAGTGAAA', 'ATCTCGGCTCACTGCAGGCTC', 'ACTGCCACCTGGAAGATACAG', 'AGGAGGTGGTCTACTTATCCC', 'AGGTTTGCTATAACTAAGCAA', 'AGAAAAAGCAGCCACCACATC', 'AGGCCGGCACTACTGGGAGGT', 'ATGTTGCCTGGGAGTACAAGT', 'AAGTGGCAGGATCAGAAAACC', 'AAAGCTACCACCCTAACCACA', 'ACTTCGCAGGTCACATGCCTA', 'AGGCTTGGAATTGGGCCTGGC', 'AAGCTACCACCCTAACCACAG', 'ACTGTGCCATGATACAGTCTG', 'AAAGCAGAGGCCCAGTCAAAT', 'ATTGACTGTGGCCACAGCTTC', 'AAACTTCTCACATGTTTGGGA', 'CTGTGTGAGCGCCATGGGGAA', 'CACTGCTCCTCTGGCCATCTG', 'CACTTTGAGTTTCCGCCGCGA', 'CATTTTTCACACTCAAGTGAA', 'CGGAGTTTCACCATGTTAGCT', 'CATGAGGCCCACAGTGTTGTG', 'CGGAAGGAGGTGGTCTACTTA', 'CAAGGAACCTGCGGCCTAATT', 'CATGTTTGGGAGGTTTTATGT', 'CCCACATCTTCACTTTCCCCC', 'CCCACTGCCGTTCCCTCTCTG', 'CTATGGATTCTGGGTGATAAG', 'CAAGTGAAACAAGGTTGACAA', 'CTCAGCTTCCCGAGTAGCTGG', 'CACTATGGATTCTGGGTGATA', 'CCTTCCTGAGGGAGCCCATGA', 'CCCTGGAGCAAGTTATTTCTT', 'CCCCTCTGTCGAGCTCCTGTC', 'CGAGGAGGGAGCCTTTACTAC', 'CTTGGAATTGGGCCTGGCCCC', 'CTCATGATCCGCCCTCCTCAG', 'CTTTCCCCCGCTATCCCTTC ', 'CCGTCTCATCGTGTCTGAGGA', 'CAGGATCAGAAAACCTGCGAG', 'CCACTGTGCCATGATACAGTC', 'CCTAATTGGCAGCTGGCCAAT', 'CCTGCGGCCTAATTGGCAGCT', 'CTCATATGATTGTCCTGCCTC', 'CATCTGTATGACCTTCCTGAG', 'CGCTGCCGGGCTTTGGGTTCT', 'CTTCCCGAGTAGCTGGGACTA', 'CAACAACACTGCTCCTCTGGC', 'CAAATGAGCATTGCATCCCAT', 'CCTCATGGAGCTGTTGTATAG', 'CAGGCAAGCCGTGGATTTTGC', 'CTGAGGTTTGCTATAACTAAG', 'CCACCTGGAAGATACAGGTGG', 'CGCCTCCTGCCCTATTTTAGT', 'CAGCTTCTGCCACAGCTGTCT', 'CACTGCCGTTCCCTCTCTGAG', 'CAAGTTATTTCTTACAAAGCT', 'CTTTATGTGACTCTCACCTTC', 'CCGTCTGCTAAGGCTACATCC', 'CTCTGGACTCTGGGAGATCCC', 'CCTCAGGCCGGCACTACTGGG', 'CCTTTGTCTCAGGGCCTCTGC', 'CACAGCTGTCTCTCTGGACTC', 'CAACCAGAAACTGCCAGACAA', 'CCATCTGTATGACCTTCCTGA', 'CCGAAAACAGAGTATTGTATG', 'CTGTTGCCCAGGCTGGAGTGC', 'CCAAGCCACCCCACTGCCGTT', 'CTACTTTAATTGTATTGGCTG', 'CCTGCCCAAGCCACCCCACTG', 'CACCACGACGCCTGGCTCATT', 'CAATCCTGAGAGATTTTACCG', 'CTGAAATGGCCTTAAGGGAGC', 'CGGAGACCATGCAGAAACTGG', 'CCAGAAACTGCCAGACAATCC', 'CCAGTGCATCTCCTCAGGCCG', 'CTCACATGTTTGGGAGGTTTT', 'CTTGATAATGTGTGAGGCCTG', 'CCTTGCTACAGCATTGGAACC', 'CGGGCTACTGGGCCTGCGCTG', 'CTACAGGCGCCCACCACGACG', 'CTGTTGTATAGATCCCAGATC', 'CAGGCTGGAGTGCAGTGGCAC', 'CTGGCTCATTTTTTTGTATTT', 'CCCGGCCCCTGGAGCAAGTTA', 'CTCAAGTGAAACAAGGTTGAC', 'CACTTATTTTAAAAAGCAGAG', 'CTTTCCCAATAGGCCAAAAAA', 'CCTGCCCTATTTTAGTCCTTG', 'CGACGCCTGGCTCATTTTTTT', 'CCTTTACTACTTCTCCCTGGT', 'CCCTCTTCCCTGATTCCCGTC', 'CTGCCCAAGCCACCCCACTGC', 'CCTCTCTGAGCACAAGCTGGG', 'CTCCTCAGGCCGGCACTACTG', 'CCCTCCTCGCCGGGTGGGAAT', 'CAAATCTTGGAGCTTGCAGCA', 'CCGAGTAGCTGGGACTACAGG', 'CTCGCCGGGTGGGAATCTTCG', 'CGGCAGCTGGGGGCAGAGGTA', 'CAGGTATCCCCTGAAACTGAG', 'CCCAGGAGAATCCCAGAACTG', 'CTCTGTTGCCCAGGCTGGAGT', 'CTTCTGACGCTGGGCCATTGG', 'CTTTTGGGCAGAAAGGAGCTT', 'CTCAGGGCCTCTGCTGGCAGT', 'CCTTTTTTCTCTAAGTGGCAG', 'CTGGAGTTGAAGACAGATTGC', 'CCCAGATCCATCCCATGATTT', 'CCGGCCCCTGGAGCAAGTTAT', 'CTGTCCCCTCTGTCGAGCTCC', 'CTAAGGTGACTCGTTGGGGTA', 'CCAGATCCATCCCATGATTTG', 'CAAAATGTAGACCGGAAGGAG', 'CAGAACCAATCTCCCTGGAGT', 'CTACCACCCTAACCACAGAGG', 'CTCGTTGGGGTAAGGATCAGG', 'CTGGGCCATTGGACGCTGCGG', 'CCAGGCTTCTTCACTTTGAGT', 'CAGGACACCATTAGATGGGCT', 'CTGTCCCATCTGTATGACCTT', 'CCTCTGGCCATCTGCTCCCT ', 'CAGAGTTGAAAGAGAGGTCGC', 'CGTGGATTTTGCACATGGTGA', 'CTTCTGCCACAGCTGTCTCTC', 'CAACACTGCTCCTCTGGCCAT', 'CATTCATGTTCTGAGGAGGGT', 'CATAGAGGCAGGCAAGCCGTG', 'CCACCCCACTGCCGTTCCCTC', 'CTGTGGCTCCCACATCTTCAC', 'CTGCGGCCTAATTGGCAGCTG', 'CATCCCATGATTTGTTCCTGT', 'CTGAGCACAAGCTGGGCAAAT', 'CAGTTCTAAGGTGACTCGTTG', 'CCGCTCTCTGGCCCTAAGTGC', 'CAGCATTGGAACCAACAACAC', 'CAGAGCATGAGGCCCACAGTG', 'CGCTGGGCCATTGGACGCTGC', 'CTGCCTCATGGAGCTGTTGTA', 'CCACTATGGATTCTGGGTGAT', 'CTGAAGTCTGTCATGAAACCA', 'CCCAGAGCTGGTTGTTGTGCT', 'CACGACAGGGCTGATGGTGAG', 'CTCTGTCTGATCCTTTGGTCT', 'CAGCGGGAGGCAGCGGAGACC', 'CACAGCCTTGGTGGAAGCCAT', 'CCCCATGGGGCTTGGAGGAC ', 'CATCTGAAGAAAGAGCAAGAA', 'CTGACTGTGGCTCCCACATCT', 'CGCTGCGGAACCAGGCTTCTT', 'CCATGATACAGTCTGCATCTT', 'CTAAGTGGCAGGATCAGAAAA', 'CTCTGAGCACAAGCTGGGCAA', 'CTGACCTCATGATCCGCCCTC', 'CCCCAGAGCATGAGGCCCACA', 'CCGGTCCCTCCTCGCCGGGTG', 'CATACCCAGAGCTGGTTGTTG', 'CAGAGGCCCAGTCAAATGAGC', 'CCAGGAGAATCCCAGAACTGG', 'CAGTTCAGTCAGTGAGGATGA', 'CCTGAGAGATTTTACCGCTAT', 'CATTCTCCTGCCTCAGCTTCC', 'CAGCTGATGTGCGCTTGGATC', 'CCGGGCTTTGGGTTCTGGGCC', 'CGATCTCGGCTCACTGCAGGC', 'CTGAAGAAAGAGCAAGAAGAG', 'CACTGCAGGCTCTGCCTCCTG', 'CAGAGGCTTGGAATTGGGCCT', 'CTGTGGCCACAGCTTCTGCCA', 'CGATTACTAGAGAAAAAGCAG', 'CGCCGGGTGGGAATCTTCGTG', 'CAAGTGGGAACTTCATGAGGC', 'CATGTTCTGAGGAGGGTGTGA', 'CGATCTCCTGACCTCATGATC', 'CACTTTCCCCCGCTATCCCTT', 'CCATGACATTTCTTTCTACAA', 'CGGAACCAGGCTTCTTCACTT', 'CTGATGTGCGCTTGGATCCAG', 'CAGGCGCCCACCACGACGCCT', 'CTAAGCAACCTTTATGTGACT', 'CAGGTCACATGCCTATACATC', 'CATGTTAGCTAGGATGGTCTC', 'CCCAATCCTGTCCTTGCCGGT', 'CTGGGCCTCTGCCGCTCTCTG', 'CCTGAAACTGAGCTGAGCCCA', 'CAAGGTTGACAAACTCACTTC', 'CGCATACCCAGAGCTGGTTGT', 'CTGGGGGCAGAGGTAGCAGCA', 'CTTTGAGTTTCCGCCGCGAAG', 'CGCCTGGCTCATTTTTTTGTA', 'CTCCTCTGGCCATCTGCTCCC', 'CAGAGGCCTGTCCGCTGGATG', 'CCTAACCACAGAGGCTTGGAA', 'CCAGGCTGAAGTCTGTCATGA', 'CTTATCCCCCCACTATGGATT', 'CTTGGTGGAAGCCATTGTGGA', 'CACGATCTCGGCTCACTGCAG', 'CGGCACTACTGGGAGGTGGAG', 'CTGCAGGCTCTGCCTCCTGGG', 'CTAGAGAAAAAGCAGCCACCA', 'CCCGAGTAGCTGGGACTACAG', 'CGCTGGATGTTGCAGGATATT', 'CAGGTGGAAACCCGAAAACAG', 'CAGCCACCACATCGGCAGCTG', 'CTTTGGGTTCTGGGCCTCTGC', 'CCTGTCCGCTGGATGTTGCAG', 'CTGCGCCCGGCCCCTGGAGCA', 'CTCACTCTGTTGCCCAGGCTG', 'CACACTCAAGTGAAACAAGGT', 'CATTGGACGCTGCGGAACCAG', 'CAGGCTTCTTCACTTTGAGTT', 'CCTGGAGTTGAAGACAGATTG', 'CTGGCCAATGTTGTAGAAAA ', 'CTACAATGTGACTGACTGTGG', 'CTTGGAGCTTGCAGCAGCCAG', 'CTCACCTTCTGACCTCCTGGC', 'CATGATCCGCCCTCCTCAGCC', 'CCCTGGTTTCATTCATGTTCT', 'CGCAGGTCACATGCCTATACA', 'CGAGCTCATCCAGCAGAGCCA', 'CAGGATATTCAGGAAGTGTTA', 'CTCTCTGGCCCTAAGTGCTGA', 'CTCTGGCCCTAAGTGCTGAGC', 'CTGCGCATACCCAGAGCTGGT', 'CTCTGCCGCTCTCTGGCCCTA', 'CCCTCTGTCGAGCTCCTGTCC', 'CGCCCTCCTCAGCCTCCCACA', 'CCTCTGCTGGCAGTCTAGACC', 'CTCCTGACCTCATGATCCGCC', 'CTCCCGTCTCATCGTGTCTGA', 'CTACTGGGAGGTGGAGGTGGG', 'CACTATGGAGACACCAACCAG', 'CCCATGACATTTCTTTCTACA', 'CCAACAACACTGCTCCTCTGG', 'CTGGGACTACAGGCGCCCACC', 'CACTGTGCCATGATACAGTCT', 'CCCCACTATGGATTCTGGGTG', 'CTGTTCATGTATGTAGGAGTT', 'CTCCTCGCCGGGTGGGAATCT', 'CCTGTCCCCTCTGTCGAGCTC', 'CACCATTAGATGGGCTTTCCC', 'CGGTGTCACAGTTCAGTCAGT', 'CCACCCTCTTCCCTGATTCCC', 'CCTGCAGCCAGTCCCCAGAGC', 'CAAGCCACCCCACTGCCGTTC', 'CGTCTCATCGTGTCTGAGGA ', 'CTCACTGCAGGCTCTGCCTCC', 'CCCACAGTGTTGTGCCAATG ', 'CAGAGGATGCTTCCCCTGAGG', 'CTGGGTGATAAGGCTGAGGAA', 'CGCCATGGGGAAAAGCTGAAG', 'CAACCTTTATGTGACTCTCAC', 'CGAGCTCCTGTCCAGCCAAGG', 'CAGCGATTACTAGAGAAAAAG', 'CGACAGGGCTGATGGTGAGGA', 'CACCTTCTGACCTCCTGGCAA', 'CTGGCCCTAAGTGCTGAGCTG', 'CTCTCACCTTCTGACCTCCTG', 'CTTCACTTTGAGTTTCCGCCG', 'CTTATTTTAAAAAGCAGAGGC', 'CAGGCCGGCACTACTGGGAGG', 'CGGGCCGAGGAGGGAGCCTTT', 'CCTGGGCGCCTCCTGCCCTAT', 'CTTGGATCCAGATACTGCTTA', 'CCACATCGGCAGCTGGGGGCA', 'CTGCAAAGAGGATGTCTTGAT', 'CATCTCCTCAGGCCGGCACTA', 'CCGGGTGGGAATCTTCGTGG ', 'CCAACCAGAAACTGCCAGACA', 'CATCTTCACTTTCCCCCGCTA', 'CATTTCTTTCTACAATGTGAC', 'CGAGCCACTGACAGGTATCCC', 'CACCATGAAAAAAAAAAAAAA', 'CCCACTATGGATTCTGGGTGA', 'CCAGTGTCCTAGTCTTGCCTT', 'CTGGTTGTTGTGCTGAGGCCA', 'CTGCCGGGCTTTGGGTTCTGG', 'CCCCACTGCCGTTCCCTCTCT', 'CAGTCCCCAGAGCATGAGGCC', 'CTGACAGGTATCCCCTGAAAC', 'CTCCCACATCTTCACTTTCCC', 'CACCAACCAGAAACTGCCAGA', 'CCCTGAAACTGAGCTGAGCCC', 'CTGTCCACCATGAAAAAAAAA', 'CAGGTCTGAGTGGGGCCTGGG', 'CCACTGACAGGTATCCCCTGA', 'CTGGAGTGCAGTGGCACGATC', 'CCCGTCTCATCGTGTCTGAGG', 'CCTCAGCTTCCCGAGTAGCTG', 'CCCACAGCCTTGGTGGAAGCC', 'CTGGGAGATCCCAGGAGAATC', 'CTTTATGAATGAGGAAACTGA', 'CAGGAGAATCCCAGAACTGGG', 'CCAGGCTGGAGTGCAGTGGCA', 'CATTGGAACCAACAACACTGC', 'CTCATCCAGCAGAGCCAGGTC', 'CATTCTGTCCACCATGAAAAA', 'CCTTTTGGGCAGAAAGGAGCT', 'CGGCCTAATTGGCAGCTGGCC', 'CCAGATACTGCTTACTCCCGT', 'CTGCTAAGGCTACATCCAGGA', 'CGTCTGCTAAGGCTACATCCA', 'CATTGTGGAAGAAGTGGCCTG', 'CACCCTAACCACAGAGGCTTG', 'CACATCGGCAGCTGGGGGCAG', 'CTGGACTCTGGGAGATCCCAG', 'CCTCTGTCGAGCTCCTGTCCA', 'CCAGTCCCCAGAGCATGAGGC', 'CTGGGCCTGCGCTGCCGGGCT', 'CCTTAAGGGAGCAAGTTATTT', 'CTGGAAGATACAGGTGGAAAC', 'CCCAGTATCCAAGGATTCCTC', 'CCCAGTCAAATGAGCATTGCA', 'CCTCCTCGCCGGGTGGGAATC', 'CACAGAGGCTTGGAATTGGGC', 'CTGAAACTGAGCTGAGCCCAG', 'CCAAAAAAATGCTGCGCATAC', 'CAGCGGAGACCATGCAGAAAC', 'CCTGTCCAGCCAAGGAACCTG', 'CATGAAACCACTTATTTTAAA', 'CACTCAAGTGAAACAAGGTTG', 'CCTCATGATCCGCCCTCCTCA', 'CCGTCCGGTGTCACAGTTCAG', 'CCAGTCAAATGAGCATTGCAT', 'CTGGGCGCCTCCTGCCCTATT', 'CAGAAAACGTGTGCACTATGG', 'CAGCTTCCCGAGTAGCTGGGA', 'CTAAGGCTACATCCAGGAATG', 'CTACATCCAGGAATGGGGCTG', 'CTCCTGCCTCAGCTTCCCGAG', 'CTGGATGTTGCAGGATATTCA', 'CCCCCATGGGGCTTGGAGGAC', 'CTTCTGACCTCCTGGCAAGAG', 'CCTGGAAGATACAGGTGGAAA', 'CAGAAACTGGAGTTGAACCAT', 'CTCCCTGGTTTCATTCATGTT', 'CAGTGCATCTCCTCAGGCCGG', 'CACCACATCGGCAGCTGGGGG', 'CACTGTCCCTTGGACTCCAGG', 'CAAGTTATTTCTTTTTTTTTG', 'CTCCCTGGAGTTGAAGACAGA', 'CAATAGGCCAAAAAAATGCTG', 'CCAGAACCAATCTCCCTGGAG', 'CATGGTGACTTTCCCACTGTG', 'CTGAGAGATTTTACCGCTATA', 'CTCTGCTGGCAGTCTAGACCT', 'CACTTCGCAGGTCACATGCCT', 'CCAGAGCTGGTTGTTGTGCTG', 'CATGCAGAAACTGGAGTTGAA', 'CCAGAGCATGAGGCCCACAGT', 'CTTTCTACAATGTGACTGACT', 'CTTTCCCACTGTGCCATGATA', 'CTGTCCTTGCCGGTCCCTCCT', 'CTAGACCTTTTGGGCAGAAAG', 'CCAGACAATCCTGAGAGATTT', 'CCTACAGCGGGAGGCAGCGGA', 'CAGTGCCTTTGTCTCAGGGCC', 'CCTGGTTTCATTCATGTTCTG', 'CATCCAGCAGAGCCAGGTCCT', 'CTTCTTCACTTTGAGTTTCCG', 'CAAGCCGTGGATTTTGCACAT', 'CCCTCGAACATCTGAAGAAAG', 'CCTGTCCCATCTGTATGACCT', 'CACTCTGTTGCCCAGGCTGGA', 'CAGCAGGGGGACACAGACCTG', 'CTGCTGGCAGTCTAGACCTTT', 'CATGGAGCTGTTGTATAGATC', 'CATAGCGAGCTCATCCAGCAG', 'CTCCAGGAGACCAGTGTCCTA', 'CCGGCACTACTGGGAGGTGGA', 'CCCACTGTGCCATGATACAGT', 'CTGCTTACTCCCGTCTCATCG', 'CGCAGAGGCCTGTCCGCTGGA', 'CAAAGCTGCTGAAGGTAAGAT', 'CAAAGAGGATGTCTTGATAAT', 'CCCCCACTATGGATTCTGGGT', 'CTGAAACTTCTCACATGTTTG', 'CTGCGGAACCAGGCTTCTTCA', 'CTCGGCTCACTGCAGGCTCTG', 'CCAAGGATTCCTCTGTCTGAT', 'CTCTTCCCTGATTCCCGTCCG', 'CGATGAGTACCCAATCCTGTC', 'CAGGCACCGATGAGTACCCAA', 'CTGTGCCATGATACAGTCTGC', 'CGTGTGCACTATGGAGACACC', 'CAGTGAGGATGATGAAGTAGA', 'CACTACTGGGAGGTGGAGGTG', 'CCACAGAGGCTTGGAATTGGG', 'CCTCTTCCCTGATTCCCGTCC', 'CATGAAAAAAAAAAAAAAAAA', 'CTCTGGGAGATCCCAGGAGAA', 'CCAACCACCCTCTTCCCTGAT', 'CCGAGCAGGCACCGATGAGTA', 'CCCAGGCTGGAGTGCAGTGGC', 'CCTCGAACATCTGAAGAAAGA', 'CAACCACCCTCTTCCCTGATT', 'CAGGAATGGGGCTGAAGGGTG', 'CTTACAAAGCTGCTGAAGGTA', 'CTGAAGACTTATGCAGCTGAT', 'CTTCCCCTGAGGTTTGCTATA', 'CTGCCGTTCCCTCTCTGAGCA', 'CTCTAAGTGGCAGGATCAGAA', 'CTCATTGGGAAAATGGGTGTA', 'CCCTCTCTGAGCACAAGCTGG', 'CTCTCTGGACTCTGGGAGATC', 'CTAAGAAAGCTACCACCCTAA', 'CTTCCCTGATTCCCGTCCGGT', 'CCTATTTTAGTCCTTGCTACA', 'CTTGCCGGTCCCTCCTCGCCG', 'CCTCTGCCGCTCTCTGGCCCT', 'CCATCCCATGATTTGTTCCTG', 'CATCGGCAGCTGGGGGCAGAG', 'CGTCCGGTGTCACAGTTCAGT', 'CTGAGTGGGGCCTGGGAGTA ', 'CTGAGGAGGGTGTGAGAAGGA', 'CAAAAAAATGCTGCGCATACC', 'CGGGCTTTGGGTTCTGGGCCT', 'CTACTGGGCCTGCGCTGCCGG', 'CCTTGGTGGAAGCCATTGTGG', 'CTGGGAGTACAAGTGGGAACT', 'CCGAGGAGGGAGCCTTTACTA', 'CCACGACGCCTGGCTCATTTT', 'CCCTATTTTAGTCCTTGCTAC', 'CCATAGCGAGCTCATCCAGCA', 'CCTCATTGGGAAAATGGGTGT', 'CACATCTTCACTTTCCCCCGC', 'CCACCCTAACCACAGAGGCTT', 'CCTGACCTCATGATCCGCCCT', 'CATATGATTGTCCTGCCTCAT', 'CCGGTGTCACAGTTCAGTCAG', 'CAGGAGCAAATCTTGGAGCTT', 'CCAGCCAAGGAACCTGCGGCC', 'CAGTGTCCTAGTCTTGCCTTT', 'CCATTGTGGAAGAAGTGGCCT', 'CCAAGGAACCTGCGGCCTAAT', 'CGCTCTCTGGCCCTAAGTGCT', 'CATTGGGAAAATGGGTGTAAT', 'CAGCAGCCAGAACCAATCTCC', 'CCGATGAGTACCCAATCCTGT', 'CCTGGGAGTACAAGTGGGAAC', 'CTGAAGGGTGACCTGTGTGAG', 'CACCATGTTAGCTAGGATGGT', 'CCTGGCTCATTTTTTTGTATT', 'CCCATGATTTGTTCCTGTCT ', 'CCAGGAGACCAGTGTCCTAGT', 'CAGTATCCAAGGATTCCTCTG', 'CCACTTATTTTAAAAAGCAGA', 'CCTGCGCTGCCGGGCTTTGGG', 'CAGTGCAGCAGGGGGACACAG', 'CACCCCACTGCCGTTCCCTCT', 'CTTTAATTGTATTGGCTGTTC', 'CAGCTGGGGGCAGAGGTAGCA', 'CCCCTGAGGTTTGCTATAACT', 'CAGCTGTCTCTCTGGACTCTG', 'CCCCTGAAACTGAGCTGAGCC', 'CTTGCAGCAGCCAGAACCAAT', 'CCTTGGACTCCAGGAGACCAG', 'CCACAGCTTCTGCCACAGCTG', 'CGCTTGGATCCAGATACTGCT', 'CTGAGCTGAGCCCAGTATCCA', 'CTAGTCTTGCCTTTTTTCTCT', 'CTTGCTACAGCATTGGAACCA', 'CTCATGGAGCTGTTGTATAGA', 'CCGTTCCCTCTCTGAGCACAA', 'CGCATTCTGTCCACCATGAAA', 'CCCAATAGGCCAAAAAAATGC', 'CCACTGCCGTTCCCTCTCTGA', 'CCAATCTCCCTGGAGTTGAAG', 'CACAGACCTGCCCAAGCCACC', 'CACAGCTTCTGCCACAGCTGT', 'CTGGAGCAAGTTATTTCTTAC', 'CAATGTGACTGACTGTGGCTC', 'CTGTCATGAAACCACTTATTT', 'CTGCCGCTCTCTGGCCCTAAG', 'CCCTGGAGTTGAAGACAGATT', 'CAGAGGTAGCAGCAGCTCTGG', 'CAAAATTATAAAGCATTTTTC', 'CTAACCACAGAGGCTTGGAAT', 'CAAGGATTCCTCTGTCTGATC', 'CCGCCCTCCTCAGCCTCCCAC', 'CAGGCTGAAGTCTGTCATGAA', 'CCTCTGTCTGATCCTTTGGTC', 'CCCGTCCGGTGTCACAGTTCA', 'CATGATACAGTCTGCATCTTA', 'CTGCCCTATTTTAGTCCTTGC', 'CGGCTCACTGCAGGCTCTGCC', 'CCTTGCCGGTCCCTCCTCGCC', 'CCCCCCACTATGGATTCTGGG', 'CAGCCTTGGTGGAAGCCATTG', 'CTGGGAGGTGGAGGTGGGAGA', 'CCTGCCTCATGGAGCTGTTGT', 'CTACAGCATTGGAACCAACAA', 'CCAGGAATGGGGCTGAAGGGT', 'CACCGATGAGTACCCAATCCT', 'CTGATGGTGAGGATCAGAGCA', 'CTCTGCCTCCTGGGTTCACGC', 'CTGCCACAGCTGTCTCTCTGG', 'CAGTGGCACGATCTCGGCTCA', 'CTGAGCCCAGTATCCAAGGAT', 'CAGAGCAGTTCTAAGGTGACT', 'CATTAGATGGGCTTTCCCAAT', 'CAGCTTCTGACGCTGGGCCAT', 'CAGAGCTGGTTGTTGTGCTGA', 'CGCCCGGCCCCTGGAGCAAGT', 'CTGATTCCCGTCCGGTGTCAC', 'CAGTCTAGACCTTTTGGGCAG', 'CTGTATGACCTTCCTGAGGGA', 'CTTCCTGAGGGAGCCCATGA ', 'CCATTGGACGCTGCGGAACCA', 'CTGCCAGACAATCCTGAGAGA', 'CCTGTCCTTGCCGGTCCCTCC', 'CAGGCTCTGCCTCCTGGGTTC', 'CTTTTTTCTCTAAGTGGCAGG', 'CGTTCCCTCTCTGAGCACAAG', 'CTTCATGAGGCCCTCGAACAT', 'CCCCTGGAGCAAGTTATTTCT', 'CAGATTGCCGTGTGCTGGGGC', 'CTCGATCTCCTGACCTCATGA', 'CAGGAGACCAGTGTCCTAGTC', 'CCTCGCCGGGTGGGAATCTTC', 'CAGCCAGTCCCCAGAGCATGA', 'CGCCCACCACGACGCCTGGCT', 'CACCCTCTTCCCTGATTCCCG', 'CTTATGCAGCTGATGTGCGCT', 'CGGTCCCTCCTCGCCGGGTGG', 'CTGCGCTGCCGGGCTTTGGGT', 'CCATGTTAGCTAGGATGGTCT', 'CCAATAGGCCAAAAAAATGCT', 'CCATGGGGAAAAGCTGAAGAT', 'CCTGAAGACTTATGCAGCTGA', 'CAGACCTGCCCAAGCCACCCC', 'CTTTGTCTCAGGGCCTCTGCT', 'CTATGGAGACACCAACCAGAA', 'CTTGGACTCCAGGAGACCAGT', 'CTGCCACCTGGAAGATACAGG', 'CCCTGAGGTTTGCTATAACTA', 'CTTCACTTTCCCCCGCTATCC', 'CAGTCAGTGAGGATGATGAAG', 'CCACATCTTCACTTTCCCCCG', 'CACTTTATGAATGAGGAAACT', 'CCAGTATCCAAGGATTCCTCT', 'CTCATTTTTTTGTATTTTTA ', 'CACAGTTCAGTCAGTGAGGAT', 'CGAACTGCCACCTGGAAGATA', 'CATGAGCCACTGCGCCCGGCC', 'CGGCAGCTTCTGACGCTGGGC', 'CGGGAGGCAGCGGAGACCATG', 'CAATCTCCCTGGAGTTGAAGA', 'CAGCTGGCCAATGTTGTAGAA', 'CATGGGGAAAAGCTGAAGATG', 'CTGGCAGTCTAGACCTTTTGG', 'CTAATTGGCAGCTGGCCAATG', 'CCAATCCTGTCCTTGCCGGTC', 'CTGCAGCCAGTCCCCAGAGCA', 'CTGGGATTAGAGGCATGAGCC', 'CCTGGAGCAAGTTATTTCTTA', 'CATGACATTTCTTTCTACAAT', 'CCTTCTGACCTCCTGGCAAGA', 'CATGAGGCCCTCGAACATCTG', 'CTTACTCCCGTCTCATCGTGT', 'CCACCACGACGCCTGGCTCAT', 'CTAGGATGGTCTCGATCTCCT', 'CTTAAGGGAGCAAGTTATTTC', 'CCCGAAAACAGAGTATTGTAT', 'CTGGTTTCATTCATGTTCTGA', 'CTTCAAATGGCTGTGATAAGG', 'CTTCTCCCTGGTTTCATTCAT', 'CACATGGTGACTTTCCCACTG', 'CTGACGCTGGGCCATTGGACG', 'CAGCCAGAACCAATCTCCCTG', 'CTGTCCAGCCAAGGAACCTGC', 'CTGTCTGATCCTTTGGTCTTT', 'CACCTGGAAGATACAGGTGGA', 'CCCTTGGACTCCAGGAGACCA', 'CTCTCTGAGCACAAGCTGGGC', 'CCACTGCGCCCGGCCCCTGGA', 'CCCTGATTCCCGTCCGGTGTC', 'CAGTCAAATGAGCATTGCATC', 'CGAAAACAGAGTATTGTATGG', 'CCACAGCCTTGGTGGAAGCCA', 'CACTGACAGGTATCCCCTGAA', 'CCCACCACGACGCCTGGCTCA', 'CTTTACTACTTCTCCCTGGTT', 'CCACAGCTGTCTCTCTGGACT', 'CTCACTTCGCAGGTCACATGC', 'CTTTAGTTTGTATTTTCACTT', 'CACTGCGCCCGGCCCCTGGAG', 'CTCAGGCCGGCACTACTGGGA', 'CTACAGCGGGAGGCAGCGGAG', 'CTATAACTAAGCAACCTTTAT', 'CTCGAACATCTGAAGAAAGAG', 'CATAAAACCTCATATGATTGT', 'CTTATTATATTTGGGTCTGAA', 'CTGGCCCCCATGGGGCTTGGA', 'CTGGAGTTGAACCATAGCGAG', 'CATTGACTGTGGCCACAGCTT', 'CCTGTGTGAGCGCCATGGGGA', 'CACCTGTCCCCTCTGTCGAGC', 'CACGACGCCTGGCTCATTTTT', 'CACATGTTTGGGAGGTTTTAT', 'CTGTCTCTCTGGACTCTGGGA', 'CCGTGGATTTTGCACATGGTG', 'CGGCCCCTGGAGCAAGTTATT', 'CCCAGAGCATGAGGCCCACAG', 'CAAACTCACTTCGCAGGTCAC', 'CCCAAGCCACCCCACTGCCGT', 'CCGGAAGGAGGTGGTCTACTT', 'CATCCAGGAATGGGGCTGAAG', 'CCTGAGGTTTGCTATAACTAA', 'CTCCTGTCCAGCCAAGGAACC', 'CAGGGCCTCTGCTGGCAGTCT', 'CCAGCGATTACTAGAGAAAAA', 'CAGCCAAGGAACCTGCGGCCT', 'CTCTGTCGAGCTCCTGTCCAG', 'CGAACATCTGAAGAAAGAGCA', 'CCTGATTCCCGTCCGGTGTCA', 'CTGTCGAGCTCCTGTCCAGCC', 'CAGGGGGACACAGACCTGCCC', 'CCACCATGAAAAAAAAAAAAA', 'CTTGAAGTTGGTGAAAGGAAA', 'CATGTATGTAGGAGTTAAAAT', 'CTTCTCACATGTTTGGGAGGT', 'CTGTCCGCTGGATGTTGCAGG', 'CCATGCAGAAACTGGAGTTGA', 'CAGAAAGGAGCTTCAAATGGC', 'CAGAAACTGCCAGACAATCCT', 'CCGGGCCGAGGAGGGAGCCTT', 'CAATCCTGTCCTTGCCGGTCC', 'CCATTAGATGGGCTTTCCCAA', 'CGAGCAGGCACCGATGAGTAC', 'CAGTCCGGGCCGAGGAGGGAG', 'CGAGTAGCTGGGACTACAGGC', 'CAAATGGCTGTGATAAGGAA ', 'CCGCTGGATGTTGCAGGATAT', 'CCTCATATGATTGTCCTGCCT', 'CCTCCTGCCCTATTTTAGTCC', 'CTACTTATCCCCCCACTATGG', 'CCTGGCCCCCATGGGGCTTGG', 'CCATGAAAAAAAAAAAAAAAA', 'CGTTGGGGTAAGGATCAGGAC', 'CCACCACATCGGCAGCTGGGG', 'CTGTCCCTTGGACTCCAGGAG', 'CTCCTGCCCTATTTTAGTCCT', 'CAGATCCATCCCATGATTTGT', 'CCTTTATGTGACTCTCACCTT', 'CAGATACTGCTTACTCCCGTC', 'CTGCCTCAGCTTCCCGAGTAG', 'CCCTAACCACAGAGGCTTGGA', 'CTGCTCCTCTGGCCATCTGCT', 'CAGGGCTGATGGTGAGGATCA', 'CTTCGCAGGTCACATGCCTAT', 'CTACTTCTCCCTGGTTTCATT', 'CAGACAATCCTGAGAGATTTT', 'CTGCGGGCTACTGGGCCTGCG', 'CTATTTTAGTCCTTGCTACAG', 'CCTGCCTCAGCTTCCCGAGTA', 'CCTAGTCTTGCCTTTTTTCTC', 'CTTGCCTTTTTTCTCTAAGTG', 'CCCATCTGTATGACCTTCCTG', 'GATCCCAGGAGAATCCCAGAA', 'GACTTATGCAGCTGATGTGCG', 'GGAATTGGGCCTGGCCCCCAT', 'GACGGAGTTTCACCATGTTAG', 'GCAAGTTATTTCTTTTTTTTT', 'GAGGTGGTCTACTTATCCCCC', 'GGCAGCTTCTGACGCTGGGCC', 'GTGGGAACTTCATGAGGCCCT', 'GAGGCAGCGGAGACCATGCAG', 'GTCCGGGCCGAGGAGGGAGCC', 'GAGACAGGTCTGAGTGGGGCC', 'GTCCAGCCAAGGAACCTGCGG', 'GCAGGATCAGAAAACCTGCGA', 'GCTTGCAGCAGCCAGAACCAA', 'GGCCTGTCCGCTGGATGTTGC', 'GAAATGGCCTTAAGGGAGCAA', 'GGCGCCTCCTGCCCTATTTTA', 'GTTGAAAGAGAGGTCGCAGAG', 'GAGGGTGTGAGAAGGAACCAT', 'GAGCTCATCCAGCAGAGCCAG', 'GCCTAATTGGCAGCTGGCCAA', 'GTTTGGGAGGTTTTATGTGTC', 'GTAAGCAAAATGTAGACCGGA', 'GGCAGAAAGGAGCTTCAAATG', 'GCAGCTGGCCAATGTTGTAGA', 'GACCGGAAGGAGGTGGTCTAC', 'GAGGTAGCAGCAGCTCTGGCC', 'GCCACCACATCGGCAGCTGGG', 'GTGTTTCTACTTTAATTGTAT', 'GCAGGCACCGATGAGTACCCA', 'GCTGGGACTACAGGCGCCCAC', 'GTGCATCTCCTCAGGCCGGCA', 'GCTGGGATTAGAGGCATGAGC', 'GATGGTGAGGATCAGAGCAGT', 'GGAACCAGGCTTCTTCACTTT', 'GTTATTTCTTACAAAGCTGCT', 'GAACCAATCTCCCTGGAGTTG', 'GAAACCACTTATTTTAAAAAG', 'GAAAACGTGTGCACTATGGAG', 'GAGACCATGCAGAAACTGGAG', 'GCCTCATGGAGCTGTTGTATA', 'GGCCCTCGAACATCTGAAGAA', 'GCATTGGAACCAACAACACTG', 'GTCTGTCATGAAACCACTTAT', 'GCCCTCGAACATCTGAAGAAA', 'GATTACTAGAGAAAAAGCAGC', 'GAAAACAGAGTATTGTATGGG', 'GCCTACAGCGGGAGGCAGCGG', 'GGAGAATCCCAGAACTGGGGT', 'GAAGTGGCCTGTCCCATCTGT', 'GATCAGGACTTTGTCCATGT ', 'GGGCCTGCGCTGCCGGGCTTT', 'GTCTCGATCTCCTGACCTCAT', 'GTCCACCATGAAAAAAAAAAA', 'GGGCTGATGGTGAGGATCAGA', 'GGATGTTGCCTGGGAGTACAA', 'GTGACCTGTGTGAGCGCCATG', 'GTTCTGGGCCTCTGCCGCTCT', 'GAGTTTCACCATGTTAGCTAG', 'GCTTCCCCTGAGGTTTGCTAT', 'GGAGGTGGAGGTGGGAGACAG', 'GTCTCTCTGGACTCTGGGAGA', 'GGGAAAATGGGTGTAATTCA ', 'GTACAAGTGGGAACTTCATGA', 'GCTCATCCAGCAGAGCCAGGT', 'GCAGGGGGACACAGACCTGCC', 'GGGGCTGAAGGGTGACCTGTG', 'GCCAGTGCATCTCCTCAGGCC', 'GAGGCCCTCGAACATCTGAAG', 'GGCCGAGGAGGGAGCCTTTAC', 'GATGGGCTTTCCCAATAGGCC', 'GGAAGCCATTGTGGAAGAAGT', 'GGAGTTGAACCATAGCGAGCT', 'GGGCCGAGGAGGGAGCCTTTA', 'GTTGACAAACTCACTTCGCAG', 'GCTGCGCATACCCAGAGCTGG', 'GAGGCCCATGACATTTCTTTC', 'GATGATTGCAGAGTTGAAAGA', 'GTGGAGGATGATTGCAGAGTT', 'GATTGTCCTGCCTCATGGAGC', 'GATGGGGAGGACTAAGAAAGC', 'GACAGGTCTGAGTGGGGCCTG', 'GCAGTGGCACGATCTCGGCTC', 'GCCCACAGTGTTGTGCCAATG', 'GATCTCGGCTCACTGCAGGCT', 'GAAAGAGAGGTCGCAGAGGCC', 'GCAGCTGATGTGCGCTTGGAT', 'GAGTACCGAGCAGGCACCGAT', 'GTCATGAAACCACTTATTTTA', 'GGCTTTGGGTTCTGGGCCTCT', 'GCAGAGGCCTGTCCGCTGGAT', 'GGAGCAAGTTATTTCTTTTTT', 'GAAAGAGCAAGAAGAGGCCTG', 'GCTCCCACATCTTCACTTTCC', 'GCTGGTTGTTGTGCTGAGGCC', 'GTGTGAGCGCCATGGGGAAAA', 'GGTCTCGATCTCCTGACCTCA', 'GAAACTGAAATGGCCTTAAGG', 'GTGGAGGTGGGAGACAGGTCT', 'GCAGTTCTAAGGTGACTCGTT', 'GAACCAACAACACTGCTCCTC', 'GGTCTGAAACTTCTCACATGT', 'GTCAAATGAGCATTGCATCCC', 'GACTACAGGCGCCCACCACGA', 'GTAGCTGGGACTACAGGCGCC', 'GTTTCACCATGTTAGCTAGGA', 'GCCTCTGCTGGCAGTCTAGAC', 'GCAGCGGAGACCATGCAGAAA', 'GAAACTTCTCACATGTTTGGG', 'GGGCAGAAAGGAGCTTCAAAT', 'GGTGGTCTACTTATCCCCCCA', 'GATTCCTCTGTCTGATCCTTT', 'GCATAGAGGCAGGCAAGCCGT', 'GATGTCTTGATAATGTGTGAG', 'GCCATTGTGGAAGAAGTGGCC', 'GAAACCCGAAAACAGAGTATT', 'GCAGAGTTGAAAGAGAGGTCG', 'GACAATCCTGAGAGATTTTAC', 'GTCTGAAACTTCTCACATGTT', 'GCGGGAGGCAGCGGAGACCAT', 'GAGCTTGCAGCAGCCAGAACC', 'GCAGCAGCCAGAACCAATCTC', 'GCATGAGCCACTGCGCCCGGC', 'GAGCTGAGCCCAGTATCCAAG', 'GCTTCTGCCACAGCTGTCTCT', 'GTGACTGACTGTGGCTCCCAC', 'GGTGACCTGTGTGAGCGCCAT', 'GACCTTTTGGGCAGAAAGGAG', 'GCTGCGGGCTACTGGGCCTGC', 'GGAAGATACAGGTGGAAACCC', 'GCCTCTGCCGCTCTCTGGCCC', 'GAGGTCGCAGAGGCCTGTCCG', 'GCCCATGACATTTCTTTCTAC', 'GGGAAGCCAGTGCATCTCCTC', 'GGAGCTGTTGTATAGATCCCA', 'GGGTCTGAAACTTCTCACATG', 'GAGCGCCATGGGGAAAAGCTG', 'GTAGAGACGGAGTTTCACCAT', 'GTTGGGGTAAGGATCAGGACT', 'GTGGATTTTGCACATGGTGAC', 'GCCCTAAGTGCTGAGCTGCC ', 'GACAGGGCTGATGGTGAGGAT', 'GAGCAGTTCTAAGGTGACTCG', 'GTGCAGCAGGGGGACACAGAC', 'GCTGGAGTGCAGTGGCACGAT', 'GAGGTGGAGGTGGGAGACAGG', 'GACTCCAGGAGACCAGTGTCC', 'GGAGTACAAGTGGGAACTTCA', 'GGAGTCTCACTCTGTTGCCCA', 'GGCTCATTTTTTTGTATTTTT', 'GATCCTGAAGACTTATGCAGC', 'GCCTGCAGCCAGTCCCCAGAG', 'GAGTTTCCGCCGCGAAGCGC ', 'GCAGGATATTCAGGAAGTGTT', 'GATCAGAGCAGTTCTAAGGTG', 'GGCAGCTGGCCAATGTTGTAG', 'GCCGTGGATTTTGCACATGGT', 'GATCTCCTGACCTCATGATCC', 'GCGCCATGGGGAAAAGCTGAA', 'GCCAGAACCAATCTCCCTGGA', 'GCCTTTGTCTCAGGGCCTCTG', 'GACACCAACCAGAAACTGCCA', 'GAAAGGAAACGAACTGCCACC', 'GGAGGGAGCCTTTACTACTTC', 'GACAAACTCACTTCGCAGGTC', 'GATTGCCGTGTGCTGGGGCTA', 'GGATCCCACAGCCTTGGTGGA', 'GCTGATGTGCGCTTGGATCCA', 'GTGAGGATGATGAAGTAGATA', 'GCCCAGGCTGGAGTGCAGTGG', 'GCCCAAGCCACCCCACTGCCG', 'GCAGCCAGTCCCCAGAGCATG', 'GGCTTCTTCACTTTGAGTTTC', 'GAAGATACAGGTGGAAACCCG', 'GGTGGAAACCCGAAAACAGAG', 'GGTATCCCCTGAAACTGAGCT', 'GACAGATTGCCGTGTGCTGGG', 'GAGGCTTGGAATTGGGCCTGG', 'GGAACGGCAGCTTCTGACGCT', 'GAGCATTGCATCCCATGAGGG', 'GGTCTGAGTGGGGCCTGGGAG', 'GGCTACTGGGCCTGCGCTGCC', 'GCAGGCTCTGCCTCCTGGGTT', 'GGATCCAGATACTGCTTACTC', 'GGCTGAAGTCTGTCATGAAAC', 'GAAAGGAGCTTCAAATGGCTG', 'GGTCCCTCCTCGCCGGGTGGG', 'GGAGGACTAAGAAAGCTACCA', 'GGTAAGGATCAGGACTTTGTC', 'GAGGAAACTGAAATGGCCTTA', 'GGCTTTCCCAATAGGCCAAAA', 'GAGCAGGCACCGATGAGTACC', 'GACTCGTTGGGGTAAGGATCA', 'GAGGCATAGAGGCAGGCAAGC', 'GTGGGAGACAGGTCTGAGTGG', 'GCGCCCACCACGACGCCTGGC', 'GACGCTGCGGAACCAGGCTTC', 'GGGAACGGCAGCTTCTGACGC', 'GAAGACAGATTGCCGTGTGCT', 'GGAATGGGGCTGAAGGGTGAC', 'GGCCCACAGTGTTGTGCCAAT', 'GGCCCCTGGAGCAAGTTATTT', 'GAGGCCCACAGTGTTGTGCCA', 'GTGTGAGGCCTGCAGCCAGTC', 'GCCACTGACAGGTATCCCCTG', 'GTTAAAATAGGCCAAACTGGA', 'GAGCAAGTTATTTCTTACAAA', 'GGTTGACAAACTCACTTCGCA', 'GGATGATGAAGTAGATACAGT', 'GAGCTGGTTGTTGTGCTGAGG', 'GTCTTGATAATGTGTGAGGCC', 'GGCCCATGACATTTCTTTCTA', 'GTCCCCTCTGTCGAGCTCCTG', 'GGGAGGTGGAGGTGGGAGACA', 'GGATTAGAGGCATGAGCCACT', 'GGGAGGACTAAGAAAGCTACC', 'GAGTAGCTGGGACTACAGGCG', 'GGGAGCCTTTACTACTTCTCC', 'GAGCCACTGACAGGTATCCCC', 'GGAGTTTCACCATGTTAGCTA', 'GCAGCCAGAACCAATCTCCCT', 'GGCTGTTCATGTATGTAGGAG', 'GTGCGCTTGGATCCAGATACT', 'GAAGCCAGTGCATCTCCTCAG', 'GCTGCGGAACCAGGCTTCTTC', 'GCGATTACTAGAGAAAAAGCA', 'GCCTGGGAGTACAAGTGGGAA', 'GCCTGGCTCATTTTTTTGTAT', 'GGGTTCTGGGCCTCTGCCGCT', 'GCCTCCTGCCCTATTTTAGTC', 'GTGGAAGAAGTGGCCTGTCCC', 'GGAGGTGGTCTACTTATCCCC', 'GCTAAGGCTACATCCAGGAAT', 'GTGCAGTGGCACGATCTCGGC', 'GCCCCTGGAGCAAGTTATTTC', 'GTGTCACAGTTCAGTCAGTGA', 'GGGTGATAAGGCTGAGGAAGG', 'GAGACGGAGTTTCACCATGTT', 'GAGCAAGTTATTTCTTTTTTT', 'GACTGTGGCCACAGCTTCTGC', 'GGCCATTGGACGCTGCGGAAC', 'GAGAGATCCTGAAGACTTATG', 'GAAAAATACCAGCGATTACTA', 'GAGACCAGTGTCCTAGTCTTG', 'GAAAAAAAAAAAAAAAAAAAA', 'GACTCTGGGAGATCCCAGGAG', 'GTTGAACCATAGCGAGCTCAT', 'GGACACAGACCTGCCCAAGCC', 'GCCGCTCTCTGGCCCTAAGTG', 'GAACATCTGAAGAAAGAGCAA', 'GTCCCTCCTCGCCGGGTGGGA', 'GTTGAAGACAGATTGCCGTGT', 'GCCAGACAATCCTGAGAGATT', 'GCTGGATGTTGCAGGATATTC', 'GCATTTTTCACACTCAAGTGA', 'GATACAGTCTGCATCTTATAG', 'GGCATAGAGGCAGGCAAGCCG', 'GTAAGGATCAGGACTTTGTCC', 'GCTGGGCCATTGGACGCTGCG', 'GCCGGGTGGGAATCTTCGTGG', 'GCTGGCCAATGTTGTAGAAAA', 'GGGAGATCCCAGGAGAATCCC', 'GATGTGCGCTTGGATCCAGAT', 'GAGCCCAGTATCCAAGGATTC', 'GAACTTCATGAGGCCCTCGAA', 'GAAGACTTATGCAGCTGATGT', 'GTGAAACAAGGTTGACAAACT', 'GCACGACAGGGCTGATGGTGA', 'GCACATGGTGACTTTCCCACT', 'GCAGAAAGGAGCTTCAAATGG', 'GGCCGGCACTACTGGGAGGTG', 'GGCTGAAGGGTGACCTGTGTG', 'GGAGGCAGCGGAGACCATGCA', 'GATCCAGATACTGCTTACTCC', 'GCCAAGGAACCTGCGGCCTAA', 'GAGGCCCAGTCAAATGAGCAT', 'GCAGTCTAGACCTTTTGGGCA', 'GGAGTTGAAGACAGATTGCCG', 'GGACGCTGCGGAACCAGGCTT', 'GCAGAAACTGGAGTTGAACCA', 'GCGCCCGGCCCCTGGAGCAAG', 'GGCCTGGCCCCCATGGGGCTT', 'GAGATTTTACCGCTATAATAT', 'GCTACATCCAGGAATGGGGCT', 'GTTCTGAGGAGGGTGTGAGAA', 'GCTTCCCGAGTAGCTGGGACT', 'GGGAGACAGGTCTGAGTGGGG', 'GGTGAGGATCAGAGCAGTTCT', 'GTGAGGATCAGAGCAGTTCTA', 'GGACTCTGGGAGATCCCAGGA', 'GGCCTAATTGGCAGCTGGCCA', 'GGCCCAGTCAAATGAGCATTG', 'GAGATCCCAGGAGAATCCCAG', 'GCGGAGACCATGCAGAAACTG', 'GCTGAAGGGTGACCTGTGTGA', 'GGAGCCTTTACTACTTCTCCC', 'GTTCCCTCTCTGAGCACAAGC', 'GACATTTCTTTCTACAATGTG', 'GTCAGAGGATGCTTCCCCTGA', 'GCAAAGAGGATGTCTTGATAA', 'GAACCTGCGGCCTAATTGGCA', 'GTGCCATGATACAGTCTGCAT', 'GTCCGCTGGATGTTGCAGGAT', 'GCTCTCTGGCCCTAAGTGCTG', 'GTGAGCGCCATGGGGAAAAGC', 'GAGCCACTGCGCCCGGCCCCT', 'GGATATTCAGGAAGTGTTAA ', 'GTTAGCTAGGATGGTCTCGAT', 'GAGATCCTGAAGACTTATGCA', 'GCGCCTCCTGCCCTATTTTAG', 'GAACTGCCACCTGGAAGATAC', 'GTCCTGCCTCATGGAGCTGTT', 'GCACCGATGAGTACCCAATCC', 'GGAGTTAAAATAGGCCAAACT', 'GAGCCTTTACTACTTCTCCCT', 'GGATTTTGCACATGGTGACTT', 'GCAGCTGGGGGCAGAGGTAGC', 'GCTTCAAATGGCTGTGATAAG', 'GACCTGTGTGAGCGCCATGGG', 'GTATAGATCCCAGATCCATCC', 'GAGGTGGGAGACAGGTCTGAG', 'GCCAAAAAAATGCTGCGCATA', 'GCTTGGAATTGGGCCTGGCCC', 'GGGGAGGACTAAGAAAGCTAC', 'GCTCATTTTTTTGTATTTTTA', 'GCAAAATGTAGACCGGAAGGA', 'GCATCTCCTCAGGCCGGCACT', 'GCAGCCACCACATCGGCAGCT', 'GAAGGAGGTGGTCTACTTATC', 'GGTGGGAGACAGGTCTGAGTG', 'GCAAGTTATTTCTTACAAAGC', 'GGAACCAACAACACTGCTCCT', 'GTGGCCACAGCTTCTGCCACA', 'GCAGAGGCCCAGTCAAATGAG', 'GTTGTATAGATCCCAGATCCA', 'GCTTCTTCACTTTGAGTTTCC', 'GGTGACTTTCCCACTGTGCCA', 'GCTGTCTCTCTGGACTCTGGG', 'GCTCACTGCAGGCTCTGCCTC', 'GTCTGCTAAGGCTACATCCAG', 'GAGACACCAACCAGAAACTGC', 'GTCCGTCTGCTAAGGCTACAT', 'GCTTTCCCAATAGGCCAAAAA', 'GGAGCTTCAAATGGCTGTGAT', 'GTTTGCTATAACTAAGCAACC', 'GGGGCAGAGGTAGCAGCAGCT', 'GCGGAACCAGGCTTCTTCACT', 'GTCCGGTGTCACAGTTCAGTC', 'GATCCCAGATCCATCCCATGA', 'GAGCTTCAAATGGCTGTGATA', 'GCACGATCTCGGCTCACTGCA', 'GACTTTCCCACTGTGCCATGA', 'GTTGGTGAAAGGAAACGAACT', 'GAGGAGGGAGCCTTTACTACT', 'GGGGTAAGGATCAGGACTTTG', 'GCATTGACTGTGGCCACAGCT', 'GTTATTTCTTTTTTTTTGACA', 'GCCTTGGTGGAAGCCATTGTG', 'GGTTCTGGGCCTCTGCCGCTC', 'GGTTTGCTATAACTAAGCAAC', 'GATCCGCCCTCCTCAGCCTCC', 'GGTGTCACAGTTCAGTCAGTG', 'GAAACGAACTGCCACCTGGAA', 'GATTTTGCACATGGTGACTTT', 'GCAGGCAAGCCGTGGATTTTG', 'GACTGACTGTGGCTCCCACAT', 'GCCAACCACCCTCTTCCCTGA', 'GGACACCATTAGATGGGCTTT', 'GTCCTTGCCGGTCCCTCCTCG', 'GATTCTGGGTGATAAGGCTGA', 'GGGAGGCAGCGGAGACCATGC', 'GAGCAAATCTTGGAGCTTGCA', 'GTGCTGGGATTAGAGGCATGA', 'GGCCCCCATGGGGCTTGGAGG', 'GGCAGCGGAGACCATGCAGAA', 'GTGTCCTAGTCTTGCCTTTTT', 'GACGCTGGGCCATTGGACGCT', 'GCCACCCCACTGCCGTTCCCT', 'GAGTACAAGTGGGAACTTCAT', 'GCAACCTTTATGTGACTCTCA', 'GTGACTTTCCCACTGTGCCAT', 'GGAGCAAGTTATTTCTTACAA', 'GTGCCTTTGTCTCAGGGCCTC', 'GGACTACAGGCGCCCACCACG', 'GCTTGAAGTTGGTGAAAGGAA', 'GGCCAAAAAAATGCTGCGCAT', 'GTTCTAAGGTGACTCGTTGGG', 'GTGGTCTACTTATCCCCCCAC', 'GGCCTGTCCCATCTGTATGAC', 'GACCAGTGTCCTAGTCTTGCC', 'GCGGCCTAATTGGCAGCTGGC', 'GTGGCAGGATCAGAAAACCTG', 'GATAATGTGTGAGGCCTGCAG', 'GCCACAGCTTCTGCCACAGCT', 'GGAACCTGCGGCCTAATTGGC', 'GACCTCATGATCCGCCCTCCT', 'GGAGACCATGCAGAAACTGGA', 'GAGGAGGGTGTGAGAAGGAAC', 'GCCTGGCCCCCATGGGGCTTG', 'GTGACTCGTTGGGGTAAGGAT', 'GCCATGGGGAAAAGCTGAAGA', 'GGATGTTGCAGGATATTCAGG', 'GAAGTTGGTGAAAGGAAACGA', 'GGATGATTGCAGAGTTGAAAG', 'GGATTCCTCTGTCTGATCCTT', 'GCTGTTGTATAGATCCCAGAT', 'GGAGCAAATCTTGGAGCTTGC', 'GGTGAAAGGAAACGAACTGCC', 'GGCATGAGCCACTGCGCCCGG', 'GGAAGAAGTGGCCTGTCCCAT', 'GAGGTTTTATGTGTCCTCATT', 'GGCAGCTGGGGGCAGAGGTAG', 'GGATCAGAGCAGTTCTAAGGT', 'GAGTTGAAAGAGAGGTCGCAG', 'GGACTCCAGGAGACCAGTGTC', 'GCTCCTGTCCAGCCAAGGAAC', 'GGGCGCCTCCTGCCCTATTTT', 'GAGTGCAGTGGCACGATCTCG', 'GGGCAGAGGTAGCAGCAGCTC', 'GCCTGTCCCATCTGTATGACC', 'GCCCTCCTCAGCCTCCCACA ', 'GTTGCCCAGGCTGGAGTGCAG', 'GAGGATGTTGCCTGGGAGTAC', 'GACGCCTGGCTCATTTTTTTG', 'GTAGCCAACCACCCTCTTCCC', 'GAGAATCCCAGAACTGGGGT ', 'GCCCAGTCAAATGAGCATTGC', 'GTGGAAGCCATTGTGGAAGAA', 'GGCAGAGGTAGCAGCAGCTCT', 'GGCTCCCACATCTTCACTTTC', 'GGGCCTCTGCTGGCAGTCTAG', 'GCCCGGCCCCTGGAGCAAGTT', 'GGCTCACTGCAGGCTCTGCCT', 'GGGACACAGACCTGCCCAAGC', 'GGGAGCAAGTTATTTCTTTTT', 'GAAACTGGAGTTGAACCATAG', 'GGGTAAGGATCAGGACTTTGT', 'GCCGTTCCCTCTCTGAGCACA', 'GTGGCCTGTCCCATCTGTATG', 'GAAAAAGCAGCCACCACATCG', 'GACCTGCCCAAGCCACCCCAC', 'GCCTGTCCGCTGGATGTTGCA', 'GACTGTGGCTCCCACATCTTC', 'GGAAGGAGGTGGTCTACTTAT', 'GGCAGGATCAGAAAACCTGCG', 'GAGTTGAACCATAGCGAGCTC', 'GATGGTCTCGATCTCCTGACC', 'GGAGGTGGGAGACAGGTCTGA', 'GATGATGAAGTAGATACAGTC', 'GGCTTGGAATTGGGCCTGGCC', 'GCAGAGGTAGCAGCAGCTCTG', 'GCTTCTGACGCTGGGCCATTG', 'GGCCACAGCTTCTGCCACAGC', 'GATACAGGTGGAAACCCGAAA', 'GGATTCTGGGTGATAAGGCTG', 'GAATTGGGCCTGGCCCCCATG', 'GGGCTTTCCCAATAGGCCAAA', 'GGCCTCTGCCGCTCTCTGGCC', 'GAACCATAGCGAGCTCATCCA', 'GAGTTGAAGACAGATTGCCGT', 'GATGTTGCCTGGGAGTACAAG', 'GGCTACATCCAGGAATGGGGC', 'GAGAAAAAGCAGCCACCACAT', 'GCCGAGGAGGGAGCCTTTACT', 'GAGGCATGAGCCACTGCGCCC', 'GTTTCTACTTTAATTGTATTG', 'GTTCATGTATGTAGGAGTTAA', 'GGCGCCCACCACGACGCCTGG', 'GAGGCAGGCAAGCCGTGGATT', 'GTCTCACTCTGTTGCCCAGGC', 'GCACTACTGGGAGGTGGAGGT', 'GTACCGAGCAGGCACCGATGA', 'GAAGCCATTGTGGAAGAAGTG', 'GCTACTGGGCCTGCGCTGCCG', 'GCTTGGATCCAGATACTGCTT', 'GCTTACTCCCGTCTCATCGTG', 'GGAGGTTTTATGTGTCCTCAT', 'GAGGATGATTGCAGAGTTGAA', 'GGCTGGAGTGCAGTGGCACGA', 'GTGACTCTCACCTTCTGACCT', 'GCCGGCACTACTGGGAGGTGG', 'GTTTTATGTGTCCTCATTGGG', 'GAGGATGATGAAGTAGATACA', 'GCATAAAACCTCATATGATTG', 'GCCCCCATGGGGCTTGGAGGA', 'GTGAGGCCTGCAGCCAGTCCC', 'GAGGTTTGCTATAACTAAGCA', 'GAAGGGTGACCTGTGTGAGCG', 'GGCAGTCTAGACCTTTTGGGC', 'GTATCCAAGGATTCCTCTGTC', 'GTGGCACGATCTCGGCTCACT', 'GATGTTGCAGGATATTCAGGA', 'GGTCTACTTATCCCCCCACTA', 'GCATACCCAGAGCTGGTTGTT', 'GAGTCTCACTCTGTTGCCCAG', 'GCCGGTCCCTCCTCGCCGGGT', 'GGTTTCATTCATGTTCTGAGG', 'GTCCCATCTGTATGACCTTCC', 'GCTTTGGGTTCTGGGCCTCTG', 'GCCTTTACTACTTCTCCCTGG', 'GCTGTTCATGTATGTAGGAGT', 'GTGATAAGGCTGAGGAAGGG ', 'GTCTCAGGGCCTCTGCTGGCA', 'GCCTTTTTTCTCTAAGTGGCA', 'GGGGGCAGAGGTAGCAGCAGC', 'GGGCTGAAGGGTGACCTGTGT', 'GCCACCTGGAAGATACAGGTG', 'GCAGCAGGGGGACACAGACCT', 'GGATGGGGAGGACTAAGAAAG', 'GCCATGATACAGTCTGCATCT', 'GGGTGACCTGTGTGAGCGCCA', 'GTGTCCTCATTGGGAAAATGG', 'GTAGACCGGAAGGAGGTGGTC', 'GTTTCATTCATGTTCTGAGGA', 'GTCAGTGAGGATGATGAAGTA', 'GCGCTTGGATCCAGATACTGC', 'GACCTTCCTGAGGGAGCCCAT', 'GCCGGGCTTTGGGTTCTGGGC', 'GACCATGCAGAAACTGGAGTT', 'GAGGATGTCTTGATAATGTGT', 'GCTACCACCCTAACCACAGAG', 'GGGACTACAGGCGCCCACCAC', 'GGGGGACACAGACCTGCCCAA', 'GAGGATCAGAGCAGTTCTAAG', 'GCCACTGCGCCCGGCCCCTGG', 'GGAACTTCATGAGGCCCTCGA', 'GCGAGCTCATCCAGCAGAGCC', 'GATTGCAGAGTTGAAAGAGAG', 'GATTTTACCGCTATAATATCG', 'GAATGGGGCTGAAGGGTGACC', 'GTCTTGCCTTTTTTCTCTAAG', 'GAATGAGGAAACTGAAATGGC', 'GACTAAGAAAGCTACCACCCT', 'GGCTCTGCCTCCTGGGTTCAC', 'GAGGCCTGTCCGCTGGATGTT', 'GAACGGCAGCTTCTGACGCTG', 'GCTCTGCCTCCTGGGTTCACG', 'GGGATTAGAGGCATGAGCCAC', 'GGCCTTAAGGGAGCAAGTTAT', 'GATTCCCGTCCGGTGTCACAG', 'GCCAGTCCCCAGAGCATGAGG', 'GAGGATGCTTCCCCTGAGGTT', 'GACAGGTATCCCCTGAAACTG', 'GCACTATGGAGACACCAACCA', 'GGTTTTATGTGTCCTCATTGG', 'GGGAGGTTTTATGTGTCCTCA', 'GGGGACACAGACCTGCCCAAG', 'GCATGAGGCCCACAGTGTTGT', 'GACACAGACCTGCCCAAGCCA', 'GCTGGCAGTCTAGACCTTTTG', 'GCAGGTCACATGCCTATACAT', 'GCGCATACCCAGAGCTGGTTG', 'GCGGGCTACTGGGCCTGCGCT', 'GGATGTCTTGATAATGTGTGA', 'GCCCAGTATCCAAGGATTCCT', 'GGAAACCCGAAAACAGAGTAT', 'GGCTGATGGTGAGGATCAGAG', 'GATCCCACAGCCTTGGTGGAA', 'GTAGGAGTTAAAATAGGCCAA', 'GGAAACGAACTGCCACCTGGA', 'GTCGCAGAGGCCTGTCCGCTG', 'GTATTTTCACTTTATGAATGA', 'GTATGACCTTCCTGAGGGAGC', 'GGAGACAGGTCTGAGTGGGGC', 'GGGCCATTGGACGCTGCGGAA', 'GAGCTCCTGTCCAGCCAAGGA', 'GAGCAAGAAGAGGCCTGGAA ', 'GGTGATAAGGCTGAGGAAGGG', 'GATGAGTACCCAATCCTGTCC', 'GTCTACTTATCCCCCCACTAT', 'GTATTGGCTGTTCATGTATGT', 'GCTAGGATGGTCTCGATCTCC', 'GTGAAAGGAAACGAACTGCCA', 'GTCTGATCCTTTGGTCTTTGC', 'GTCCTTGCTACAGCATTGGAA', 'GGAGATCCCAGGAGAATCCCA', 'GATACTGCTTACTCCCGTCTC', 'GATGCTTCCCCTGAGGTTTGC', 'GGTGGAAGCCATTGTGGAAGA', 'GTACCCAATCCTGTCCTTGCC', 'GGCCTGCAGCCAGTCCCCAGA', 'GAGCTGTTGTATAGATCCCAG', 'GGAGACACCAACCAGAAACTG', 'GCTGATGGTGAGGATCAGAGC', 'GCTACAGCATTGGAACCAACA', 'GCTGAAGTCTGTCATGAAACC', 'GCCTCAGCTTCCCGAGTAGCT', 'GAAACTGAGCTGAGCCCAGTA', 'GAGGGAGCCTTTACTACTTCT', 'GGCCTGCGCTGCCGGGCTTTG', 'GAAGAAGTGGCCTGTCCCATC', 'GGACTAAGAAAGCTACCACCC', 'GTCCCTTGGACTCCAGGAGAC', 'GGAGTGCAGTGGCACGATCTC', 'GGGAGTACAAGTGGGAACTTC', 'GAAACTGCCAGACAATCCTGA', 'GGGCTTTGGGTTCTGGGCCTC', 'GCTGAGCCCAGTATCCAAGGA', 'GGAAGCCAGTGCATCTCCTCA', 'GCCATTGGACGCTGCGGAACC', 'GGCAGGCAAGCCGTGGATTTT', 'GCCCTATTTTAGTCCTTGCTA', 'GGCACGATCTCGGCTCACTGC', 'GAAGAAAGAGCAAGAAGAGGC', 'GTTGCAGGATATTCAGGAAGT', 'GACTCTCACCTTCTGACCTCC', 'GAAGTCTGTCATGAAACCACT', 'GTGCACTATGGAGACACCAAC', 'GGCCTCTGCTGGCAGTCTAGA', 'GTTTGTATTTTCACTTTATGA', 'GGTCGCAGAGGCCTGTCCGCT', 'GTCCTAGTCTTGCCTTTTTTC', 'GGCAAGCCGTGGATTTTGCAC', 'GATTAGAGGCATGAGCCACTG', 'GGGCTACTGGGCCTGCGCTGC', 'GTCTAGACCTTTTGGGCAGAA', 'GCTCCTCTGGCCATCTGCTCC', 'GCCTGCGCTGCCGGGCTTTGG', 'GCGCTGCCGGGCTTTGGGTTC', 'GTGGAAACCCGAAAACAGAGT', 'GCTGGGGGCAGAGGTAGCAGC', 'GACACCATTAGATGGGCTTTC', 'GGCCCTAAGTGCTGAGCTGCC', 'GGTGGAGGTGGGAGACAGGTC', 'GAAACAAGGTTGACAAACTCA', 'GGCACCGATGAGTACCCAATC', 'GAGGCCTGCAGCCAGTCCCCA', 'GTATGTAGGAGTTAAAATAGG', 'GATCCATCCCATGATTTGTTC', 'GGTGACTCGTTGGGGTAAGGA', 'GGATGCTTCCCCTGAGGTTTG', 'GTGTGCACTATGGAGACACCA', 'GGAAACTGAAATGGCCTTAAG', 'GTCCCCAGAGCATGAGGCCCA', 'GCTATAACTAAGCAACCTTTA', 'GCATTCTGTCCACCATGAAAA', 'GGATGGTCTCGATCTCCTGAC', 'GGGAACTTCATGAGGCCCTCG', 'GCCTTAAGGGAGCAAGTTATT', 'GTATCCCCTGAAACTGAGCTG', 'GCAAATCTTGGAGCTTGCAGC', 'GGATCAGGACTTTGTCCATGT', 'GGAGGATGATTGCAGAGTTGA', 'GAGAGGTCGCAGAGGCCTGTC', 'GATGAAGTAGATACAGTCTT ', 'GTCGAGCTCCTGTCCAGCCAA', 'GAGGACTAAGAAAGCTACCAC', 'GAGCATGAGGCCCACAGTGTT', 'GTCTGAGTGGGGCCTGGGAGT', 'GTTGCCTGGGAGTACAAGTGG', 'GTGGCTCCCACATCTTCACTT', 'GAGAGATTTTACCGCTATAAT', 'GCCCACCACGACGCCTGGCTC', 'GGAGGGTGTGAGAAGGAACCA', 'GTTCAGTCAGTGAGGATGATG', 'GTCCTCATTGGGAAAATGGGT', 'GGAGCTTGCAGCAGCCAGAAC', 'GAACCAGGCTTCTTCACTTTG', 'GGCACTACTGGGAGGTGGAGG', 'GGGCCTGGCCCCCATGGGGCT', 'GAGTTAAAATAGGCCAAACTG', 'GAGTACCCAATCCTGTCCTTG', 'GCAGCTTCTGACGCTGGGCCA', 'GAAAGCTACCACCCTAACCAC', 'GGAGACCAGTGTCCTAGTCTT', 'GCTGCCGGGCTTTGGGTTCTG', 'GCCACAGCTGTCTCTCTGGAC', 'GTCACAGTTCAGTCAGTGAGG', 'GCAAGCCGTGGATTTTGCACA', 'GGGCCTCTGCCGCTCTCTGGC', 'TATGTGACTCTCACCTTCTGA', 'TCGCCGGGTGGGAATCTTCGT', 'TTGGAACCAACAACACTGCTC', 'TCCTCTGTCTGATCCTTTGGT', 'TGGGAGGTGGAGGTGGGAGAC', 'TCACTTCGCAGGTCACATGCC', 'TTTTAAAAAGCAGAGGCCCAG', 'TGTCCCATCTGTATGACCTTC', 'TCATGGAGCTGTTGTATAGAT', 'TACACCTGTCCCCTCTGTCGA', 'TTCTCCCTGGTTTCATTCATG', 'TGACTGTGGCTCCCACATCTT', 'TCTGTCTGATCCTTTGGTCTT', 'TTAGCTAGGATGGTCTCGATC', 'TCCCAATAGGCCAAAAAAATG', 'TCATGATCCGCCCTCCTCAGC', 'TCAGTCAGTGAGGATGATGAA', 'TGGGCCTGCGCTGCCGGGCTT', 'TACAATGTGACTGACTGTGGC', 'TCCTGCCCTATTTTAGTCCTT', 'TCTACTTTAATTGTATTGGCT', 'TTTAATTGTATTGGCTGTTCA', 'TGGGGAGGACTAAGAAAGCTA', 'TGCCACCTGGAAGATACAGGT', 'TATTTCTTACAAAGCTGCTGA', 'TTCCCCTGAGGTTTGCTATAA', 'TTTCCCAATAGGCCAAAAAAA', 'TGCAGTGGCACGATCTCGGCT', 'TCTAGACCTTTTGGGCAGAAA', 'TGAGCCACTGCGCCCGGCCCC', 'TCATGAAACCACTTATTTTAA', 'TATATTTGGGTCTGAAACTTC', 'TGGTCTCGATCTCCTGACCTC', 'TTTTCACACTCAAGTGAAACA', 'TGGCTGTTCATGTATGTAGGA', 'TGCGCTGCCGGGCTTTGGGTT', 'TTATGTGACTCTCACCTTCTG', 'TCATCCAGCAGAGCCAGGTCC', 'TCACTTTGAGTTTCCGCCGCG', 'TTCTGGGCCTCTGCCGCTCTC', 'TGATTGCAGAGTTGAAAGAGA', 'TGGAAGATACAGGTGGAAACC', 'TGAGCCCAGTATCCAAGGATT', 'TTATGCAGCTGATGTGCGCTT', 'TTCATGTATGTAGGAGTTAAA', 'TCTGAAGAAAGAGCAAGAAGA', 'TGTGTCCTCATTGGGAAAATG', 'TGGAGCTGTTGTATAGATCCC', 'TGTGCCATGATACAGTCTGCA', 'TTCATTCATGTTCTGAGGAGG', 'TGACCTCATGATCCGCCCTCC', 'TTTATGTGTCCTCATTGGGAA', 'TTCCCGAGTAGCTGGGACTAC', 'TGTGTTTCTACTTTAATTGTA', 'TTTTATGTGTCCTCATTGGGA', 'TTTGTATTTTCACTTTATGAA', 'TAGGCCAAACTGGAGAAATA ', 'TAGGAGTTAAAATAGGCCAAA', 'TTCACTTTGAGTTTCCGCCGC', 'TCCACCATGAAAAAAAAAAAA', 'TGGGGAAAAGCTGAAGATGT ', 'TGAGTACCCAATCCTGTCCTT', 'TTTCCCACTGTGCCATGATAC', 'TTAAAAATTGTGTTTCTACTT', 'TGACATTTCTTTCTACAATGT', 'TTACTACTTCTCCCTGGTTTC', 'TTTAGTTTGTATTTTCACTTT', 'TGTGTGAGCGCCATGGGGAAA', 'TTCTCTAAGTGGCAGGATCAG', 'TACTTATCCCCCCACTATGGA', 'TGGAGGATGATTGCAGAGTTG', 'TGTATAGATCCCAGATCCATC', 'TGATCCGCCCTCCTCAGCCTC', 'TGTTGTATAGATCCCAGATCC', 'TGTAGGAGTTAAAATAGGCCA', 'TCCAAGGATTCCTCTGTCTGA', 'TTGGGCAGAAAGGAGCTTCAA', 'TTCTGACGCTGGGCCATTGGA', 'TAAGGATCAGGACTTTGTCCA', 'TAAGGCTACATCCAGGAATGG', 'TTGGACTCCAGGAGACCAGTG', 'TTAAGGGAGCAAGTTATTTCT', 'TCTCACCTTCTGACCTCCTGG', 'TCCCACTGTGCCATGATACAG', 'TGGGCTTTCCCAATAGGCCAA', 'TGACCTGTGTGAGCGCCATGG', 'TATGACCTTCCTGAGGGAGCC', 'TGAAACTTCTCACATGTTTGG', 'TCCCCAGAGCATGAGGCCCAC', 'TGTTGCCTGGGAGTACAAGTG', 'TCTGCCTCCTGGGTTCACGC ', 'TGATAATGTGTGAGGCCTGCA', 'TATGTAGGAGTTAAAATAGGC', 'TGTGACTGACTGTGGCTCCCA', 'TGGTTGTTGTGCTGAGGCCA ', 'TGCCGGTCCCTCCTCGCCGGG', 'TGGAAACCCGAAAACAGAGTA', 'TTTCTCTAAGTGGCAGGATCA', 'TATTTTAAAAAGCAGAGGCCC', 'TTAGATGGGCTTTCCCAATAG', 'TTGAAGTTGGTGAAAGGAAAC', 'TAGAGAAAAAGCAGCCACCAC', 'TTTCACCATGTTAGCTAGGAT', 'TCCTCTGGCCATCTGCTCCCT', 'TTGACTGTGGCCACAGCTTCT', 'TAACCACAGAGGCTTGGAATT', 'TTCACTTTATGAATGAGGAAA', 'TACTTTAATTGTATTGGCTGT', 'TCAAATGAGCATTGCATCCCA', 'TCTACTTATCCCCCCACTATG', 'TCCGTCTGCTAAGGCTACATC', 'TTACTCCCGTCTCATCGTGTC', 'TTTGGGTCTGAAACTTCTCAC', 'TTGGGCCTGGCCCCCATGGGG', 'TGAGCTGAGCCCAGTATCCAA', 'TTTAGTCCTTGCTACAGCATT', 'TGGCCTTAAGGGAGCAAGTTA', 'TTTACTACTTCTCCCTGGTTT', 'TGAGGTTTGCTATAACTAAGC', 'TGGGGCTGAAGGGTGACCTGT', 'TACTCCCGTCTCATCGTGTCT', 'TGAGCATTGCATCCCATGAGG', 'TTCAGTGCAGCAGGGGGACAC', 'TATAAAGCATTTTTCACACTC', 'TACTGCTTACTCCCGTCTCAT', 'TCACTTTATGAATGAGGAAAC', 'TCTGTCCACCATGAAAAAAAA', 'TTGGGTTCTGGGCCTCTGCCG', 'TTATTTCTTACAAAGCTGCTG', 'TGGTGAGGATCAGAGCAGTTC', 'TCATATGATTGTCCTGCCTCA', 'TGCGGCCTAATTGGCAGCTGG', 'TGGCACGATCTCGGCTCACTG', 'TAAGTGGCAGGATCAGAAAAC', 'TATCCCCCCACTATGGATTCT', 'TGACTCTCACCTTCTGACCTC', 'TAGTCTTGCCTTTTTTCTCTA', 'TCTCCCTGGTTTCATTCATGT', 'TCCAGGAGACCAGTGTCCTAG', 'TGCAAAGAGGATGTCTTGATA', 'TTTAAAAAGCAGAGGCCCAGT', 'TTGGAGCTTGCAGCAGCCAGA', 'TTGTGTTTCTACTTTAATTGT', 'TCTGAAACTTCTCACATGTTT', 'TCCTTGCCGGTCCCTCCTCGC', 'TTGGTGAAAGGAAACGAACTG', 'TCATTGGGAAAATGGGTGTAA', 'TGAGGCCCTCGAACATCTGAA', 'TTAGAGGCATGAGCCACTGCG', 'TCTTCACTTTGAGTTTCCGCC', 'TGGGAGTACAAGTGGGAACTT', 'TTGCTACAGCATTGGAACCAA', 'TAGTCCTTGCTACAGCATTGG', 'TATGATTGTCCTGCCTCATGG', 'TGGAAGAAGTGGCCTGTCCCA', 'TTGTATTGGCTGTTCATGTAT', 'TTCACCATGTTAGCTAGGATG', 'TGCCGGGCTTTGGGTTCTGGG', 'TTATGAATGAGGAAACTGAAA', 'TTCTGAGGAGGGTGTGAGAAG', 'TTGCAGAGTTGAAAGAGAGGT', 'TTATTTCTTTTTTTTTGACAC', 'TCTGTCATGAAACCACTTATT', 'TGCCCAGGCTGGAGTGCAGTG', 'TCACTGCAGGCTCTGCCTCCT', 'TAAAAAGCAGAGGCCCAGTCA', 'TGAGTTTCCGCCGCGAAGCGC', 'TGGGTGATAAGGCTGAGGAAG', 'TTTACCGCTATAATATCGTCC', 'TGAGTACCGAGCAGGCACCGA', 'TCCGCTGGATGTTGCAGGATA', 'TGCCACAGCTGTCTCTCTGGA', 'TCGGCAGCTGGGGGCAGAGGT', 'TGACAAACTCACTTCGCAGGT', 'TACAAGTGGGAACTTCATGAG', 'TGTGAGCGCCATGGGGAAAAG', 'TCCTGAGAGATTTTACCGCTA', 'TGAAGGGTGACCTGTGTGAGC', 'TGCACATGGTGACTTTCCCAC', 'TAGACCTTTTGGGCAGAAAGG', 'TCACACTCAAGTGAAACAAGG', 'TCAGCTTCCCGAGTAGCTGGG', 'TGTCTTGATAATGTGTGAGGC', 'TGCCTTTGTCTCAGGGCCTCT', 'TCATGTATGTAGGAGTTAAAA', 'TATTTGGGTCTGAAACTTCTC', 'TCTCGATCTCCTGACCTCATG', 'TTTATGAATGAGGAAACTGAA', 'TCACTCTGTTGCCCAGGCTGG', 'TTCCCACTGTGCCATGATACA', 'TTGTCCTGCCTCATGGAGCTG', 'TTTTAGTCCTTGCTACAGCAT', 'TGCAGCCAGTCCCCAGAGCAT', 'TGCAGCTGATGTGCGCTTGGA', 'TGCTGCGCATACCCAGAGCTG', 'TTTCTTTCTACAATGTGACTG', 'TCCTGTCCAGCCAAGGAACCT', 'TCTACAATGTGACTGACTGTG', 'TGTCATGAAACCACTTATTTT', 'TCTTGCCTTTTTTCTCTAAGT', 'TAAAACCTCATATGATTGTCC', 'TCAAATGGCTGTGATAAGGAA', 'TGGCAGTCTAGACCTTTTGGG', 'TCTGAGTGGGGCCTGGGAGTA', 'TCAAAATTATAAAGCATTTTT', 'TGGAGCTTGCAGCAGCCAGAA', 'TAAGAAAGCTACCACCCTAAC', 'TGTGGCCACAGCTTCTGCCAC', 'TTGGTGGAAGCCATTGTGGAA', 'TCCGCCCTCCTCAGCCTCCCA', 'TCCTGCCTCAGCTTCCCGAGT', 'TGGATTTTGCACATGGTGACT', 'TCAGAGGATGCTTCCCCTGAG', 'TCAGAGCAGTTCTAAGGTGAC', 'TCTGGACTCTGGGAGATCCCA', 'TTGCCTTTTTTCTCTAAGTGG', 'TTGTCTCAGGGCCTCTGCTGG', 'TTCTGTCCACCATGAAAAAAA', 'TCTAAGTGGCAGGATCAGAAA', 'TGGGCCTCTGCCGCTCTCTGG', 'TCATTCATGTTCTGAGGAGGG', 'TGTGGAAGAAGTGGCCTGTCC', 'TCAAGTGAAACAAGGTTGACA', 'TGGGATTAGAGGCATGAGCCA', 'TGGAGCAAGTTATTTCTTACA', 'TGGAACCAACAACACTGCTCC', 'TCCCACATCTTCACTTTCCCC', 'TCGATCTCCTGACCTCATGAT', 'TGTTTCTACTTTAATTGTATT', 'TGGAAGCCATTGTGGAAGAAG', 'TGGTTTCATTCATGTTCTGAG', 'TTCTGCCACAGCTGTCTCTCT', 'TGTCCACCATGAAAAAAAAAA', 'TGGATGTTGCAGGATATTCAG', 'TGAGGCCCATGACATTTCTTT', 'TGTATTGGCTGTTCATGTATG', 'TTGGGGTAAGGATCAGGACTT', 'TATTGGCTGTTCATGTATGTA', 'TCTCCTGACCTCATGATCCGC', 'TTCTAAGGTGACTCGTTGGGG', 'TCCCAGGAGAATCCCAGAACT', 'TCTAAGGTGACTCGTTGGGGT', 'TCGAGCTCCTGTCCAGCCAAG', 'TGAATGAGGAAACTGAAATGG', 'TTAATTGTATTGGCTGTTCAT', 'TGTCCCTTGGACTCCAGGAGA', 'TGGAGTGCAGTGGCACGATCT', 'TGCCCTATTTTAGTCCTTGCT', 'TCTGACGCTGGGCCATTGGAC', 'TTTTTCTCTAAGTGGCAGGAT', 'TGGCCCTAAGTGCTGAGCTGC', 'TGCCTCATGGAGCTGTTGTAT', 'TTATATTTGGGTCTGAAACTT', 'TCTCTGGCCCTAAGTGCTGAG', 'TGGCTCCCACATCTTCACTTT', 'TTGCACATGGTGACTTTCCCA', 'TTGCCCAGGCTGGAGTGCAGT', 'TGGGAAGCCAGTGCATCTCCT', 'TTGCAGCAGCCAGAACCAATC', 'TGCCCAAGCCACCCCACTGCC', 'TTCTGACCTCCTGGCAAGAG ', 'TCCCCTGAAACTGAGCTGAGC', 'TGCCAGACAATCCTGAGAGAT', 'TGGGGTAAGGATCAGGACTTT', 'TCTCTGAGCACAAGCTGGGCA', 'TCAGTGCAGCAGGGGGACACA', 'TTTTCTCTAAGTGGCAGGATC', 'TCTCACTCTGTTGCCCAGGCT', 'TCTGTATGACCTTCCTGAGGG', 'TTCACACTCAAGTGAAACAAG', 'TGCAGAGTTGAAAGAGAGGTC', 'TACAGGTGGAAACCCGAAAAC', 'TTGCAGGATATTCAGGAAGTG', 'TCCCCTGAGGTTTGCTATAAC', 'TTCTCCTGCCTCAGCTTCCCG', 'TAAAAATTGTGTTTCTACTTT', 'TCCTTGCTACAGCATTGGAAC', 'TATTTCTTTTTTTTTGACAC ', 'TTATAAAGCATTTTTCACACT', 'TGGGAACTTCATGAGGCCCTC', 'TGAAACTGAGCTGAGCCCAGT', 'TGGGACTACAGGCGCCCACCA', 'TTTGTCTCAGGGCCTCTGCTG', 'TGGAGTTGAAGACAGATTGCC', 'TAGACCGGAAGGAGGTGGTCT', 'TCCTCGCCGGGTGGGAATCTT', 'TGTCCGCTGGATGTTGCAGGA', 'TCGAACATCTGAAGAAAGAGC', 'TTCTACTTTAATTGTATTGGC', 'TAGGCCAAAAAAATGCTGCGC', 'TCCGGTGTCACAGTTCAGTCA', 'TTTGGGAGGTTTTATGTGTCC', 'TCCGGGCCGAGGAGGGAGCCT', 'TTGAGTTTCCGCCGCGAAGCG', 'TACAGCATTGGAACCAACAAC', 'TCCCCCCACTATGGATTCTGG', 'TCCCAGATCCATCCCATGATT', 'TGTCTCAGGGCCTCTGCTGGC', 'TCCCATCTGTATGACCTTCCT', 'TGGATCCAGATACTGCTTACT', 'TTGGGAAAATGGGTGTAATTC', 'TCTGCCACAGCTGTCTCTCTG', 'TGGGCAGAAAGGAGCTTCAAA', 'TAGCGAGCTCATCCAGCAGAG', 'TTTGCACATGGTGACTTTCCC', 'TACCACCCTAACCACAGAGGC', 'TTCAAATGGCTGTGATAAGGA', 'TCCCTTGGACTCCAGGAGACC', 'TTAGTTTGTATTTTCACTTTA', 'TGATTGTCCTGCCTCATGGAG', 'TCCCTCCTCGCCGGGTGGGAA', 'TGTTCTGAGGAGGGTGTGAGA', 'TCTCGGCTCACTGCAGGCTCT', 'TTATCCCCCCACTATGGATTC', 'TTACTAGAGAAAAAGCAGCCA', 'TGGGAGGTTTTATGTGTCCTC', 'TCATGAGGCCCTCGAACATCT', 'TGGGGGCAGAGGTAGCAGCAG', 'TCCCATGATTTGTTCCTGTCT', 'TAGAGGCAGGCAAGCCGTGGA', 'TTGAAGACAGATTGCCGTGTG', 'TTGAAAGAGAGGTCGCAGAGG', 'TGACCTTCCTGAGGGAGCCCA', 'TAGCTAGGATGGTCTCGATCT', 'TACAGGCGCCCACCACGACGC', 'TGATTCCCGTCCGGTGTCACA', 'TGATACAGTCTGCATCTTATA', 'TCACCATGTTAGCTAGGATGG', 'TTGTATAGATCCCAGATCCAT', 'TGCGCCCGGCCCCTGGAGCAA', 'TACATCCAGGAATGGGGCTGA', 'TCTGGGTGATAAGGCTGAGGA', 'TGCACTATGGAGACACCAACC', 'TGAGGAGGGTGTGAGAAGGAA', 'TTCTTACAAAGCTGCTGAAGG', 'TGCAGAAACTGGAGTTGAACC', 'TCTTACAAAGCTGCTGAAGGT', 'TTGTATTTTCACTTTATGAAT', 'TGGCCCCCATGGGGCTTGGAG', 'TCGCAGAGGCCTGTCCGCTGG', 'TGTTCATGTATGTAGGAGTTA', 'TGGGAGATCCCAGGAGAATCC', 'TCTCAGGGCCTCTGCTGGCAG', 'TTTCACTTTATGAATGAGGAA', 'TCTCTAAGTGGCAGGATCAGA', 'TATGGATTCTGGGTGATAAGG', 'TGTAAGCAAAATGTAGACCGG', 'TGGGTTCTGGGCCTCTGCCGC', 'TCTTGATAATGTGTGAGGCCT', 'TTCATGTTCTGAGGAGGGTGT', 'TGTCCTAGTCTTGCCTTTTTT', 'TGTATGACCTTCCTGAGGGAG', 'TGGACTCTGGGAGATCCCAGG', 'TTTCACACTCAAGTGAAACAA', 'TCCCTGATTCCCGTCCGGTGT', 'TGACTTTCCCACTGTGCCATG', 'TTTTGCACATGGTGACTTTCC', 'TGTCCTTGCCGGTCCCTCCTC', 'TGGGCCTGGCCCCCATGGGGC', 'TGTTGCAGGATATTCAGGAAG', 'TATTTTCACTTTATGAATGAG', 'TCCCTGGAGTTGAAGACAGAT', 'TGAGAGATTTTACCGCTATAA', 'TATCCAAGGATTCCTCTGTCT', 'TGCCTGGGAGTACAAGTGGGA', 'TGACTGTGGCCACAGCTTCTG', 'TGTGCACTATGGAGACACCAA', 'TGCTGGGATTAGAGGCATGAG', 'TGGAATTGGGCCTGGCCCCCA', 'TTTTGGGCAGAAAGGAGCTTC', 'TCCAGCCAAGGAACCTGCGGC', 'TTATGTGTCCTCATTGGGAAA', 'TCCTAGTCTTGCCTTTTTTCT', 'TGCTTCCCCTGAGGTTTGCTA', 'TTGCCGGTCCCTCCTCGCCGG', 'TTGGATCCAGATACTGCTTAC', 'TAGGATGGTCTCGATCTCCTG', 'TAATTGTATTGGCTGTTCATG', 'TACCCAATCCTGTCCTTGCCG', 'TACCGAGCAGGCACCGATGAG', 'TAGCTGGGACTACAGGCGCCC', 'TACCCAGAGCTGGTTGTTGTG', 'TGACTGACTGTGGCTCCCACA', 'TTGGGAGGTTTTATGTGTCCT', 'TATTAAAAATTGTGTTTCTAC', 'TGAAACCACTTATTTTAAAAA', 'TGAGGAAACTGAAATGGCCTT', 'TCTCTCTGGACTCTGGGAGAT', 'TCTTTCTACAATGTGACTGAC', 'TTATTTTAAAAAGCAGAGGCC', 'TTGGCAGCTGGCCAATGTTGT', 'TGTGACTCTCACCTTCTGACC', 'TGTTAGCTAGGATGGTCTCGA', 'TTGATAATGTGTGAGGCCTGC', 'TAGTTTGTATTTTCACTTTAT', 'TGCAGCAGGGGGACACAGACC', 'TCCCGTCCGGTGTCACAGTTC', 'TCCCGAGTAGCTGGGACTACA', 'TAATGTGTGAGGCCTGCAGCC', 'TCAGGCCGGCACTACTGGGAG', 'TGCAGCAGCCAGAACCAATCT', 'TGCTCCTCTGGCCATCTGCTC', 'TCCCTCTCTGAGCACAAGCTG', 'TGATGGTGAGGATCAGAGCAG', 'TTACAAAGCTGCTGAAGGTAA', 'TCCATCCCATGATTTGTTCCT', 'TTTTACCGCTATAATATCGTC', 'TGCATCTCCTCAGGCCGGCAC', 'TTGTGGAAGAAGTGGCCTGTC', 'TAGATGGGCTTTCCCAATAGG', 'TATGGAGACACCAACCAGAAA', 'TTTCTACAATGTGACTGACTG', 'TTCTTCACTTTGAGTTTCCGC', 'TGAAAAATACCAGCGATTACT', 'TAGAGGCATGAGCCACTGCGC', 'TGCTACAGCATTGGAACCAAC', 'TCCTCATTGGGAAAATGGGTG', 'TGGCTCATTTTTTTGTATTTT', 'TGGGCCATTGGACGCTGCGGA', 'TTTTTCACACTCAAGTGAAAC', 'TCAGGGCCTCTGCTGGCAGTC', 'TCTGTTGCCCAGGCTGGAGTG', 'TTAGTCCTTGCTACAGCATTG', 'TGCTAAGGCTACATCCAGGAA', 'TTGAAAAATACCAGCGATTAC', 'TTGGACGCTGCGGAACCAGGC', 'TCTGCAAAGAGGATGTCTTGA', 'TTAAAAAGCAGAGGCCCAGTC', 'TATTATATTTGGGTCTGAAAC', 'TACAAAGCTGCTGAAGGTAAG', 'TGGAGTTGAACCATAGCGAGC', 'TGCTATAACTAAGCAACCTTT', 'TTCATGAGGCCCTCGAACATC', 'TCCTGTCCTTGCCGGTCCCTC', 'TCCCCTCTGTCGAGCTCCTGT', 'TGGCCTGTCCCATCTGTATGA', 'TGGTGACTTTCCCACTGTGCC', 'TTCGCAGGTCACATGCCTATA', 'TCTGAGGAGGGTGTGAGAAGG', 'TTGGCTGTTCATGTATGTAGG', 'TGCTGGCAGTCTAGACCTTTT', 'TCCCTGGTTTCATTCATGTTC', 'TGAAAGGAAACGAACTGCCAC', 'TTCCCAATAGGCCAAAAAAAT', 'TGTATGTAGGAGTTAAAATAG', 'TGGATTCTGGGTGATAAGGCT', 'TTGGAATTGGGCCTGGCCCCC', 'TCCCGTCTCATCGTGTCTGAG', 'TCCTCAGGCCGGCACTACTGG', 'TCACCTTCTGACCTCCTGGCA', 'TTACCGCTATAATATCGTCC ', 'TAGCCAACCACCCTCTTCCCT', 'TCTGATCCTTTGGTCTTTGC ', 'TTTTTTCTCTAAGTGGCAGGA', 'TATGTGTCCTCATTGGGAAAA', 'TCTTCCCTGATTCCCGTCCGG', 'TCTGGGCCTCTGCCGCTCTCT', 'TTGCCTGGGAGTACAAGTGGG', 'TGCGCTTGGATCCAGATACTG', 'TTTCTACTTTAATTGTATTGG', 'TGTGAGGCCTGCAGCCAGTCC', 'TGATGTGCGCTTGGATCCAGA', 'TGCGGGCTACTGGGCCTGCGC', 'TGAAAAAAAAAAAAAAAAAAA', 'TGGTCTACTTATCCCCCCACT', 'TGAAGAAAGAGCAAGAAGAGG', 'TTCCCTGATTCCCGTCCGGTG', 'TCTCCTCAGGCCGGCACTACT', 'TGCCATGATACAGTCTGCATC', 'TGCTTACTCCCGTCTCATCGT', 'TCTCCTGCCTCAGCTTCCCGA', 'TCTGGGAGATCCCAGGAGAAT', 'TCTCCCTGGAGTTGAAGACAG', 'TGGTGGAAGCCATTGTGGAAG', 'TACCAGGCTGAAGTCTGTCAT', 'TGGACTCCAGGAGACCAGTGT', 'TCCAGATACTGCTTACTCCCG', 'TGAAAGAGAGGTCGCAGAGGC', 'TGACGCTGGGCCATTGGACGC', 'TATGAATGAGGAAACTGAAAT', 'TGGGCGCCTCCTGCCCTATTT', 'TCACAGTTCAGTCAGTGAGGA', 'TTTCTTACAAAGCTGCTGAAG', 'TAGAGACGGAGTTTCACCATG', 'TGCGCATACCCAGAGCTGGTT', 'TGGAGACACCAACCAGAAACT', 'TGAAACAAGGTTGACAAACTC', 'TGGCAGCTGGCCAATGTTGTA', 'TGTCTGATCCTTTGGTCTTTG', 'TCGCAGGTCACATGCCTATAC', 'TATAGATCCCAGATCCATCCC', 'TGTGTGAGGCCTGCAGCCAGT', 'TGCCGCTCTCTGGCCCTAAGT', 'TGAAGACTTATGCAGCTGATG', 'TGGGAAAATGGGTGTAATTCA', 'TTATGAGGCCCATGACATTTC', 'TTGACAAACTCACTTCGCAGG', 'TGGCAGGATCAGAAAACCTGC', 'TGTATTTTCACTTTATGAATG', 'TCACTTTCCCCCGCTATCCCT', 'TCTCTGGACTCTGGGAGATCC', 'TTATTATATTTGGGTCTGAAA', 'TGCCGTTCCCTCTCTGAGCAC', 'TCCCACAGCCTTGGTGGAAGC', 'TAAGCAAAATGTAGACCGGAA', 'TAAAGCATTTTTCACACTCAA', 'TTCCTCTGTCTGATCCTTTGG', 'TTCACTTTCCCCCGCTATCCC', 'TGAGGATGATGAAGTAGATAC', 'TGGGTCTGAAACTTCTCACAT', 'TGTCGAGCTCCTGTCCAGCCA', 'TATGAGGCCCATGACATTTCT', 'TCATGTTCTGAGGAGGGTGTG', 'TGAGCGCCATGGGGAAAAGCT', 'TGTGCGCTTGGATCCAGATAC', 'TTCTACAATGTGACTGACTGT', 'TGAAATGGCCTTAAGGGAGCA', 'TTTGAGTTTCCGCCGCGAAGC', 'TTTGGGTTCTGGGCCTCTGCC', 'TATCCCCTGAAACTGAGCTGA', 'TGAGGATCAGAGCAGTTCTAA', 'TCTTGGAGCTTGCAGCAGCCA', 'TCGTTGGGGTAAGGATCAGGA', 'TGCAGGATATTCAGGAAGTGT', 'TGCCTTTTTTCTCTAAGTGGC', 'TGGTGAAAGGAAACGAACTGC', 'TTTGCTATAACTAAGCAACCT', 'TACTGGGCCTGCGCTGCCGGG', 'TCGGCTCACTGCAGGCTCTGC', 'TAGATCCCAGATCCATCCCAT', 'TCAGTGAGGATGATGAAGTAG', 'TCTGCCGCTCTCTGGCCCTAA', 'TAAGGGAGCAAGTTATTTCTT', 'TGCCTCAGCTTCCCGAGTAGC', 'TCCTGCCTCATGGAGCTGTTG', 'TCTGCTGGCAGTCTAGACCTT', 'TTCTCACATGTTTGGGAGGTT', 'TACAGCGGGAGGCAGCGGAGA', 'TGGCCACAGCTTCTGCCACAG', 'TACTTCTCCCTGGTTTCATTC', 'TGGACGCTGCGGAACCAGGCT', 'TGTGGCTCCCACATCTTCACT', 'TTGGGTCTGAAACTTCTCACA', 'TGTTTGGGAGGTTTTATGTGT', 'TCTCACATGTTTGGGAGGTTT', 'TACTAGAGAAAAAGCAGCCAC', 'TAAAATAGGCCAAACTGGAGA', 'TGTCCAGCCAAGGAACCTGCG', 'TGAGGCCTGCAGCCAGTCCCC', 'TCTGAGCACAAGCTGGGCAAA', 'TACTACTTCTCCCTGGTTTCA', 'TGAAGACAGATTGCCGTGTGC', 'TGAACCATAGCGAGCTCATCC', 'TCTGGCCCTAAGTGCTGAGCT', 'TAAGGTGACTCGTTGGGGTAA', 'TGAGGCCCACAGTGTTGTGCC', 'TAAGCAACCTTTATGTGACTC', 'TATAACTAAGCAACCTTTATG', 'TGGGAGACAGGTCTGAGTGGG', 'TACCAGCGATTACTAGAGAAA', 'TGAGCACAAGCTGGGCAAAT ', 'TTTATGTGACTCTCACCTTCT', 'TTTTCACTTTATGAATGAGGA', 'TTGAACCATAGCGAGCTCATC', 'TTTGGGCAGAAAGGAGCTTCA', 'TGATGAAGTAGATACAGTCTT', 'TGTAGACCGGAAGGAGGTGGT', 'TGAAGTTGGTGAAAGGAAACG', 'TGTCCTCATTGGGAAAATGGG', 'TGTTGCCCAGGCTGGAGTGCA', 'TGACTCGTTGGGGTAAGGATC', 'TCTTCACTTTCCCCCGCTATC', 'TACTGGGAGGTGGAGGTGGGA', 'TATGCAGCTGATGTGCGCTTG', 'TGTCCTGCCTCATGGAGCTGT', 'TTTCATTCATGTTCTGAGGAG', 'TGCGGAACCAGGCTTCTTCAC', 'TGACAGGTATCCCCTGAAACT', 'TTCCCTCTCTGAGCACAAGCT', 'TTCAGTCAGTGAGGATGATGA', 'TGTCCCCTCTGTCGAGCTCCT', 'TCCAGGAATGGGGCTGAAGGG', 'TCACATGTTTGGGAGGTTTTA', 'TAACTAAGCAACCTTTATGTG', 'TTCTTTCTACAATGTGACTGA', 'TTGCTATAACTAAGCAACCTT', 'TAATTGGCAGCTGGCCAATGT', 'TTCTGGGTGATAAGGCTGAGG', 'TCCTGAAGACTTATGCAGCTG', 'TGTCTCTCTGGACTCTGGGAG', 'TCTGCTAAGGCTACATCCAGG', 'TGAAGTCTGTCATGAAACCAC', 'TTCCCGTCCGGTGTCACAGTT', 'TATTTTAGTCCTTGCTACAGC', 'TTAAAATAGGCCAAACTGGAG', 'TGTCACAGTTCAGTCAGTGAG', 'TCTGTCGAGCTCCTGTCCAGC', 'TGCAGGCTCTGCCTCCTGGGT', 'TGGAGGTGGGAGACAGGTCTG', 'TCCTGACCTCATGATCCGCCC']\n",
      "GetKeysOffsetsInReference::Process all elements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@ 2-Query Cassandra for 0 keys: 18.598\n",
      "######################### Number of partitions reference_keys: 192\n",
      "Query::QueryKeys: Matching desplazaments: []\n",
      "MultipleQuery:: Querys: [Row(_c0=u'GCTGCGGGCTACTGGGCCTGCGCTGCCGGGCTTTGGGTTCTGGGCCTCTGCCGCTCTCTGGCCCTAAGTGCTGAGCTGCC ', _c1=1.0, _c2=80.0)]\n",
      "MultipleQuery:: Query Keys: [(1.0, [('GCTGCGGGCTACTGGGCCTGC', 80), ('CTGCGGGCTACTGGGCCTGCG', 79), ('TGCGGGCTACTGGGCCTGCGC', 78), ('GCGGGCTACTGGGCCTGCGCT', 77), ('CGGGCTACTGGGCCTGCGCTG', 76), ('GGGCTACTGGGCCTGCGCTGC', 75), ('GGCTACTGGGCCTGCGCTGCC', 74), ('GCTACTGGGCCTGCGCTGCCG', 73), ('CTACTGGGCCTGCGCTGCCGG', 72), ('TACTGGGCCTGCGCTGCCGGG', 71), ('ACTGGGCCTGCGCTGCCGGGC', 70), ('CTGGGCCTGCGCTGCCGGGCT', 69), ('TGGGCCTGCGCTGCCGGGCTT', 68), ('GGGCCTGCGCTGCCGGGCTTT', 67), ('GGCCTGCGCTGCCGGGCTTTG', 66), ('GCCTGCGCTGCCGGGCTTTGG', 65), ('CCTGCGCTGCCGGGCTTTGGG', 64), ('CTGCGCTGCCGGGCTTTGGGT', 63), ('TGCGCTGCCGGGCTTTGGGTT', 62), ('GCGCTGCCGGGCTTTGGGTTC', 61), ('CGCTGCCGGGCTTTGGGTTCT', 60), ('GCTGCCGGGCTTTGGGTTCTG', 59), ('CTGCCGGGCTTTGGGTTCTGG', 58), ('TGCCGGGCTTTGGGTTCTGGG', 57), ('GCCGGGCTTTGGGTTCTGGGC', 56), ('CCGGGCTTTGGGTTCTGGGCC', 55), ('CGGGCTTTGGGTTCTGGGCCT', 54), ('GGGCTTTGGGTTCTGGGCCTC', 53), ('GGCTTTGGGTTCTGGGCCTCT', 52), ('GCTTTGGGTTCTGGGCCTCTG', 51), ('CTTTGGGTTCTGGGCCTCTGC', 50), ('TTTGGGTTCTGGGCCTCTGCC', 49), ('TTGGGTTCTGGGCCTCTGCCG', 48), ('TGGGTTCTGGGCCTCTGCCGC', 47), ('GGGTTCTGGGCCTCTGCCGCT', 46), ('GGTTCTGGGCCTCTGCCGCTC', 45), ('GTTCTGGGCCTCTGCCGCTCT', 44), ('TTCTGGGCCTCTGCCGCTCTC', 43), ('TCTGGGCCTCTGCCGCTCTCT', 42), ('CTGGGCCTCTGCCGCTCTCTG', 41), ('TGGGCCTCTGCCGCTCTCTGG', 40), ('GGGCCTCTGCCGCTCTCTGGC', 39), ('GGCCTCTGCCGCTCTCTGGCC', 38), ('GCCTCTGCCGCTCTCTGGCCC', 37), ('CCTCTGCCGCTCTCTGGCCCT', 36), ('CTCTGCCGCTCTCTGGCCCTA', 35), ('TCTGCCGCTCTCTGGCCCTAA', 34), ('CTGCCGCTCTCTGGCCCTAAG', 33), ('TGCCGCTCTCTGGCCCTAAGT', 32), ('GCCGCTCTCTGGCCCTAAGTG', 31), ('CCGCTCTCTGGCCCTAAGTGC', 30), ('CGCTCTCTGGCCCTAAGTGCT', 29), ('GCTCTCTGGCCCTAAGTGCTG', 28), ('CTCTCTGGCCCTAAGTGCTGA', 27), ('TCTCTGGCCCTAAGTGCTGAG', 26), ('CTCTGGCCCTAAGTGCTGAGC', 25), ('TCTGGCCCTAAGTGCTGAGCT', 24), ('CTGGCCCTAAGTGCTGAGCTG', 23), ('TGGCCCTAAGTGCTGAGCTGC', 22), ('GGCCCTAAGTGCTGAGCTGCC', 21), ('GCCCTAAGTGCTGAGCTGCC ', 20)])]\n",
      "MultipleQuery:: Matching desplazaments: []\n",
      "Not matching keys in reference sequence. No hits found\n",
      "# Total time required for processing the 42 query with 42 keys: 46.828 seconds.\n",
      "########################### FINAL STATISTICS Multiple QUERY 02/26/2020 12:50:31 ###########################\n",
      "# Reference: grch38F  \tFile: hdfs://babel.udl.cat///user/nando/Datasets/Sequences/NM_018073.csv.gz   \tQuerys: 42.\n",
      "# Key Size: 21            \tMethod: 1.\n",
      "# Num Executors: None      \tExecutors/cores: None      \tExecutor Mem: None.\n",
      "# Total Time: 46.828         \tData Read Time: 1.617       \tCassandra Read Time: 18.598 .\n",
      "# DF Joining Time: -1.0    \tTop matching Time: -1.0   \tAlig. Extension Time: -1.0.\n",
      "# Num Aligments: 0      \tGood Aligments: 0.\n",
      "############################################################################################\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmptyRDD[350] at emptyRDD at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Test 4: Calculate Multiple Querys \n",
    "    print(\"Test 3: Calculate Multipe Querys\")\n",
    "    global QueryFilename, ReferenceName, DDebug, NumberPartitions, KeySize, Method\n",
    "    QueryFilename = 'hdfs://babel.udl.cat//user/nando/Datasets/Sequences/GRCh38_latest_genomic_200-400.csv.gz'\n",
    "    ReferenceName = \"grch38F\"\n",
    "    DDebug = True\n",
    "    NumberPartitions = 192\n",
    "    KeySize = 21\n",
    "    Method = DDefaultMethod\n",
    "    DoMultipleQuery = True\n",
    "    #MultipleQuery(sc, sqlContext, QueryFilename, ReferenceName)  \n",
    "    \n",
    "    # Test 4B: Calculate Multiple Querys \n",
    "    print(\"Test 4B: Calculate Multipe Querys\")\n",
    "    global QueryFilename, ReferenceName, DDebug, NumberPartitions, KeySize, Method\n",
    "    QueryFilename = 'hdfs://babel.udl.cat///user/nando/Datasets/Sequences/NM_018073.csv.gz'\n",
    "    ReferenceName = \"grch38F\"\n",
    "    HashName = \"grch38F_K21\"\n",
    "    DDebug = True\n",
    "    NumberPartitions = 192\n",
    "    KeySize = 21\n",
    "    Method = DDefaultMethod\n",
    "    DoMultipleQuery = True\n",
    "    MultipleQuery(sc, sqlContext, QueryFilename, ReferenceName, KeySize, ReferenceName)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-457d45c0023c>:328: SyntaxWarning: name 'keysdespl_rdd' is assigned to before global declaration\n",
      "  global keysdespl_rdd, reference_keys\n",
      "<ipython-input-1-457d45c0023c>:328: SyntaxWarning: name 'reference_keys' is assigned to before global declaration\n",
      "  global keysdespl_rdd, reference_keys\n",
      "<ipython-input-1-457d45c0023c>:1495: SyntaxWarning: name 'NumberPartitions' is assigned to before global declaration\n",
      "  global NumberPartitions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Main__\n",
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py(<function MultipleQuery at 0x29bc7d0>, -f, /run/user/1010/jupyter/kernel-62caf0ca-9559-42b5-abd3-5049634475b2.json, 11).\n",
      "++++++++++++ INITIAL STATISTICS 02/26/2020 12:43:09 +++++++++++++\n",
      "+ Reference: /run/user/1010/jupyter/kernel-62caf0ca-9559-42b5-abd3-5049634475b2.json  \tQuery file: -f.\n",
      "+ Key Size: 11   \tMethod: 1.\n",
      "+ Num Executors: None  \tExecutors/cores: None  \tExecutor Mem: None.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopFile.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/home/fcores/Spark_Blast/Standalone/-f\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopFile(PythonRDD.scala:276)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-457d45c0023c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2173\u001b[0m     \u001b[0;31m# Execute Main functionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}({}, {}, {}, {}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultipleQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQueryFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReferenceName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeySize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQueryFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReferenceName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeySize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoMultipleQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-457d45c0023c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(sc, sqlContext, queryFilename, referenceName, keySize, DoMultipleQuery, HashName)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mDoMultipleQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueryFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferenceName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeySize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m         \u001b[0mMultipleQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueryFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferenceName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeySize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-457d45c0023c>\u001b[0m in \u001b[0;36mQuery\u001b[0;34m(sc, sqlContext, queryFilename, referenceName, keySize, hashName)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mkeysdespl_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mkeysdespl_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculateQueryOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueryFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeySize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0mt_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-457d45c0023c>\u001b[0m in \u001b[0;36mCalculateQueryOffset\u001b[0;34m(sc, queryFilename, keySize)\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;34m'org.apache.hadoop.mapreduce.lib.input.TextInputFormat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;34m'org.apache.hadoop.io.LongWritable'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m         \u001b[0;34m'org.apache.hadoop.io.Text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m     )\n\u001b[1;32m   1398\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDDebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36mnewAPIHadoopFile\u001b[0;34m(self, path, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[1;32m    722\u001b[0m         jrdd = self._jvm.PythonRDD.newAPIHadoopFile(self._jsc, path, inputFormatClass, keyClass,\n\u001b[1;32m    723\u001b[0m                                                     \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                                                     jconf, batchSize)\n\u001b[0m\u001b[1;32m    725\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopFile.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/home/fcores/Spark_Blast/Standalone/-f\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopFile(PythonRDD.scala:276)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# SparkBlast_DoQuery program to perform blast querys on cassandra using spark\n",
    "# Usage: SparkBlast_DoQuery <Query_Files> <ReferenceName> [Key_size=11].\n",
    "\n",
    "from __future__ import print_function\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import Row, _infer_schema, _has_nulltype, _merge_type, _create_converter\n",
    "from pyspark.sql.types import StringType, ArrayType, LongType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import udf, upper, desc, collect_list, size, mean, length\n",
    "from cassandra.cluster import Cluster\n",
    "from operator import add\n",
    "import re\n",
    "from time import time, sleep\n",
    "import os, shutil, sys, subprocess\n",
    "import traceback\n",
    "from subprocess import PIPE, Popen\n",
    "import random\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import cython\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "## Constants\n",
    "DCassandraNodes = ['192.168.1.1', '192.168.1.2', '192.168.1.3', '192.168.1.4', '192.168.1.5', '192.168.1.6']\n",
    "#DCassandraNodes = ['192.168.1.3']\n",
    "DDoTesting = False\n",
    "DDebug = False\n",
    "DTiming = True\n",
    "DShowResult = False\n",
    "APP_NAME = \"SparkBlast_DoQuery\"\n",
    "DKeySize = 11\n",
    "DQueryFilename = '../Datasets/References/Query1.txt'\n",
    "DReferenceName = \"example\"\n",
    "DReferenceHashTableName = \"hash\"\n",
    "DReferenceContentTableName = \"sequences\"\n",
    "DCreateWindowWithPartitions = True\n",
    "DCreateBlocksDataFrame = True\n",
    "DPartitionBlockSize = 128  * 1024 \n",
    "DProcessingByPartitions = True\n",
    "DNumberPartitions = 200\n",
    "DMaxNumberStages = 3\n",
    "DBalanceGetKeysOffsesPartitions = False\n",
    "DBalanceAligmentsCalculation = False\n",
    "DAligmentMaxNumberStages = 10\n",
    "DMaxOffsetsQuery = 5\n",
    "DDoMultipleQuery = False\n",
    "DMinAligmentScore = 0.7\n",
    "DAligmentExtension = False\n",
    "#DAligmentExtensionLength = 15\n",
    "DAligmentExtensionLength = 6\n",
    "DBlockCacheSize = 3\n",
    "DCythonLibsPath = 'hdfs://babel.udl.cat/user/nando/cython_libs/'\n",
    "#DCythonLibsPath = '/tmp/cython_libs/'\n",
    "cyt_calculateMultipleQueryKeysDesplR = None\n",
    "\n",
    "# Error Handling\n",
    "DCassandraRetriesNumber = 5\n",
    "DCassandraRetryTimeout = 100/1000000.0 # 100 microsecs\n",
    "\n",
    "# Statistics \n",
    "DHdfsHomePath = \"hdfs://babel.udl.cat/user/nando/\"\n",
    "DHdfsTmpPath = DHdfsHomePath + \"Tmp/\"\n",
    "DHdfsOutputPath = DHdfsHomePath + \"Output/\"\n",
    "DCalculateStageStatistics = False\n",
    "DCalculateStatistics = True\n",
    "\n",
    "## Types\n",
    "# Enum Methods:\n",
    "ECreate2LinesData = 1\n",
    "ECreate1LineDataWithoutDependencies = 2\n",
    "ECreateBlocksData = 3\n",
    "DDefaultMethod = ECreate2LinesData\n",
    "    \n",
    "\n",
    "## Global Variables\n",
    "Method = DDefaultMethod\n",
    "CreateWindowWithPartitions = DCreateWindowWithPartitions\n",
    "BlockSize = DPartitionBlockSize\n",
    "DoMultipleQuery = DDoMultipleQuery\n",
    "NumberPartitions = DNumberPartitions\n",
    "DKeyMatchingThreshold = 15\n",
    "KeyMatchingThreshold = DKeyMatchingThreshold\n",
    "DKeyMatchingPercentageThreshold = 0.25\n",
    "StatisticsFileName = None\n",
    "YarnJobId = None\n",
    "key_size_bc = 0\n",
    "query_length_bc = 0\n",
    "gt0_bc = time()\n",
    "reference_name_bc = 0\n",
    "query_sequence_bc = 0\n",
    "content_block_size_bc = 0\n",
    "keysdespl_rdd = 0\n",
    "reference_keys = 0\n",
    "\n",
    "\n",
    "\n",
    "## Functions ##\n",
    "\n",
    "def Query(sc, sqlContext, queryFilename, referenceName, keySize=DKeySize, hashName=None):\n",
    "    dfc(\"Query\", sc, sqlContext, queryFilename, referenceName, keySize, hashName)\n",
    "    \n",
    "    # Broadcast Global variables\n",
    "    global DDebug, key_size_bc, gt0_bc, reference_name_bc, hash_name_bc\n",
    "    t0 = time()\n",
    "    gt0_bc = sc.broadcast(t0)\n",
    "    key_size_bc = sc.broadcast(keySize)\n",
    "    reference_name_bc = sc.broadcast(referenceName)\n",
    "    hash_name_bc = sc.broadcast(hashName)\n",
    "    \n",
    "    if (DTiming):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"++++++++++++ INITIAL STATISTICS {} +++++++++++++\".format(date_time))\n",
    "        print(\"+ Reference: {}  \\tQuery file: {}.\".format(referenceName, queryFilename))\n",
    "        print(\"+ Key Size: {}   \\tMethod: {}.\".format(keySize, Method))\n",
    "        print(\"+ Num Executors: {}  \\tExecutors/cores: {}  \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"+ Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Create Cassandra Sesion\n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "        try:\n",
    "            cluster = Cluster(DCassandraNodes)\n",
    "            session = cluster.connect()\n",
    "        except:\n",
    "            # Print exception info.\n",
    "            print(\"@@@@@ Capatured Exception\")\n",
    "            exc_info = sys.exc_info()\n",
    "            traceback.print_exception(*exc_info)\n",
    "            del exc_info\n",
    "            # Retry operation after a delay.\n",
    "            sleep(DCassandraRetryTimeout)\n",
    "            continue\n",
    "        break\n",
    "    if session is None:\n",
    "        raise  \n",
    " \n",
    "    # Steps:\n",
    "    #   1. Read and split query in keySize-Segments + Desplazament\n",
    "    #   2. Query Cassandra for the keys\n",
    "    #   3. Calculate Top-matching zones\n",
    "    #   4. Make extension in Top-matching zones.\n",
    "    \n",
    "\n",
    "    # 1. Read and split query in keySize-Segments + Desplazament\n",
    "    global keysdespl_rdd, reference_keys\n",
    "    t1 = time()\n",
    "    keysdespl_rdd, query_sequence = CalculateQueryOffset(sc, queryFilename, keySize)\n",
    "    t_read = time()-t1\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Query::QueryKeys:\")\n",
    "        keysdespl_rdd.persist()\n",
    "        print(keysdespl_rdd.take(1))\n",
    "        \n",
    "    # 2. Query Cassandra for the keys\n",
    "    t2 = time()\n",
    "    if (DProcessingByPartitions):\n",
    "        reference_keys = GetKeysOffsetsInReferenceByPartition(sc, session, referenceName, keysdespl_rdd)\n",
    "    else:\n",
    "        reference_keys = GetKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd)\n",
    "    t_qcass = time()-t2\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"Query::QueryKeys: Matching desplazaments: {}\".format(reference_keys.take(1)))\n",
    "        \n",
    "          \n",
    "    # 3. Calculate Top-matching offsets\n",
    "    t3 = time()\n",
    "    offsets_count = reference_keys.map(lambda off: (off,1)).reduceByKey(add)\n",
    "    top_offsets = offsets_count.sortBy(lambda kv: kv[1], False)\n",
    "    # Get Top-matching zones that exceed the threshold.\n",
    "    top_matching = offsets_count.filter(lambda kv: kv[1]>DKeyMatchingThreshold) \n",
    "    top_matchng.persist()\n",
    "    candidate_offsets = top_matching.count()\n",
    "    t_topmat = time()-t3\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Top-10 offsets: {}\".format(top_offsets.take(1)))\n",
    "        print(\"Top-Matching offsets: {}\".format(top_matching.take(1)))  \n",
    "      \n",
    "    # 4. Make extension in Top-matching zones.\n",
    "    t4 = time()\n",
    "    good_aligments = CalculateAligments(sc, session, query_sequence, referenceName, top_matching)\n",
    "    tc = time()\n",
    "    good_aligments.persist()\n",
    "    n_aligments = good_aligments.count()\n",
    "    t_ext = tc-t4\n",
    "    tt = tc - t0\n",
    "\n",
    "    if (DTiming):\n",
    "        print(\"# Time required for calculate {} aligments: {} seconds.\\n\".format(n_aligments,round(tt,3)))\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Good Aligments:\")\n",
    "        print(good_aligments.take(1))\n",
    "        \n",
    "    if (DShowResult):\n",
    "        map(ShowAligmentResult,good_aligments.collect())\n",
    "\n",
    "    global YarnJobId\n",
    "    ResultFile = DHdfsOutputPath+APP_NAME+\"_\"+ os.path.splitext(os.path.basename(queryFilename))[0] +\"_\"+referenceName+\"_\"+YarnJobId\n",
    "    print(\"Writing matching aligments in file {}\".format(ResultFile))\n",
    "    good_aligments.coalesce(1).saveAsTextFile(ResultFile)\n",
    "\n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "               \n",
    "    if (DTiming):\n",
    "        print(\"# Total time required for processing the query with {} keys: {} seconds.\".format(keysdespl_rdd.count(), round(tt,3)))\n",
    "\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"########################### FINAL STATISTICS Single QUERY {} ###########################\".format(date_time))\n",
    "        print(\"# Reference: {}  \\tFile: {}   \\tQuerys: {}.\".format(ReferenceName, queryFilename, 1))\n",
    "        print(\"# Key Size: {}            \\tMethod: {}.\".format(KeySize, Method))\n",
    "        print(\"# Num Executors: {}      \\tExecutors/cores: {}      \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"# Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"# Total Time: {}         \\tData Read Time: {}       \\tCassandra Read Time: {} .\".format(round(tt,3), round(t_read,3), round(t_qcass,3)))\n",
    "        print(\"# Top matching Time: {}   \\tAlig. Extension Time: {}.\".format(round(t_topmat,3), round(t_ext,3)))\n",
    "        print(\"############################################################################################\")\n",
    "        #result.persist()\n",
    "        #print(\"# Total time required for processing {} keys using {} partitions in {} seconds.\".format(result.count(), result.getNumPartitions(), round(tt,3)))\n",
    "        #print(\"# Reference data size: {} MBytes.\\n\".format(round(get_size(ReferenceFilename)/(1024.0*1024.0),3)))\n",
    "        if (StatisticsFileName):\n",
    "            write_statistics_single_query(StatisticsFileName, queryFilename, ReferenceName, KeySize, date_time, 1, candidate_offsets, n_aligments, offsets_count.getNumPartitions() ,tt, t_read, t_qcass, t_topmat, t_ext)\n",
    "        \n",
    "    print(\"Done.\")\n",
    "        \n",
    "    return good_aligments\n",
    "\n",
    "\n",
    "def pquerykeys(qk):\n",
    "    res = []\n",
    "    for off in qk[1]:\n",
    "        res.append((qk[0], off[0], off[1])) \n",
    "    return ([x for x in res])\n",
    "\n",
    "def addDespl(off_list,despl):\n",
    "    res = [off + despl for off in off_list] \n",
    "    return res\n",
    "    \n",
    "def flatten(off_list):\n",
    "    return ([y for x in off_list for y in x])\n",
    "\n",
    "def countby(off_list):\n",
    "    return(collections.Counter(off_list).items())\n",
    "\n",
    "def countbyfilter(off_list):\n",
    "    return(filter(lambda (k,v): v>DKeyMatchingThreshold,collections.Counter(off_list).items()))\n",
    "\n",
    "def countbyfilterdespl(off_list):\n",
    "    return(map(lambda (k,v): k, filter(lambda (k,v): v>DKeyMatchingThreshold,collections.Counter(off_list).items())))\n",
    "\n",
    "\n",
    "#map(lambda (k,v): k,filter(lambda (k,v): v>2,counter.items()))\n",
    "\n",
    "def MultipleQuery(sc, sqlContext, queryFilename, referenceName, keySize=DKeySize, hashName=None):\n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"MultipleQuery({}, {}, {}).\".format( queryFilename, referenceName, keySize, hashName))\n",
    "    dfc(\"MultipleQuery\",sc, sqlContext, queryFilename, referenceName, keySize, hashName)\n",
    "    \n",
    "    # Broadcast Global variables\n",
    "    global key_size_bc, gt0_bc, reference_name_bc, hash_name_bc\n",
    "    t0 = time()\n",
    "    gt0_bc = sc.broadcast(t0)\n",
    "    key_size_bc = sc.broadcast(keySize)\n",
    "    reference_name_bc = sc.broadcast(referenceName)\n",
    "    hash_name_bc = sc.broadcast(hashName)\n",
    "\n",
    "    nquerys = -1\n",
    "    nkeysdespl = -1\n",
    "    candidates_offsets = -1\n",
    "    npartitions = -1\n",
    "    n_aligments = -1\n",
    "    query_sequence = sc.emptyRDD()\n",
    "    keysdespl_rdd = sc.emptyRDD()\n",
    "    reference_keys = sc.emptyRDD()\n",
    "    good_aligments = sc.emptyRDD()\n",
    "\n",
    "    if (DTiming):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"++++++++++++ INITIAL STATISTICS {} +++++++++++++\".format(date_time))\n",
    "        print(\"+ Reference: {}  \\tQuery file: {}.\".format(ReferenceName, queryFilename))\n",
    "        print(\"+ Key Size: {}   \\tMethod: {}.\".format(KeySize, Method))\n",
    "        print(\"+ Num Executors: {}  \\tExecutors/cores: {}  \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"+ Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Create Cassandra Sesion\n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                cluster = Cluster(DCassandraNodes)\n",
    "                session = cluster.connect()\n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "    if session is None:\n",
    "        raise\n",
    "\n",
    "\n",
    "  \n",
    "    # 1. Read and split query in keySize-Segments + Desplazament\n",
    "    global keysdespl_rdd, reference_keys\n",
    "    t1 = time()\n",
    "    keysdespl_rdd, query_sequence = CalculateMultipleQueryOffset(sc, sqlContext, queryFilename, keySize)\n",
    "    if (DCalculateStageStatistics):\n",
    "        query_sequence.persist()\n",
    "        keysdespl_rdd.persist()\n",
    "        nquerys = 0\n",
    "        if (query_sequence!=sc.emptyRDD()):\n",
    "            nquerys = query_sequence.count()\n",
    "        nkeysdespl = 0\n",
    "        if (keysdespl_rdd!=sc.emptyRDD()):\n",
    "            nkeysdespl = keysdespl_rdd.count()\n",
    "        print(\"@@@@@ nkeysdespl: {}\".format(nkeysdespl))\n",
    "    t_read = time()-t1\n",
    "    print(\"@@@@@ 1-Read and split query: {}\".format(round(t_read,3)))  \n",
    "    #print(keysdespl_rdd.take(100))\n",
    "\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"######################### Number of partitions query_sequence: {}\".format(query_sequence.getNumPartitions()))\n",
    "        print(\"######################### Number of partitions keysdespl_rdd: {}\".format(keysdespl_rdd.getNumPartitions()))\n",
    "        keysdespl_rdd.persist()\n",
    "        print(\"Query::QueryKeys:\")\n",
    "        print(keysdespl_rdd.take(1))\n",
    "        print(query_sequence.take(1))\n",
    "   \n",
    "   \n",
    "    # 2. Query Cassandra for the keys\n",
    "    t2 = time()\n",
    "    reference_keys = GetMultipleKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd)\n",
    "    if (DCalculateStageStatistics):    \n",
    "        reference_keys.persist()\n",
    "        print(\"@@@@@ Reference_keys: {}\".format(reference_keys.count()))     \n",
    "    t_qcass = time()-t2\n",
    "    print(\"@@@@@ 2-Query Cassandra for {} keys: {}\".format(reference_keys.count(), round(t_qcass,3)))  \n",
    "        \n",
    "    if (DDebug):\n",
    "        reference_keys.persist()\n",
    "        print(\"######################### Number of partitions reference_keys: {}\".format(reference_keys.getNumPartitions()))\n",
    "        print(\"Query::QueryKeys: Matching desplazaments: {}\".format(reference_keys.take(1)))\n",
    " \n",
    "    # Joining querys & keys offsets\n",
    "    if (DDebug):\n",
    "        query_sequence.persist()\n",
    "        keysdespl_rdd.persist()\n",
    "        print(\"MultipleQuery:: Querys: {}\".format(query_sequence.take(1)))\n",
    "        print(\"MultipleQuery:: Query Keys: {}\".format(keysdespl_rdd.take(1)))\n",
    "        print(\"MultipleQuery:: Matching desplazaments: {}\".format(reference_keys.take(1)))\n",
    "        \n",
    "    t3 = time()\n",
    "    if (not reference_keys.isEmpty()):\n",
    "        query_keys_df = keysdespl_rdd.flatMap(pquerykeys).toDF([\"Query\", \"Key\" , \"Despl\"])\n",
    "        reference_keys_df = reference_keys.toDF([\"Key2\" , \"Offsets\"])\n",
    "        query_keys_df.groupBy('Key')\n",
    "        reference_keys_df.groupBy('Key2')\n",
    "        if (DDebug):\n",
    "            reference_keys_df.persist()\n",
    "            query_keys_df.persist()\n",
    "            print(\"MultipleQuery:: query_keys_df: \")\n",
    "            query_keys_df.show(10)\n",
    "            print(\"MultipleQuery:: reference_keys_df: \")\n",
    "            reference_keys_df.show(10)\n",
    "\n",
    "        joined_df = query_keys_df.join(reference_keys_df, query_keys_df.Key == reference_keys_df.Key2)\n",
    "        joined_df = joined_df.drop(joined_df.Key2)\n",
    "        \n",
    "        #joined_df.printSchema()\n",
    "        t3a = time()\n",
    "        t_join = t3a-t3\n",
    "\n",
    "        if (DDebug):\n",
    "            joined_df.persist()\n",
    "            print(\"######################### Number of partitions joined_df: {}\".format(joined_df.rdd.getNumPartitions()))\n",
    "            print(\"MultipleQuery:: joined_df:\")\n",
    "            joined_df.show(10)\n",
    "\n",
    "        addDespl_udf = udf(addDespl,  ArrayType(LongType())) \n",
    "        joined_df = joined_df.withColumn(\"Offsets\", addDespl_udf('Offsets','Despl'))\n",
    "        t3b = time()\n",
    "        t_adddespl = t3b-t3a\n",
    "\n",
    "        if (DDebug):\n",
    "            print(\"MultipleQuery:: joined_df addind despl:\")\n",
    "            joined_df.show()\n",
    "\n",
    "        grouped_df = joined_df.drop(joined_df.Key).groupby('Query').agg(F.collect_list(\"Offsets\")).withColumnRenamed(\"collect_list(Offsets)\", \"Offsets\") \n",
    "        flatten_udf = udf(flatten,  ArrayType(LongType())) \n",
    "        grouped_df = grouped_df.withColumn(\"Offsets\", flatten_udf('Offsets'))\n",
    "        if (DCalculateStageStatistics):  \n",
    "            grouped_df.persist()\n",
    "            print(\"@@@@@ grouped_df: {}\".format(grouped_df.count()))\n",
    "        t_joining = time()-t3\n",
    "        print(\"@@@@@ 3-Joining querys & keys offsets: {}\".format(round(t_joining,3)))\n",
    "\n",
    "        if (DDebug):\n",
    "            grouped_df.persist()\n",
    "            print(\"MultipleQuery:: grouped_df flatten despl:\")\n",
    "            grouped_df.show()\n",
    "\n",
    "\n",
    "\n",
    "        # 3. Calculate Top-matching offsets\n",
    "        t4 = time()\n",
    "        offcount_schema = ArrayType(StructType([\n",
    "                                        StructField(\"Offset\", IntegerType(), False),\n",
    "                                        StructField(\"count\", IntegerType(), False)\n",
    "                                    ]))\n",
    "        countby_udf = udf(countbyfilter,  offcount_schema) \n",
    "        counted_df = grouped_df.withColumn(\"Offsets\", countby_udf('Offsets'))\n",
    "\n",
    "        if (DDebug):\n",
    "            counted_df.persist()\n",
    "            print(\"MultipleQuery:: countbyfilter_udf despl:\")\n",
    "            print(counted_df.take(1))\n",
    "\n",
    "        countby_udf = udf(countbyfilterdespl, ArrayType(LongType())) \n",
    "        counted_df = grouped_df.withColumn(\"Offsets\", countby_udf('Offsets'))\n",
    "\n",
    "        if (DDebug):\n",
    "            print(\"MultipleQuery:: countbyfilterdespl despl:\")\n",
    "            counted_df.persist()\n",
    "            print(counted_df.take(1))\n",
    "\n",
    "        filter_df = counted_df.filter(size('Offsets')>0)\n",
    "        query_sequence_df = query_sequence.toDF()\n",
    "        query_offset_df = filter_df.join(query_sequence_df, query_sequence_df._c1 == filter_df.Query)\n",
    "        query_offset_df = query_offset_df.drop(query_offset_df._c1).drop(query_offset_df._c2).withColumnRenamed(\"_c0\", \"QuerySeq\")\n",
    "        if (DCalculateStageStatistics):  \n",
    "            query_offset_df.persist()\n",
    "            if (query_offset_df.rdd==sc.emptyRDD()):\n",
    "                candidates_offsets = 0\n",
    "                npartitions = 0\n",
    "            else:\n",
    "                candidates_offsets = query_offset_df.select(F.sum(F.size('Offsets'))).collect()[0][0]\n",
    "                npartitions = query_offset_df.rdd.getNumPartitions()\n",
    "            if (candidates_offsets is None):\n",
    "                candidates_offsets = 0\n",
    "            if (npartitions is None):\n",
    "                npartitions = 0\n",
    "            print(\"@@@@@ candidates_offsets: {}\".format(candidates_offsets))\n",
    "        t_topmat = time()-t4\n",
    "        print(\"@@@@@ 4-Calculate Top-matching offsets: {}\".format(round(t_topmat,3)))\n",
    "\n",
    "        if (DDebug):\n",
    "            print(\"######################### Number of partitions query_offset_df: {}\".format(query_offset_df.rdd.getNumPartitions()))\n",
    "            print(\"MultipleQuery:: Candidate Offsets: {}\".format(candidates_offsets ))\n",
    "            print(\"MultipleQuery:: query_offset_df:\")\n",
    "            print(query_offset_df.take(1))\n",
    "\n",
    "\n",
    "        # 4. Make extension in Top-matching zones.\n",
    "        t5 = time()\n",
    "        good_aligments = CalculateAligmentsMultipleQuery(sc, session, query_offset_df, referenceName)\n",
    "        if (DCalculateStageStatistics):      \n",
    "            good_aligments.persist()\n",
    "            if (good_aligments==sc.emptyRDD()):\n",
    "                n_aligments = 0\n",
    "            else:\n",
    "                n_aligments = good_aligments.count()\n",
    "            print(\"@@@@@ N good aligments: {}\".format(n_aligments))\n",
    "        t6 = time()\n",
    "        t_ext = t6-t5\n",
    "        print(\"@@@@@ 5-Aligment extension: {}\".format(round(t_ext,3)))\n",
    "        tt = t6-t0\n",
    "        print(\"@@@@@ TOTAL TIME: {}\".format(round(tt,3)))\n",
    "\n",
    "\n",
    "        if (DDebug):\n",
    "            print(\"Good Aligments:\")       \n",
    "            print(good_aligments.collect())\n",
    "\n",
    "        if (DShowResult):\n",
    "            good_aligments.persist()\n",
    "            map(ShowMultipleQueryAligmentResult,good_aligments.collect()) \n",
    "\n",
    "        global YarnJobId\n",
    "        ResultFile = DHdfsOutputPath+APP_NAME+\"_\"+ os.path.splitext(os.path.basename(queryFilename))[0] +\"_\"+referenceName+\"_\"+YarnJobId\n",
    "        print(\"Writing matching aligments in file {}\".format(ResultFile))\n",
    "        good_aligments.coalesce(1).saveAsTextFile(ResultFile)\n",
    "                \n",
    "    else:\n",
    "        # Not Matching keys.\n",
    "        candidates_offsets=0\n",
    "        n_aligments=0\n",
    "        tt = t3-t0\n",
    "        t_joining=-1\n",
    "        t_topmat=-1\n",
    "        t_ext=-1\n",
    "        print(\"Not matching keys in reference sequence. No hits found\")\n",
    "\n",
    "        \n",
    "        \n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "\n",
    "        \n",
    "        \n",
    "    if (DTiming):\n",
    "        if (DCalculateStatistics):   \n",
    "            if (not DCalculateStageStatistics):\n",
    "                query_sequence.persist()\n",
    "                keysdespl_rdd.persist()\n",
    "                nquerys = 0\n",
    "                if (query_sequence!=sc.emptyRDD()):\n",
    "                    nquerys = query_sequence.count()\n",
    "                nkeysdespl = 0\n",
    "                if (keysdespl_rdd!=sc.emptyRDD()):\n",
    "                    nkeysdespl = keysdespl_rdd.count()\n",
    "                if (reference_keys.isEmpty() or query_offset_df.rdd==sc.emptyRDD()):\n",
    "                    candidates_offsets = 0\n",
    "                    npartitions = 0\n",
    "                else:\n",
    "                    query_offset_df.persist()\n",
    "                    candidates_offsets = query_offset_df.select(F.sum(F.size('Offsets'))).collect()[0][0]\n",
    "                    npartitions = query_offset_df.rdd.getNumPartitions()\n",
    "                if (good_aligments==sc.emptyRDD()):\n",
    "                    n_aligments = 0\n",
    "                else:\n",
    "                    good_aligments.persist()\n",
    "                    n_aligments = good_aligments.count()      \n",
    "                    \n",
    "        print(\"# Total time required for processing the {} query with {} keys: {} seconds.\".format(nquerys, keysdespl_rdd.count(), round(tt,3)))\n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        print(\"########################### FINAL STATISTICS Multiple QUERY {} ###########################\".format(date_time))\n",
    "        print(\"# Reference: {}  \\tFile: {}   \\tQuerys: {}.\".format(ReferenceName, queryFilename, nquerys))\n",
    "        print(\"# Key Size: {}            \\tMethod: {}.\".format(KeySize, Method))\n",
    "        print(\"# Num Executors: {}      \\tExecutors/cores: {}      \\tExecutor Mem: {}.\".format(executors, cores, memory))\n",
    "        #print(\"# Hash Groups Size: {}  \\tPartition Size: {}  \\tContent Block Size: {}.\".format(HashGroupsSize, BlockSize, ContentBlockSize))\n",
    "        print(\"# Total Time: {}         \\tData Read Time: {}       \\tCassandra Read Time: {} .\".format(round(tt,3), round(t_read,3), round(t_qcass,3)))\n",
    "        print(\"# DF Joining Time: {}    \\tTop matching Time: {}   \\tAlig. Extension Time: {}.\".format(round(t_joining,3), round(t_topmat,3), round(t_ext,3)))\n",
    "        print(\"# Num Aligments: {}      \\tGood Aligments: {}.\".format(candidates_offsets, n_aligments))\n",
    "        print(\"############################################################################################\")\n",
    "        #result.persist()\n",
    "        #print(\"# Total time required for processing {} keys using {} partitions in {} seconds.\".format(result.count(), result.getNumPartitions(), round(tt,3)))\n",
    "        #print(\"# Reference data size: {} MBytes.\\n\".format(round(get_size(ReferenceFilename)/(1024.0*1024.0),3)))\n",
    "        if (StatisticsFileName):\n",
    "            write_statistics_multiple_query(StatisticsFileName, queryFilename, ReferenceName, KeySize, date_time, nquerys, candidates_offsets, n_aligments, npartitions, tt, t_read, t_qcass, t_joining, t_topmat, t_ext)\n",
    "\n",
    "            \n",
    "    print(\"Done.\")\n",
    "            \n",
    "    return good_aligments\n",
    "    \n",
    "    \n",
    "def ShowAligmentResult(aligment):\n",
    "    global reference_name_bc\n",
    "    print(\"Find in \"+reference_name_bc.value+\": \"+str(aligment[0])+' ---> '+str(aligment[1])+\", align score: \"+str(aligment[2]))\n",
    "    Display(aligment[3], aligment[4])\n",
    "    \n",
    "    \n",
    "def ShowMultipleQueryAligmentResult(aligment):\n",
    "    global reference_name_bc\n",
    "    print(\"Query \" +str(aligment[0])+ \" find in \"+reference_name_bc.value+\": \"+str(aligment[1])+' ---> '+str(aligment[2])+\", align score: \"+str(aligment[3]))\n",
    "    Display(aligment[4], aligment[5])\n",
    "\n",
    "    \n",
    "def CalculateAligments(sc, session, querySequence, referenceName, top_matching):\n",
    "    dfc(\"CalculateAligments\", sc, session, querySequence, referenceName, top_matching)\n",
    "    \n",
    "    # Get Content block size  \n",
    "    querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=0\"\n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                resultSelect = session.execute(querySelect)\n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                print(querySelect)\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "    if querySelect is None:\n",
    "        raise  \n",
    "\n",
    "    if resultSelect:\n",
    "        contentBlockSize = resultSelect[0].size\n",
    "        if (DDebug):\n",
    "            print(\"CalculateAligments::Content block size: {}.\".format(contentBlockSize))\n",
    "    else:\n",
    "        print(\"ERROR: Refererence contante table {} do not exist.\".format(referenceName + \".\" + DReferenceContentTableName))     \n",
    "            \n",
    "    aligments = DistributeAligments(sc, querySequence, referenceName, top_matching, contentBlockSize)\n",
    "    \n",
    "    #quality_aligments = aligments.filter(lambda algn: algn[2]>DMinAligmentScore) \n",
    "           \n",
    "    return quality_aligments\n",
    "    \n",
    "\n",
    "def DistributeAligments(sc, querySequence, referenceName, offsets_count, contentBlockSize):\n",
    "    dfc(\"DistributeAligments\", sc, querySequence, referenceName, offsets_count, contentBlockSize)\n",
    "    \n",
    "    global query_sequence_bc, content_block_size_bc\n",
    "    query_sequence_bc = sc.broadcast(querySequence)\n",
    "    content_block_size_bc = sc.broadcast(contentBlockSize)\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"DistributeAligments: {} {}\".format(querySequence, referenceName))\n",
    "        print(\"offsets_count: \",)\n",
    "        print(offsets_count.take(1))\n",
    "    \n",
    "    #CassandraAligmentR(offsets_count.collect()[0][0])\n",
    "    aligments = offsets_count.map(lambda off: CassandraAligmentR(off[0]))\n",
    "    \n",
    "    if (DDebug and not aligments.isEmpty()):\n",
    "        print(\"ResultAligments: {}\".format(aligments.take(1)))\n",
    "    \n",
    "    return aligments\n",
    "\n",
    "\n",
    "def CassandraAligment(record):\n",
    "    return CassandraAligmentR(record[0])\n",
    "\n",
    "def CassandraAligmentR(offset):\n",
    "    global reference_name_bc, query_sequence_bc, content_block_size_bc, key_size_bc\n",
    "    querySequence = query_sequence_bc.value\n",
    "    queryLength = len(querySequence)\n",
    "    referenceName = reference_name_bc.value\n",
    "    contentBlockSize = content_block_size_bc.value\n",
    "    keySize  = key_size_bc.value\n",
    "    \n",
    "    # calculate reference begin and end\n",
    "    if DAligmentExtension:\n",
    "        reference_seq_begin = offset - (queryLength + keySize - 5)\n",
    "        reference_seq_end = reference_seq_begin + queryLength + keySize\n",
    "    else:\n",
    "        reference_seq_begin = offset - (queryLength)\n",
    "        reference_seq_end = reference_seq_begin + queryLength\n",
    "        \n",
    "    reference_seq_begin = offset - (queryLength)\n",
    "    reference_seq_end = reference_seq_begin + queryLength\n",
    "\n",
    "#    reference_seq_begin = offset - (queryLength + DAligmentExtensionLength)\n",
    "#    reference_seq_end = reference_seq_begin + queryLength + DAligmentExtensionLength\n",
    "        \n",
    "    \n",
    "    # Calculate reference content start and end block\n",
    "    bbegin = reference_seq_begin/contentBlockSize\n",
    "    bend = reference_seq_end/contentBlockSize\n",
    "    \n",
    "    # Get Reference Content Sequence\n",
    "    end, contentSequence = GetRefereceContentBlocks(referenceName, bbegin, bend)\n",
    "    if (contentSequence is None):\n",
    "        return (-1,-1, 0, \"Error in GetRefereceContentBlocks\",\"Block Id %d-%d not exist inf table %s\" % (bbegin, bend, referenceName))\n",
    "    #print(contentSequence.first())\n",
    "    if (DDebug):\n",
    "        print(\"Reference Content Sequence {}-{}: {}\".format(bbegin, bend, contentSequence))\n",
    "        \n",
    "    if (reference_seq_end>end):\n",
    "        contentSequence = contentSequence + \" \" * (reference_seq_end-end+1)\n",
    "    \n",
    "    # Calculate Alignment\n",
    "    boffset = reference_seq_begin-(contentBlockSize * bbegin) \n",
    "    if (DDebug):\n",
    "        print(\"Short Reference Content Sequence {}-{}: {}\".format(bbegin, bend, contentSequence[boffset:reference_seq_end]))\n",
    "    align_seq1, align_seq2, align_score, align_off = DoAligment(querySequence, contentSequence[boffset:reference_seq_end])\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"find in \"+referenceName+\": \"+str(reference_seq_begin+align_off)+' ---> '+str(reference_seq_begin+align_off+queryLength-1)+\", align score: \"+str(align_score))\n",
    "        Display(align_seq1, align_seq2)\n",
    "        \n",
    "    if (align_score>DMinAligmentScore):\n",
    "        return (offset+align_off, offset+align_off+queryLength-1, align_score, align_seq1, align_seq2)\n",
    "    else:\n",
    "        return sc.emptyRDD()\n",
    "\n",
    "\n",
    "def CalculateAligmentsMultipleQuery(sc, session, querySequences, referenceName):\n",
    "    dfc(\"CalculateAligmentsMultipleQuery\", sc, session, querySequences, referenceName)\n",
    "    \n",
    "    # Get Content block size\n",
    "    querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=0\"\n",
    "\n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                resultSelect = session.execute(querySelect)\n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                print(querySelect)\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "    if querySelect is None:\n",
    "        raise  \n",
    "    \n",
    "    if resultSelect:\n",
    "        contentBlockSize = resultSelect[0].size\n",
    "        if (DDebug):\n",
    "            print(\"CalculateAligmentsMultipleQuery::Content block size: {}.\".format(contentBlockSize))\n",
    "    else:\n",
    "        print(\"ERROR: Refererence contante table {} do not exist.\".format(referenceName + \".\" + DReferenceContentTableName))\n",
    "    \n",
    "    aligments = DistributeAligmentsMultipleQuery(sc, querySequences, referenceName, contentBlockSize)\n",
    "    print(aligments)\n",
    "    if (DDebug):\n",
    "        aligments.persist()\n",
    "    \n",
    "    if not aligments.isEmpty():\n",
    "        #print(\"CalculateAligmentsMultipleQuery::aligments: {}\".format(aligments.take(10)))\n",
    "        #quality_aligments = aligments.filter(lambda algn: algn[3]>DMinAligmentScore)    \n",
    "        return aligments\n",
    "    else:\n",
    "        return sc.emptyRDD()\n",
    "    \n",
    "\n",
    "def DistributeAligmentsMultipleQuery(sc, querySequences, referenceName, contentBlockSize):\n",
    "    dfc(\"DistributeAligmentsMultipleQuery\", sc, querySequences, referenceName, contentBlockSize)\n",
    "    \n",
    "    global content_block_size_bc\n",
    "    content_block_size_bc = sc.broadcast(contentBlockSize)\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"DistributeAligmentsMultipleQuery: {} \".format(referenceName))\n",
    "        print(\"querySequences: \",)\n",
    "        print(querySequences.take(1))\n",
    "    \n",
    "    #CassandraAligmentR(offsets_count.collect()[0][0])\n",
    "    #global YarnJobId  \n",
    "    #querySequences.write.parquet(DHdfsTmpPath+'querySequences_df_'+YarnJobId+'.parquet')\n",
    "    \n",
    "    if (DBalanceAligmentsCalculation and querySequences!=sc.emptyRDD()):\n",
    "\n",
    "        print(\"###### ORIGINAL RDD ############################################################\")\n",
    "        ShowBalanceStatistics(querySequences.rdd)\n",
    "        print(\"###########################################################\")\n",
    "\n",
    "        # Balancing\n",
    "        balancedQuerySequences = querySequences.rdd.flatMap(BalancingQuerys)\n",
    "\n",
    "        # Calculate Number of Partitions    \n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        if executors is None or cores is None:\n",
    "            total_cores = 1\n",
    "        else:\n",
    "            total_cores = int(executors) * int(cores)\n",
    "        max_partitions = DAligmentMaxNumberStages * total_cores\n",
    "        balancedQuerySequences.persist()\n",
    "        naligments = balancedQuerySequences.count()\n",
    "        if (naligments>max_partitions):\n",
    "            NumberPartitions = max_partitions\n",
    "        else:\n",
    "            NumberPartitions = naligments\n",
    "            \n",
    "        if (NumberPartitions==0):\n",
    "            NumberPartitions = 1\n",
    "\n",
    "        balancedQuerySequences = balancedQuerySequences.repartition(NumberPartitions)\n",
    "\n",
    "        print(\"###### BALANCE RDD {} ############################################################\".format(NumberPartitions))\n",
    "        ShowBalanceStatistics(balancedQuerySequences)\n",
    "        print(\"###########################################################\")\n",
    "\n",
    "    else:\n",
    "        balancedQuerySequences = querySequences.rdd\n",
    "\n",
    "    aligments = balancedQuerySequences.flatMap(CassandraAligmentMultipleQuery)\n",
    "    \n",
    "    if (DDebug and aligments.count()>0):\n",
    "        print(\"ResultAligments: {}\".format(aligments.take(1)))\n",
    "    \n",
    "    return aligments\n",
    "\n",
    "\n",
    "def CassandraAligmentMultipleQuery(record):\n",
    "    #if (DDebug and record[0]!=4):\n",
    "    #    return ([(record[0], -1, -1, 0, \"Not Processed\",\"%s\" % (record[2]))])\n",
    "    return CassandraAligmentMultipleQueryR(record[0], record[1], record[2])\n",
    "\n",
    "def CassandraAligmentMultipleQueryR(queryId, offsetList, querySequence):\n",
    "    dfc(\"CassandraAligmentMultipleQueryR\", queryId, offsetList, querySequence)\n",
    "    \n",
    "    global reference_name_bc, content_block_size_bc, key_size_bc\n",
    "    referenceName = reference_name_bc.value\n",
    "    contentBlockSize = content_block_size_bc.value\n",
    "    keySize  = key_size_bc.value\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"CassandraAligmentMultipleQueryR ({}, {}, {}).\".format(queryId, offsetList, querySequence))\n",
    "    \n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                cluster = Cluster(DCassandraNodes)\n",
    "                session = cluster.connect()  \n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "    if session is None:\n",
    "        raise  \n",
    "    \n",
    "    queryLength = len(querySequence)\n",
    "    \n",
    "    aligments = []\n",
    "    contentSequence = \"\"\n",
    "    #print(\"Offsets {} sort: {}\".format(offsetList, offsetList.sort()))\n",
    "    offsetList.sort()\n",
    "    for offset in offsetList:\n",
    "        if (DDebug):\n",
    "            print(\"CassandraAligmentMultipleQueryR:: Processing offset {}.\".format(offset))\n",
    "            \n",
    "        # calculate reference begin and end\n",
    "        reference_seq_begin = offset - (queryLength + keySize - 5)\n",
    "        reference_seq_end = reference_seq_begin + queryLength + keySize\n",
    "        \n",
    "        \n",
    "        if DAligmentExtension:\n",
    "            reference_seq_begin = offset - int(queryLength + math.floor(DAligmentExtensionLength/2.0))\n",
    "            reference_seq_end = reference_seq_begin + queryLength + int(math.ceil(DAligmentExtensionLength/2.0))\n",
    "        else:\n",
    "            reference_seq_begin = offset - (queryLength)\n",
    "            reference_seq_end = reference_seq_begin + queryLength\n",
    "        \n",
    "        # Calculate reference content start and end block\n",
    "        bbegin = reference_seq_begin/contentBlockSize\n",
    "        bend = reference_seq_end/contentBlockSize\n",
    "        \n",
    "        # Get Reference Content Sequence\n",
    "        end, contentSequence = GetRefereceContentBlocksCache(session, referenceName, bbegin, bend)\n",
    "        if (contentSequence is None):\n",
    "            print(\"ERROR CassandraAligmentMultipleQueryR: CassandraAligmentMultipleQueryR ({}, {}, {}).\".format(queryId, offsetList, querySequence))\n",
    "            continue\n",
    "        if (DDebug):\n",
    "            print(\"Reference Content Sequence {}-{}: {}\".format(bbegin, bend, contentSequence))\n",
    "\n",
    "        if (reference_seq_end>end):\n",
    "            contentSequence = contentSequence + \" \" * (reference_seq_end-end+1)\n",
    "            \n",
    "        # Calculate Alignment\n",
    "        boffset = reference_seq_begin-(contentBlockSize * bbegin) \n",
    "        if (DDebug):\n",
    "            print(\"Short Reference Content Sequence {}-{}, {}-{}: {}\".format(bbegin, bend, reference_seq_begin, reference_seq_end, contentSequence[boffset:reference_seq_end]))\n",
    "        align_seq1, align_seq2, align_score, align_off = DoAligment(querySequence, contentSequence[boffset:reference_seq_end+1])\n",
    "\n",
    "        if (DDebug):\n",
    "            print(\"find in \"+referenceName+\": \"+str(reference_seq_begin+align_off)+' ---> '+str(reference_seq_begin+align_off+queryLength-1)+\", align score: \"+str(align_score))\n",
    "            Display(align_seq1, align_seq2)\n",
    "            \n",
    "        if (align_score>DMinAligmentScore):          \n",
    "            aligments.append((queryId, offset+align_off, offset+align_off+queryLength-1, align_score, align_seq1, align_seq2))\n",
    "        \n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    if (False and len(aligments)==0):\n",
    "        print(\"CassandraAligmentMultipleQueryR {} {} {}\".format(queryId, offsetList, querySequence))\n",
    "        aligments.append((queryId, -1, -1, 0, \"Error in GetRefereceContentBlocks\",\"Block Id %d-%d not exist inf table %s - offsets %s\" % (bbegin, bend, referenceName,offsetList)))\n",
    "\n",
    "    return(aligments)\n",
    "    \n",
    "   \n",
    "   \n",
    "def GetRefereceContentBlocks(referenceName, bbegin, bend):\n",
    "    dfc(\"GetRefereceContentBlocks\", referenceName, bbegin, bend)\n",
    "    \n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()  \n",
    "    \n",
    "    contentSequence = \"\"\n",
    "    blocks_read = 0\n",
    "    end = 0\n",
    "    for block in range(bbegin, bend+1):\n",
    "        # Get Content block\n",
    "        if (DDebug):\n",
    "            print(\"Reading block {} froma {} table.\".format(block,referenceName))\n",
    "        querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=%s\"\n",
    "        resultSelect = ses.execute(querySelect, [block])\n",
    "        if resultSelect:\n",
    "            contentSequence = contentSequence + resultSelect[0].value\n",
    "            blocks_read +=1\n",
    "            end = resultSelect[0].offset + resultSelect[0].size\n",
    "            \n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    if (blocks_read>0):\n",
    "        return (end, contentSequence)\n",
    "    else:\n",
    "        print(\"ERROR: Refererence Content block id {} do not exist in table {} .\".format(end+1,referenceName + \".\" + DReferenceContentTableName))\n",
    "        return (0,None)\n",
    "      \n",
    "        \n",
    "        \n",
    "def static_vars(**kwargs):\n",
    "    def decorate(func):\n",
    "        for k in kwargs:\n",
    "            setattr(func, k, kwargs[k])\n",
    "        return func\n",
    "    return decorate\n",
    "\n",
    "@static_vars(BlockCache = collections.defaultdict(list))\n",
    "def GetRefereceContentBlocksCache(ses, referenceName, bbegin, bend):\n",
    "    \n",
    "    contentSequence = \"\"\n",
    "    blocks_read = 0\n",
    "    end = 0\n",
    "    for block in range(bbegin, bend+1):\n",
    "        if block in GetRefereceContentBlocksCache.BlockCache:\n",
    "            if (DDebug):\n",
    "                print(\"HIT block cache {} -> {}.\".format(block,GetRefereceContentBlocksCache.BlockCache[block][0]))\n",
    "            contentSequence = contentSequence + GetRefereceContentBlocksCache.BlockCache[block][1]\n",
    "            blocks_read +=1\n",
    "            end = GetRefereceContentBlocksCache.BlockCache[block][0] + len(GetRefereceContentBlocksCache.BlockCache[block][1]) \n",
    "        else:\n",
    "            # Get Content block\n",
    "            if (DDebug):\n",
    "                print(\"MISS block {} Reading froma {} table.\".format(block,referenceName))\n",
    "            querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceContentTableName + \" WHERE blockid=%s\"\n",
    "            resultSelect = ses.execute(querySelect, [block])\n",
    "            if resultSelect:\n",
    "                contentSequence = contentSequence + resultSelect[0].value\n",
    "                blocks_read +=1\n",
    "                end = resultSelect[0].offset + resultSelect[0].size\n",
    "                #Put Block in cache.\n",
    "                GetRefereceContentBlocksCache.BlockCache[block]=(resultSelect[0].offset, resultSelect[0].value)\n",
    "                #Pop Blcok\n",
    "                if block-DBlockCacheSize in GetRefereceContentBlocksCache.BlockCache:\n",
    "                    GetRefereceContentBlocksCache.BlockCache.pop(block-DBlockCacheSize)\n",
    "            else:\n",
    "                print(\"ERROR2 GetRefereceContentBlocksCache: Refererence Content block id {} ({}-{}) do not exist in table {} .\".format(block, bbegin, bend,referenceName + \".\" + DReferenceContentTableName))\n",
    "                    \n",
    "            \n",
    "    if (blocks_read>0):\n",
    "        return (end, contentSequence)\n",
    "    else:\n",
    "        print(\"ERROR GetRefereceContentBlocksCache: Refererence Content block id {} ({}-{}) do not exist in table {} .\".format(block, bbegin, bend,referenceName + \".\" + DReferenceContentTableName))\n",
    "        return (0, None)\n",
    "    \n",
    "\n",
    "        \n",
    "def ProcessCassandraQuery(tuple):\n",
    "\n",
    "    global reference_name_bc, hash_name_bc\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()\n",
    "            \n",
    "    res = ProcessCassandraQueryR(tuple[0], tuple[1], ses, reference_name_bc.value, hash_name_bc.value)\n",
    "\n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    return (res)\n",
    "    \n",
    "    \n",
    "def ProcessCassandraQueryR(key, despl, session, referenceName, hashName):\n",
    "    dfc(\"ProcessCassandraQueryR\", key, despl, session, referenceName, hashName)\n",
    "        \n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"Processing Query Record {} with despl {}.\".format(key, despl))\n",
    "   \n",
    "    if hashName is None:\n",
    "        querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "    else:\n",
    "        querySelect = \"SELECT * FROM \" + hashName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryR::Select: {} {}\".format(querySelect,key))\n",
    "        \n",
    "        \n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                resultSelect = session.execute(querySelect, [key] )\n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "    if resultSelect is None:\n",
    "        raise  \n",
    "\n",
    "    res = []\n",
    "    for result_row in resultSelect:\n",
    "        if (DDebug):\n",
    "            print(\"{}.{}->{}\".format(result_row.seq, result_row.block, list(map(lambda offset: offset, result_row.value))))\n",
    "        res.append(map(lambda offset: offset+int(despl), result_row.value))        \n",
    "        #res.append(map(lambda offset: int(offset)+int(despl), result_row.value))        \n",
    "    #print(res)\n",
    "    res = flattened_list = [y for x in res for y in x]           \n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"Processing Query Record result: {}.\".format(res))\n",
    "    \n",
    "    return res\n",
    "    \n",
    "      \n",
    "def ProcessMultipleCassandraQuery(tuple):\n",
    "\n",
    "    global reference_name_bc\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()\n",
    "            \n",
    "    res = ProcessMultipleCassandraQueryR(tuple, ses, reference_name_bc.value, hash_name_bc.value)\n",
    "\n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    return (res)\n",
    "    \n",
    "    \n",
    "def ProcessMultipleCassandraQueryR(key, session, referenceName, hashName):\n",
    "    dfc(\"ProcessMultipleCassandraQueryR\", key, session, referenceName, hashName)\n",
    "    \n",
    "    global DDebug\n",
    "    if (DDebug and False):\n",
    "        print(\"Processing Multilple Query Record {}.\".format(key))\n",
    "        \n",
    "    if hashName is None:\n",
    "        querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "    else:\n",
    "        querySelect = \"SELECT * FROM \" + hashName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryR::Select: {} {}\".format(querySelect,key))\n",
    "        \n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                resultSelect = session.execute(querySelect, [key] )\n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "    if resultSelect is None:\n",
    "        raise\n",
    "    \n",
    "    res = []\n",
    "    for result_row in resultSelect:\n",
    "        if (DDebug):\n",
    "            print(\"{}.{}->{}\".format(result_row.seq, result_row.block, list(map(lambda offset: offset, result_row.value))))\n",
    "        res.append(map(lambda offset: offset, result_row.value))        \n",
    "        #res.append(result_row.value)        \n",
    "        #res.append(map(lambda offset: int(offset)+int(despl), result_row.value))        \n",
    "    #print(res)\n",
    "    res = flattened_list = [y for x in res for y in x]           \n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"Processing Multilple Query Record result: {}.\".format(res))\n",
    "    \n",
    "    return ((key, res))\n",
    "\n",
    "\n",
    "\n",
    "def ProcessMultipleCassandraQueryByPartition(list):\n",
    "\n",
    "    global reference_name_bc, hash_name_bc\n",
    "    cluster = Cluster(DCassandraNodes)\n",
    "    ses = cluster.connect()\n",
    "            \n",
    "    res = ProcessMultipleCassandraQueryByPartitionR(list, ses, reference_name_bc.value, hash_name_bc.value)\n",
    "\n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    return (res)\n",
    "\n",
    "\n",
    "\n",
    "def ProcessMultipleCassandraQueryByPartitionR(list, session, referenceName, hashName):\n",
    "\n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"Processing Multilple Query Record {}.\".format(list))\n",
    "    \n",
    "    print(\"@@@@@ Processing Multilple Query By Partition size: {}.\".format(len(list)))\n",
    "           \n",
    "    if (DDebug):\n",
    "        print(list)\n",
    "        \n",
    "    if (DDebug):\n",
    "        for tuple in list:       \n",
    "            print(\"ProcessCassandraQueryByPartition: Key: {}.\".format(tuple))\n",
    "    \n",
    "    res = map(lambda tuple : ProcessMultipleCassandraQueryP(tuple, session, referenceName, hashName), list)\n",
    "    #res = flattened_list = [y for x in res for y in x]     \n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryByPartition::Result: {}.\".format(res))\n",
    "\n",
    "    return (res)\n",
    "\n",
    "\n",
    "\n",
    "def ProcessMultipleCassandraQueryP(key, session, referenceName, hashName):\n",
    "    dfc(\"ProcessCassandraQueryP\", key, session, referenceName, hashName)\n",
    "        \n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"Processing Query Record {}.\".format(key))\n",
    "   \n",
    "    if hashName is None:\n",
    "        querySelect = \"SELECT * FROM \" + referenceName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "    else:\n",
    "        querySelect = \"SELECT * FROM \" + hashName + \".\" + DReferenceHashTableName + \" WHERE seq=%s\"\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryR::Select: {} {}\".format(querySelect,key))\n",
    "        \n",
    "        \n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                resultSelect = session.execute(querySelect, [key] )\n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "    if resultSelect is None:\n",
    "        raise  \n",
    "\n",
    "    res = []\n",
    "    for result_row in resultSelect:\n",
    "        if (DDebug):\n",
    "            print(\"{}.{}->{}\".format(result_row.seq, result_row.block, list(map(lambda offset: offset, result_row.value))))\n",
    "        res.append(map(lambda offset: offset, result_row.value))   \n",
    "        #res.append(map(lambda offset: int(offset)+int(despl), result_row.value))        \n",
    "    #print(res)\n",
    "    res = flattened_list = [y for x in res for y in x]           \n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"Processing Query Record result: {}.\".format(res))\n",
    "    \n",
    "    return ((key, res))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def BalancingQuerys(record):\n",
    "    id = record[0]\n",
    "    offsets = record[1]\n",
    "    sequence = record[2]\n",
    "    queries = []\n",
    "    for chunk in chunks(offsets, DMaxOffsetsQuery):\n",
    "        queries.append((id,chunk,sequence))\n",
    "    return(queries)\n",
    "\n",
    "\n",
    "def ShowBalanceStatistics(rdd):\n",
    "    if (not DDebug):\n",
    "        return\n",
    "\n",
    "    print(\"###### Aligments number distribution among partitions: \")\n",
    "    partition = 1\n",
    "    Total = 0\n",
    "    Max = 0\n",
    "    for par in rdd.glom().collect():\n",
    "        alig = 0\n",
    "        for query in par:\n",
    "            #print(\"       ##### {} Query {} -> {} aligments ({}).\".format(partition, query[0],len(query[1]),sorted(query[1])))\n",
    "            alig = alig + len(query[1])\n",
    "        print(\"###### Partition {} queries: {} aligments: {}.\".format(partition, len(par),  alig))\n",
    "        partition = partition + 1\n",
    "        Total = Total + alig\n",
    "        if (alig>Max):\n",
    "            Max = alig\n",
    "    print(\"###### TOTAL aligments: {}.\".format(Total))\n",
    "    num_part = rdd.getNumPartitions()\n",
    "    print(\"###### Number of partitions: {}.\".format(num_part))\n",
    "    if (num_part>=1 and Total>0):    \n",
    "        ratio = float(Total)/float(num_part)\n",
    "        print(\"###### Number of partition: {}  Mean aligms/part: {}  Unbalancing: {}%.\".format(num_part, Total/num_part, round(float((Max-ratio)/ratio)*100.0,3)))\n",
    "    else:\n",
    "        print(\"###### Number of partition: {}  Mean aligms/part: {}  Unbalancing: {}%.\".format(num_part, 0, 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# Loads and returns data frame for a table including key space given\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df\n",
    "    \n",
    "\n",
    "## \n",
    "## Receives Keys+Desplazament tuples (keysdespl_rdd) and select the keys in cassandra reference table and\n",
    "## sums the corresponding query Desplazament to the reference Offset\n",
    "## \n",
    "def GetKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd):\n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference\")\n",
    "        \n",
    "    if (DDebug & False):\n",
    "        # Show Reference Table\n",
    "        GenRef = load_and_get_table_df(referenceName, DReferenceContentTableName)\n",
    "        print(\"Reference Table:\")\n",
    "        GenRef.show()\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::QueryKeys:\")\n",
    "        print(keysdespl_rdd.take(1))\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Process all elements\")\n",
    "    matching_despl_rdd = keysdespl_rdd.flatMap(lambda kv: ProcessCassandraQuery(kv))\n",
    "          \n",
    "    if (DDebug & matching_despl_rdd.isEmpty()==False):\n",
    "        print(\"GetKeysOffsetsInReference::Result: {}\".format(matching_despl_rdd.isEmpty()))\n",
    "        print(matching_despl_rdd.take(1))\n",
    "       \n",
    "    return matching_despl_rdd\n",
    "\n",
    "\n",
    "## \n",
    "## Receives Keys+Desplazament tuples (keysdespl_rdd) and select the keys in cassandra reference table and\n",
    "## sums the corresponding query Desplazament to the reference Offset\n",
    "## \n",
    "def GetMultipleKeysOffsetsInReference(sc, session, referenceName, keysdespl_rdd):\n",
    "    dfc(\"GetMultipleKeysOffsetsInReference\", sc, session, referenceName, keysdespl_rdd)\n",
    "        \n",
    "    #offsets_rdd = keysdespl_rdd.flatMap(lambda res: off[0] for off in res[1])\n",
    "    offsets_rdd = keysdespl_rdd.flatMap(lambda res: map(lambda off: off[0], res[1]))\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Print all {} offsets: {}\".format(offsets_rdd.count(), offsets_rdd.sortBy(lambda r:r[0]).take(100)))\n",
    "        \n",
    "    #print(offsets_rdd.take(100))\n",
    "    #row = Row(\"Key\") # Or some other column name\n",
    "    #sc.parallelize(offsets_rdd.collect()).map(row).toDF().write.parquet(DHdfsTmpPath+'offsets_rdd_'+YarnJobId+'.parquet')\n",
    "    \n",
    "    offsets_rdd = offsets_rdd.distinct()\n",
    "    if (DBalanceGetKeysOffsesPartitions):\n",
    "        global NumberPartitions\n",
    "        print(\"@@@@@ Repartition offsets rdd from {} to {}\".format(offsets_rdd.getNumPartitions(),NumberPartitions))\n",
    "        if (NumberPartitions>offsets_rdd.getNumPartitions()):\n",
    "            offsets_rdd = offsets_rdd.repartition(NumberPartitions)\n",
    "        else:\n",
    "            offsets_rdd = offsets_rdd.coalesce(NumberPartitions)\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Print distintct {} offsets: {}\".format(offsets_rdd.count(), offsets_rdd.sortBy(lambda r:r[0]).collect()))\n",
    "               \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReference::Process all elements\")\n",
    "    if DProcessingByPartitions:\n",
    "        matching_despl_rdd = offsets_rdd.glom().flatMap(lambda kv: ProcessMultipleCassandraQueryByPartition(kv) if len(kv) > 0 else \"\")\n",
    "    else:\n",
    "        matching_despl_rdd = offsets_rdd.map(lambda kv: ProcessMultipleCassandraQuery(kv))\n",
    "    \n",
    "    # Filter emtpy offsets\n",
    "    matching_despl_rdd =  matching_despl_rdd.filter(lambda off: len(off[1])>0)\n",
    "          \n",
    "    if (DDebug and matching_despl_rdd.isEmpty()==False):\n",
    "        print(\"GetKeysOffsetsInReference::Result: {}\".format(matching_despl_rdd.isEmpty()))\n",
    "        print(matching_despl_rdd.take(1))\n",
    "       \n",
    "    return matching_despl_rdd\n",
    "\n",
    "\n",
    "def ProcessCassandraQueryByPartition(list):\n",
    "\n",
    "    global reference_name_bc, DDebug\n",
    "    \n",
    "    for i in range(0,DCassandraRetriesNumber):\n",
    "#        while True:\n",
    "            try:\n",
    "                cluster = Cluster(DCassandraNodes)\n",
    "                ses = cluster.connect()\n",
    "            except:\n",
    "                # Print exception info.\n",
    "                print(\"@@@@@ Capatured Exception\")\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "                del exc_info\n",
    "                # Retry operation after a delay.\n",
    "                sleep(DCassandraRetryTimeout)\n",
    "                continue\n",
    "            break\n",
    "    if ses is None:\n",
    "        raise\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(list)\n",
    "        \n",
    "    if (DDebug):\n",
    "        for tuple in list:       \n",
    "            print(\"ProcessCassandraQueryByPartition: Key: {}, value: {}.\".format(tuple[0], tuple[1]))\n",
    "    \n",
    "    res = map(lambda tuple : ProcessCassandraQueryR(tuple[0], tuple[1], ses, reference_name_bc.value), list)\n",
    "    res = flattened_list = [y for x in res for y in x]     \n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"ProcessCassandraQueryByPartition::Result: {}.\".format(res))\n",
    "        \n",
    "    ses.shutdown()\n",
    "    cluster.shutdown()\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "## \n",
    "## Receives Keys+Desplazament tuples (keysdespl_rdd) and select the keys in cassandra reference table and\n",
    "## sums the corresponding query Desplazament to the reference Offset\n",
    "## The procesing is done at high-level one-partition one task, in order to reducer the cassandra connections.\n",
    "\n",
    "def GetKeysOffsetsInReferenceByPartition(sc, session, referenceName, keysdespl_rdd):\n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition\")\n",
    "        \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition::QueryKeys:\")\n",
    "        print(keysdespl_rdd.take(1))\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition::Process all elements\")\n",
    "    matching_despl_rdd = keysdespl_rdd.glom().flatMap(lambda kv: ProcessCassandraQueryByPartition(kv) if len(kv) > 0 else \"\")\n",
    "          \n",
    "    if (DDebug & matching_despl_rdd.isEmpty()==False):\n",
    "        print(\"GetKeysOffsetsInReferenceByPartition::Result: {}\".format(matching_despl_rdd.isEmpty()))\n",
    "        print(matching_despl_rdd.take(1))\n",
    "       \n",
    "    return matching_despl_rdd\n",
    "\n",
    "\n",
    "    \n",
    "def CalculateQueryOffset(sc, queryFilename, keySize):\n",
    "    if (DDebug):\n",
    "        print(\"CalculateQueryOffset\")\n",
    "\n",
    "    # Read query file (offset,line) from hdfs\n",
    "    query_rdd = sc.newAPIHadoopFile(\n",
    "        queryFilename,\n",
    "        'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "        'org.apache.hadoop.io.LongWritable',\n",
    "        'org.apache.hadoop.io.Text',\n",
    "    )\n",
    "    if (DDebug):\n",
    "        print(\"Query::Input file has {} lines and {} partitions: \".format(query_rdd.count(),query_rdd.getNumPartitions()))\n",
    "        print(\"Query::First 10 records: \"+format(query_rdd.take(1)))\n",
    "        #query_rdd.count()\n",
    "    \n",
    "    # Calculate Dataframe\n",
    "    query_df, query_sequence = CreateDataFrame(query_rdd)\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Query Data Frame: \")\n",
    "        query_df.show(10)\n",
    "    \n",
    "    # Calculate Query length\n",
    "    queryLength =len(query_sequence)\n",
    "#    if CreateWindowWithPartitions:\n",
    "#        row = query_df.rdd.reduce(lambda x, y: x if int(x[3]) > int(y[3]) else y)\n",
    "#    else:\n",
    "#        row = query_df.rdd.reduce(lambda x, y: x if int(x[2]) > int(y[2]) else y)\n",
    "#       \n",
    "#    queryLength =  int(row['offset']) + int(row['size'])\n",
    "    global query_length_bc\n",
    "    query_length_bc = sc.broadcast(queryLength)\n",
    "    \n",
    "    # Calculate keys & offsets\n",
    "    t1 = time()\n",
    "    query_keys_despl_rdd = query_df.rdd.flatMap(calculateQueryKeysDespl)\n",
    "            \n",
    "    if (False and DTiming):\n",
    "        query_keys_despl_rdd.persist()\n",
    "        print(\"Time required for calculate keys in {} seconds.\\n\".format(round(time() - t1,3)))\n",
    "        tt = time() - gt0_bc.value\n",
    "        print(\"Total time required for processing {} keys using {} partitions in {} seconds.\".format(query_keys_despl_rdd.count(), query_keys_despl_rdd.getNumPartitions(), round(tt,3)))\n",
    "        print(\"Query data size: {} MBytes.\\n\".format(round(get_size(queryFilename)/(1024.0*1024.0),3)))\n",
    "\n",
    "    return query_keys_despl_rdd, query_sequence\n",
    "\n",
    "\n",
    "\n",
    "def CalculateMultipleQueryOffset(sc, sqlContext, queryFilename, keySize):\n",
    "    dfc(\"CalculateMultipleQueryOffset\", sc, sqlContext, queryFilename, keySize)\n",
    "    \n",
    "      \n",
    "    # Read querys csv files: (query, id, length)\n",
    "    query_df = sqlContext.read.option(\"delimiter\", \"\\\\t\").csv(queryFilename, inferSchema=True)\n",
    "    if (DDebug):\n",
    "        print(\"######################### Query::Input file has {} sequences and {} partitions: \".format(query_df.count(),query_df.rdd.getNumPartitions()))\n",
    "        print(\"KeySize used 1: {}\".format(keySize))\n",
    "    # Calculate Number of Partitions           \n",
    "    convertedudf = udf(nonasciitoascii)\n",
    "    query_df = query_df.withColumn(\"_c0\", convertedudf(upper(query_df._c0)))\n",
    "    query_rdd = query_df.rdd\n",
    "    \n",
    "    global KeyMatchingThreshold\n",
    "    avquerysize = query_df.agg(mean(length(query_df._c0))).first()[0]\n",
    "    KeyMatchingThreshold = int(avquerysize * DKeyMatchingPercentageThreshold)\n",
    "    print(\"@@@@@ Average query size: {}\".format(avquerysize))\n",
    "    print(\"@@@@@ Key Matching Threshold: {}\".format(KeyMatchingThreshold))\n",
    "\n",
    "    if (DDebug):\n",
    "        print(\"######################### Query::Input file has {} sequences and {} partitions: \".format(query_df.count(),query_df.rdd.getNumPartitions()))\n",
    "        print(\"KeySize used 2: {}\".format(keySize))\n",
    "        print(\"Query::First 10 records: \"+format(query_df.take(1)))\n",
    "        query_df.printSchema()\n",
    "    \n",
    "    if (DBalanceGetKeysOffsesPartitions):\n",
    "        # Calculate Number of Partitions    \n",
    "        executors = sc._conf.get(\"spark.executor.instances\")\n",
    "        cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "        memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "        if executors is None or cores is None:\n",
    "            total_cores = 1\n",
    "        else:\n",
    "            total_cores = int(executors) * int(cores)\n",
    "        max_partitions = DMaxNumberStages * total_cores\n",
    "        nquerys = query_df.count()\n",
    "        if (nquerys>max_partitions):\n",
    "            NumberPartitions = max_partitions\n",
    "        else:\n",
    "            NumberPartitions = nquerys\n",
    "\n",
    "        # Repartition.\n",
    "        print(\"######################### Total cores: {}  Max Partition: {}  NQuerys: {}  Calc Partitions: {}  Rdd Partitions: {}\".format(total_cores, max_partitions, nquerys, NumberPartitions, query_rdd.getNumPartitions()))\n",
    "\n",
    "        if (NumberPartitions>query_rdd.getNumPartitions()):\n",
    "            query_rdd = query_rdd.repartition(NumberPartitions)\n",
    "        else:\n",
    "            query_rdd = query_rdd.coalesce(NumberPartitions)\n",
    "    \n",
    "        print(\"######################### Total cores: {}  Max Partition: {}  NQuerys: {}  Calc Partitions: {}  NEW Rdd Partitions: {}\".format(total_cores, max_partitions, nquerys, NumberPartitions, query_rdd.getNumPartitions()))\n",
    "    \n",
    "    # Calculate keys & offsets\n",
    "    t1 = time()\n",
    "    #query_keys_despl_rdd = query_rdd.map(cython_calculateMultipleQueryKeysDespl)\n",
    "    query_keys_despl_rdd = query_rdd.map(calculateMultipleQueryKeysDespl)\n",
    "    \n",
    "    # Repartition.\n",
    "    if (True or DBalanceGetKeysOffsesPartitions):    \n",
    "        global NumberPartitions\n",
    "        #query_df.persist()\n",
    "        #DBalancingQueriesByPartition = 70\n",
    "        #NumberPartitions = query_df.count()/DBalancingQueriesByPartition\n",
    "        #if NumberPartitions<1:\n",
    "        #    NumberPartitions = 1\n",
    "        if (NumberPartitions>query_rdd.getNumPartitions()):\n",
    "            query_keys_despl_rdd = query_keys_despl_rdd.repartition(NumberPartitions)\n",
    "        else:\n",
    "            query_keys_despl_rdd = query_keys_despl_rdd.coalesce(NumberPartitions)\n",
    "        print(\"@@@@@ Number of Partitions: {}\".format(NumberPartitions))\n",
    "            \n",
    "    if (DTiming and DDebug):\n",
    "        query_keys_despl_rdd.persist()\n",
    "        print(\"######################### Time required for calculate keys in {} seconds.\\n\".format(round(time() - t1,3)))\n",
    "        tt = time() - gt0_bc.value\n",
    "        print(\"######################### Total time required for processing {} keys using {} partitions in {} seconds.\".format(query_keys_despl_rdd.count(), query_keys_despl_rdd.getNumPartitions(), round(tt,3)))\n",
    "        print(\"######################### Query data size: {} MBytes.\\n\".format(round(get_size(queryFilename)/(1024.0*1024.0),3)))\n",
    "\n",
    "    return query_keys_despl_rdd, query_rdd\n",
    "\n",
    "\n",
    "\n",
    "def calculateQueryKeysDespl(record):\n",
    "    return calculateQueryKeysDesplR(record.offset, record.size, record.lines, len(record.lines))\n",
    "    \n",
    "    \n",
    "# Calculate query keys with the following tuples {key,offset}\n",
    "def calculateQueryKeysDesplR(offset, size, lines, lines_size):\n",
    "    dfc(\"calculateQueryKeysDesplR\", offset, size, lines, lines_size)\n",
    "        \n",
    "    global key_size_bc, query_length_bc\n",
    "    KeySize  = key_size_bc.value\n",
    "    QueryLength = query_length_bc.value\n",
    "    KeysDespl = []\n",
    "    \n",
    "    #tg1 = time()\n",
    "    # Calculate the first and last keys desplazaments.\n",
    "    first_key = 0\n",
    "    if (size!=lines_size):    \n",
    "        # Internal lines.\n",
    "        last_key = size\n",
    "    else:\n",
    "        # Last file line.\n",
    "        last_key = size - KeySize + 1\n",
    "        \n",
    "    offset = QueryLength-(offset+first_key+1)\n",
    "    for k in range (first_key, last_key):       \n",
    "        # Add key to python list\n",
    "        KeysDespl.append((lines[k:k+KeySize],int(offset)))\n",
    "        offset -= 1\n",
    "        \n",
    "    #print(\"\\rProcessing {} keys from offset {} in {} secs\".format(len(Keys),offset, round(time() - gt0.value,3), end =\" \"))\n",
    "    \n",
    "    return KeysDespl\n",
    "  \n",
    "    \n",
    "def cython_calculateMultipleQueryKeysDespl(record):\n",
    "    #print(\"calculateMultipleQueryKeysDespl {}.\".format(record))\n",
    "    global key_size_bc\n",
    "    \n",
    "    cyt_calculateMultipleQueryKeysDesplR = spark_cython('do_query.pyx', 'cython_calculateMultipleQueryKeysDesplR')\n",
    "        \n",
    "    #cyt_calculateMultipleQueryKeysDesplR = spark_cython('do_query', 'cython_calculateMultipleQueryKeysDesplR')\n",
    "    return (record[\"_c1\"], cyt_calculateMultipleQueryKeysDesplR(record._c0.upper().encode('ascii','ignore'), len(record._c0), key_size_bc))\n",
    "\n",
    "\n",
    "def calculateMultipleQueryKeysDespl(record):\n",
    "    #print(\"calculateMultipleQueryKeysDespl {}.\".format(record))\n",
    "    return (record[\"_c1\"], calculateMultipleQueryKeysDesplR(record._c0.upper().encode('ascii','ignore'), len(record._c0)))\n",
    "\n",
    "       \n",
    "# Calculate multiple query keys with the following tuples {key,offset}\n",
    "def calculateMultipleQueryKeysDesplR(query, size):\n",
    "    dfc(\"calculateMultipleQueryKeysDesplR\", query, size)\n",
    "        \n",
    "    global key_size_bc\n",
    "    KeySize  = key_size_bc.value\n",
    "    print(\"KeySize used 3: {}\".format(KeySize))\n",
    "    KeysDespl = []\n",
    "    \n",
    "    #tg1 = time()\n",
    "    # Calculate the first and last keys desplazaments.\n",
    "    first_key = 0\n",
    "    last_key = size - KeySize + 1\n",
    "    forbiden_key = \"N\" * KeySize\n",
    "        \n",
    "    offset = size-(first_key+1)\n",
    "    for k in range (first_key, last_key):       \n",
    "        # Add key to python list\n",
    "        if (query[k:k+KeySize]!=forbiden_key):\n",
    "            KeysDespl.append((query[k:k+KeySize],int(offset)))\n",
    "        offset -= 1\n",
    "        \n",
    "    #print(\"\\rProcessing {} keys from offset {} in {} secs\".format(len(Keys),offset, round(time() - gt0.value,3), end =\" \"))\n",
    "    \n",
    "    return KeysDespl\n",
    "    \n",
    "    \n",
    "def CreateDataFrame(reference_rdd):\n",
    "\n",
    "    # Create DataFrame  \n",
    "    reference_df = sqlContext.createDataFrame(reference_rdd,[\"file_offset\",\"line\"])\n",
    "\n",
    "    # Delete first line header if exist\n",
    "    header = reference_df.first()\n",
    "    header_size = 0\n",
    "    if (header.line[0]=='>'):\n",
    "        header_size = len(header.line)+1\n",
    "        reference_df = reference_df.filter(reference_df.file_offset!=0)\n",
    "               \n",
    "    if (Method==ECreate2LinesData):\n",
    "        df = Create2LinesDataFrame(reference_df, header_size, BlockSize)\n",
    "    elif (Method==ECreate1LineDataWithoutDependencies):\n",
    "        df = Create1LineDataFrameWithoutDependencies(reference_df, header_size, BlockSize)\n",
    "    elif (Method==ECreateBlocksData):\n",
    "        df = CreateBlocksDataFrame(reference_df, BlockSize)\n",
    "    \n",
    "    # Calculate Query String\n",
    "    query_string = ''\n",
    "    query_lines = df.select('line').collect()\n",
    "    for row in query_lines:\n",
    "        query_string = query_string + row[0]\n",
    "        \n",
    "    df.drop(df.line)\n",
    "    \n",
    "    return df, query_string.upper()\n",
    "    \n",
    "def nonasciitoascii(unicodestring):\n",
    "    return unicodestring.encode(\"ascii\",\"ignore\")\n",
    "    \n",
    "def Create2LinesDataFrame(df, header_size, blocksize):\n",
    "    \n",
    "    if (DDebug):\n",
    "        print(\"Method: ECreate2LinesData\")\n",
    "        \n",
    "    if (CreateWindowWithPartitions and blocksize>0):\n",
    "        df = df.withColumn(\"block\", (df.file_offset / blocksize).cast(\"int\"))\n",
    "        my_window = Window.partitionBy(\"block\").orderBy(\"file_offset\")\n",
    "        if (DDebug):\n",
    "            print(\"Spark shuffle partitions:\".format(sqlContext.getConf(\"spark.sql.shuffle.partitions\")))\n",
    "    else: # Without partitions\n",
    "        my_window = Window.partitionBy().orderBy(\"file_offset\")\n",
    "    \n",
    "    convertedudf = udf(nonasciitoascii)\n",
    "    df1 = df.withColumn(\"line\", convertedudf(upper(df.line)))\n",
    "    df2 = df1.withColumn(\"next_line\", F.lag(df1.line,-1).over(my_window))\n",
    "    df3 = df2.withColumn(\"size\", F.length(df2.line)) \\\n",
    "             .withColumn(\"lines\", F.when(F.isnull(df2.next_line), df2.line) \\\n",
    "                                   .otherwise(F.concat(df2.line, df2.next_line))) \\\n",
    "             .withColumn(\"offset\", df2.file_offset-header_size) \\\n",
    "             .drop(df2.next_line).drop(df2.file_offset) \n",
    "    #df3.persist()\n",
    "        \n",
    "    #print(\"Query String: \")\n",
    "    #print(\"Number of total rows: {} with {} partitions\".format(df3.count(),df3.rdd.glom().count()))\n",
    "    #print(\"DF3:\")\n",
    "    #df3.show(20) \n",
    "    #val rdd = sc.cassandraTable(\"test\", \"words\")\n",
    "    if (DDebug):\n",
    "        df3.persist()\n",
    "        print(\"Time required for read and prepare dataframe with {} rows using {} partitions in {} seconds.\\n\".format(df3.count(), df3.rdd.getNumPartitions(), round(time() - gt0_bc.value,3)))\n",
    "    #print(\"DF3:\")\n",
    "    #df3.show(10)\n",
    "    #print(\"Time required for read and prepare dataframe in {} seconds.\\n\".format(round(time() - gt0_bc.value,3)))\n",
    "\n",
    "    return df3\n",
    "\n",
    "\n",
    "def Create1LineDataFrameWithoutDependencies(df, header_size, blocksize):\n",
    "    # Create Blocks of lines to avoid dependencies with the previous line.\n",
    "    # Ref: https://stackoverflow.com/questions/49468362/combine-text-from-multiple-rows-in-pyspark\n",
    "  \n",
    "    if (DDebug):\n",
    "        print(\"Method: ECreate1LineDataWithoutDependencies\")\n",
    "    if (CreateWindowWithPartitions and blocksize>0):\n",
    "        df = df.withColumn(\"block\", (df.file_offset / blocksize).cast(\"int\"))\n",
    "        my_window = Window.partitionBy(\"block\").orderBy(\"file_offset\")\n",
    "        if (DDebug):\n",
    "            print(\"Spark shuffle partitions:\".format(sqlContext.getConf(\"spark.sql.shuffle.partitions\")))\n",
    "    else: # Without partitions\n",
    "        my_window = Window.partitionBy().orderBy(\"file_offset\")\n",
    "    \n",
    "    convertedudf = udf(nonasciitoascii)\n",
    "    df = df.withColumn(\"lines\", convertedudf(upper(df.line)))\n",
    "    df3 = df.withColumn(\"size\", F.length(df.lines)-DKeySize) \\\n",
    "             .withColumn(\"offset\", df.file_offset-header_size) \\\n",
    "             .drop(df.file_offset) \n",
    "    #df3.persist()\n",
    "    #df3.rdd.getNumPartitions() \n",
    "    #print(\"Number of total rows: {} with {} partitions\".format(df3.count(),df3.rdd.glom().count()))\n",
    "    #print(\"DF3:\")\n",
    "    #df3.show(20) \n",
    "    #val rdd = sc.cassandraTable(\"test\", \"words\")\n",
    "    #print(\"Time required for read and prepare dataframe with {} rows using {} partitions in {} seconds.\\n\".format(df3.count(), df3.rdd.getNumPartitions(), round(time() - gt0_bc.value,3)))\n",
    "    if (DTiming):\n",
    "        print(\"Time required for read and prepare dataframe in {} seconds.\\n\".format(round(time() - gt0_bc.value,3)))\n",
    "\n",
    "    return df4\n",
    "\n",
    "    \n",
    "def CreateBlocksDataFrame(dfc, blocksize):\n",
    "    if (DDebug):\n",
    "        print(\"Method: ECreateBlocksData\")\n",
    "\n",
    "    #return CreateBlocksDataFrame2(dfc)   \n",
    "    t1 = time()\n",
    "    \n",
    "    global key_size_bc\n",
    "    keySize = key_size_bc.value\n",
    "    \n",
    "    if (CreateWindowWithPartitions and blocksize>0):\n",
    "        dfc = dfc.withColumn(\"block\", (dfc.offset / blocksize).cast(\"int\"))\n",
    "        my_window = Window.partitionBy(\"block\").orderBy(\"offset\")\n",
    "        if (DDebug):\n",
    "            print(\"Spark shuffle partitions:\".format(sqlContext.getConf(\"spark.sql.shuffle.partitions\")))\n",
    "    else: # Without partitions\n",
    "        my_window = Window.partitionBy().orderBy(\"offset\")\n",
    "       \n",
    "    #dfc.show(20)\n",
    "    #my_window = Window.partitionBy().orderBy(\"offset\")     \n",
    "    #df1 = dfc.select(dfc.value.substr(0,keySize).alias(\"prefix\"))\n",
    "    df0 = dfc.withColumn(\"lines\", upper(dfc.line))\n",
    "    df1 = df0.withColumn(\"prefix\",df0.value.substr(0,keySize-1))\n",
    "    df2 = df1.withColumn(\"next_line\", F.lag(df1.prefix,-1).over(my_window))\n",
    "    df3 = df2.withColumn(\"lines\", F.when(F.isnull(df2.next_line), df2.value) \\\n",
    "                                   .otherwise(F.concat(df2.value, df2.next_line))) \n",
    "    df3 = df3.withColumn(\"size\", F.length(df3.lines)) \n",
    "    #df3.sort(col(\"offset\").asc()).show(10)\n",
    "    df3 = df3.drop(\"prefix\").drop(\"next_line\").drop(\"value\")\n",
    "    #df3.sort(col(\"offset\").asc()).show(10)\n",
    "      \n",
    "    if (DDebug):\n",
    "        print(\"Dataframe done with {} rows using {} partitions in {} seconds.\\n\".format(df3.count(), df3.rdd.getNumPartitions()))\n",
    "    if (DTiming):\n",
    "        print(\"Time required for read and prepare dataframe in {} seconds.\\n\".format(round(time() - t1,3)))\n",
    "    \n",
    "    return df3\n",
    "\n",
    "\n",
    "\n",
    "def write_statistics_single_query(statisticsFileName, queryFilename, referenceName, keySize, date, nquerys, candidates_offsets, n_aligments, partitions, totalTime, readTime, qcassTime, topmTime, extTime):\n",
    "\n",
    "    DStatisticsFileHeader = \"#Procedure ; Query Method ; Query File Name ; Reference Name ; Date  ; Number of executors ; Cores/Executor ; Memory/Executor ; Total Time (sec) ; Query Read Time (sec) ; Cassandra query Time (sec) ; Top Matching Time (sec) ; Alig. Extension Time (sec) ; Key Size ; Number of Querys ; Candidate Regions ; Number of Good Aligments ; Number of partitions ; Method ;  ; Block Size ; \"\n",
    "    \n",
    "    executors = sc._conf.get(\"spark.executor.instances\")\n",
    "    cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "    memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "    \n",
    "    global Method, BlockSize\n",
    "    if (nquerys is None):\n",
    "        nquerys = -1\n",
    "    if (candidates_offsets is None):\n",
    "        candidates_offsets = -1\n",
    "    if (n_aligments is None):\n",
    "        n_aligments = -1\n",
    "    if (partitions is None):\n",
    "        partitions = -1\n",
    "    if (Method is None):\n",
    "        Method = -1    \n",
    "    if (BlockSize is None):\n",
    "        BlockSize = -1   \n",
    "    \n",
    "    new_stats = \"\\\"SparkBlast DoQuery \\\" ; \\\"Single Query \\\" ; \\\"%s\\\" ; \\\"%s\\\" ; \\\"%s\\\" ; %s ; %s ; \\\"%s\\\" ; %f ; %f ; %f ; %f ; %f ; %d ; %d ; %d ; %d ; %d ; %d ; %d ;\" % (queryFilename, referenceName, date, executors, cores, memory, totalTime, readTime, qcassTime, topmTime, extTime, keySize, nquerys, candidates_offsets, n_aligments, partitions, Method, BlockSize)\n",
    "   \n",
    "    print(\"File {} exists? {}\".format(DHdfsHomePath+statisticsFileName, check_file(DHdfsHomePath+statisticsFileName)))\n",
    " \n",
    "    # Generate statistics in hdfs using rdd.\n",
    "    # Read statstics file\n",
    "    if (not check_file(DHdfsHomePath+statisticsFileName)):\n",
    "        new_stats_rdd = sc.parallelize([new_stats],1)\n",
    "    else:\n",
    "        new_stats_rdd = sc.parallelize([DStatisticsFileHeader, new_stats],1)\n",
    "    \n",
    "    global YarnJobId   \n",
    "    OutputFile = DHdfsTmpPath+\"tmp\"+\"_\"+YarnJobId\n",
    "    new_stats_rdd.saveAsTextFile(OutputFile)\n",
    "  \n",
    "    cmd = ['hdfs', 'dfs', '-getmerge',OutputFile+\"/part-*\", \"/tmp/prueba\"]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error getmerge\")\n",
    "    cmd = ['hdfs', 'dfs', '-appendToFile',\"/tmp/prueba\", DHdfsHomePath+statisticsFileName]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error appendToFile\")\n",
    "    cmd = ['hdfs', 'dfs', '-rm -R',OutputFile]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error remove output tmp file\")\n",
    "\n",
    "            \n",
    "    \n",
    "def write_statistics_multiple_query(statisticsFileName, queryFilename, referenceName, keySize, date, nquerys, candidates_offsets, n_aligments, partitions, totalTime, readTime, qcassTime, joinTime, topmTime, extTime):\n",
    "\n",
    "    DStatisticsFileHeader = \"#Procedure ; Query Method ; Query File Name ; Reference Name ; Date  ; Number of executors ; Cores/Executor ; Memory/Executor ; Total Time (sec) ; Query Read Time (sec) ; Cassandra query Time (sec) ;  DF Joining Time (sec) ; Top Matching Time (sec) ; Alig. Extension Time (sec) ; Key Size ; Number of Querys ; Candidate Regions ; Number of Good Aligments ; Number of partitions ; Method ;  ; Block Size ; \"\n",
    "    \n",
    "    executors = sc._conf.get(\"spark.executor.instances\")\n",
    "    cores = sc.getConf().get(\"spark.executor.cores\")\n",
    "    memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "    \n",
    "    global Method, BlockSize\n",
    "    if (nquerys is None):\n",
    "        nquerys = -1\n",
    "    if (candidates_offsets is None):\n",
    "        candidates_offsets = -1\n",
    "    if (n_aligments is None):\n",
    "        n_aligments = -1\n",
    "    if (partitions is None):\n",
    "        partitions = -1\n",
    "    if (Method is None):\n",
    "        Method = -1    \n",
    "    if (BlockSize is None):\n",
    "        BlockSize = -1            \n",
    "        \n",
    "    new_stats = \"\\\"SparkBlast DoQuery\\\" ; \\\"Multiple Query\\\" ; \\\"%s\\\" ; \\\"%s\\\" ; \\\"%s\\\" ; %s ; %s ; \\\"%s\\\" ; %f ; %f ; %f ; %f ; %f ; %f ; %d ; %d ; %d ; %d ; %d ; %d ; %d ;\" % (queryFilename, referenceName, date, executors, cores, memory, totalTime, readTime, qcassTime, joinTime, topmTime, extTime, keySize, nquerys, candidates_offsets, n_aligments, partitions, Method, BlockSize)\n",
    "   \n",
    "    print(\"File {} exists? {}\".format(DHdfsHomePath+statisticsFileName, check_file(DHdfsHomePath+statisticsFileName)))\n",
    " \n",
    "    # Generate statistics in hdfs using rdd.\n",
    "    # Read statstics file\n",
    "    if (not check_file(DHdfsHomePath+statisticsFileName)):\n",
    "        new_stats_rdd = sc.parallelize([new_stats],1)\n",
    "    else:\n",
    "        new_stats_rdd = sc.parallelize([DStatisticsFileHeader, new_stats],1)\n",
    "        \n",
    "    global YarnJobId\n",
    "    OutputFile = DHdfsTmpPath+\"tmp\"+\"_\"+YarnJobId\n",
    "    new_stats_rdd.saveAsTextFile(OutputFile)\n",
    "  \n",
    "    cmd = ['hdfs', 'dfs', '-getmerge',OutputFile+\"/part-*\", \"/tmp/prueba\"]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error getmerge\")\n",
    "    cmd = ['hdfs', 'dfs', '-appendToFile',\"/tmp/prueba\", DHdfsHomePath+statisticsFileName]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error appendToFile\")  \n",
    "    cmd = ['hdfs', 'dfs', '-rm -R',OutputFile]\n",
    "    if (run_cmd(cmd)):\n",
    "        print(\"Error remove output tmp file\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def spark_cython(module, method):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        print(\"Entered function with: {}\".format(args))\n",
    "        global cython_function_\n",
    "        try:\n",
    "            return cython_function_(*args, **kwargs)\n",
    "        except:\n",
    "            import pyximport\n",
    "            #pyximport.install()\n",
    "            pyximport.install(build_dir=DCythonLibsPath)\n",
    "            print(\"Cython compilation complete\")\n",
    "            cython_function_ = getattr(__import__(module), method)\n",
    "        print(\"Defined function: {}\".format(cython_function_))\n",
    "        return cython_function_(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def spark_cython2(*args,**kwargs):\n",
    "    global cython_function_\n",
    "    module='do_query'\n",
    "    method='cython_calculateMultipleQueryKeysDesplR'\n",
    "    try:\n",
    "          return cython_function_(*args, **kwargs)\n",
    "    except:\n",
    "        import pyximport\n",
    "        pyximport.install(build_dir=DCythonLibsPath)\n",
    "        cython_function_ = getattr(__import__(module), method)\n",
    "    return cython_function_(*args, **kwargs)\n",
    "\n",
    "    \n",
    "def dfc(functionName, *args):\n",
    "    global DDebug\n",
    "    if (DDebug):\n",
    "        print(\"%%%%% [{}]----> {}.\".format(time(),functionName))\n",
    "        #print(\"[{}]----> {} ({}).\".format(time(),functionName,list(args)))\n",
    "    \n",
    "\n",
    "    \n",
    "def run_cmd(args_list):\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE)\n",
    "    proc.communicate()\n",
    "    print(\"Return code: {}\".format(proc.returncode))\n",
    "    return proc.returncode\n",
    "   \n",
    "\n",
    "def check_file(hdfs_file_path):\n",
    "    cmd = ['hdfs', 'dfs', '-test', '-e', hdfs_file_path]\n",
    "    code = run_cmd(cmd)\n",
    "    return code\n",
    "\n",
    "  \n",
    "def remove_file(hdfs_file_path):\n",
    "    cmd = ['hdfs', 'dfs', '-rm', '-R', hdfs_file_path]\n",
    "    code = run_cmd(cmd)\n",
    "    return code\n",
    "\n",
    "\n",
    "def get_size(start_path = '.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def main(sc, sqlContext, queryFilename, referenceName, keySize=DKeySize, DoMultipleQuery=DDoMultipleQuery, HashName=None):\n",
    "    if (DDebug):\n",
    "        print(\"main\")\n",
    "        \n",
    "    global YarnJobId, cyt_calculateMultipleQueryKeysDesplR\n",
    "    try:\n",
    "        a,b,YarnJobId = sc._jsc.sc().applicationId().split('_')\n",
    "    except: \n",
    "        a,YarnJobId = sc._jsc.sc().applicationId().split('-')\n",
    "        \n",
    "    sc.addPyFile(DCythonLibsPath+'do_query.pyx')\n",
    "    \n",
    "    #import pyximport\n",
    "    #pyximport.install(build_dir=DCythonLibsPath)\n",
    "    #import do_query\n",
    "    #cyt_calculateMultipleQueryKeysDesplR = spark_cython('do_query', 'cython_calculateMultipleQueryKeysDesplR')\n",
    "    #print(\"@@@@ cyt_calculateMultipleQueryKeysDesplR result: {}\".format( cyt_calculateMultipleQueryKeysDesplR(\"12345678901234567890\",len(\"12345678901234567890\"), 11)))\n",
    "    \n",
    "    if (not DoMultipleQuery):\n",
    "        Query(sc, sqlContext, queryFilename, referenceName, keySize, HashName)\n",
    "    else:\n",
    "        MultipleQuery(sc, sqlContext, queryFilename, referenceName, keySize, HashName)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#######################\n",
    "## Sequences alignments\n",
    "#######################\n",
    "\n",
    "def DoAligment(querySequence, referenceSequence):\n",
    "    ti=time()\n",
    "    align_seq1,align_seq2,align_score,i_start = Aligment(querySequence, referenceSequence)\n",
    "    tf=time()\n",
    "    td=round(tf-ti,3)\n",
    "    #print(\"@@@@@@ {} DoAligment({}, {}); {}.\".format(td, querySequence, referenceSequence, td))   \n",
    "    return align_seq1,align_seq2,align_score, i_start\n",
    "\n",
    "def Aligment(query_seq, candidate_sequence):\n",
    "\n",
    "    if (False or DDebug):\n",
    "        print(\"Alignment\")\n",
    "        print(\"Candi Seg: {}\".format(candidate_sequence))\n",
    "        print(\"Query Seg: {}\".format(query_seq))\n",
    "    #candidate_seq_pos = finded_postion - query_seq_length + 11 - 5\n",
    "    #candidate_seq_length = query_seq_length + 11\n",
    "    #candidate_sequence = ExtractSeq(chr_index,candidate_seq_pos,candidate_seq_length)\n",
    "    \n",
    "    #query_seq = querySequence\n",
    "    #candidate_sequence = referenceSequence\n",
    "    #query_seq_length = len(query_seq)\n",
    "    #candidate_seq_length = query_seq_length + KeySize\n",
    "    #candidate_seq_pos  = finded_position - query_seq_length + keySize - 5  \n",
    "    \n",
    "\n",
    "    if (DAligmentExtension):\n",
    "        i_start_indexs = []\n",
    "        for i_start in range(int(math.floor(DAligmentExtensionLength/2.0))+1):\n",
    "            _,_,score = SMalignment(candidate_sequence[i_start:],query_seq)\n",
    "            i_start_indexs.append(score)\n",
    "        #i_start = np.array(i_start_indexs).argmax()\n",
    "        i_start = i_start_indexs.index(max(i_start_indexs))\n",
    "\n",
    "        i_end_indexs = []\n",
    "        for i_end in range(1,int(math.ceil(DAligmentExtensionLength/2.0))+2):\n",
    "            _,_,score = SMalignment(candidate_sequence[:-i_end],query_seq)\n",
    "            i_end_indexs.append(score)\n",
    "        #i_end = np.array(i_end_indexs).argmax()+1\n",
    "        i_end = i_end_indexs.index(max(i_end_indexs))+1\n",
    "    else:\n",
    "        i_start=0\n",
    "        i_end=1\n",
    "            \n",
    "    candidate_sequence = candidate_sequence[i_start:-i_end]\n",
    "    if (DDebug):\n",
    "        print(\"Best aligment {}-{}: {}\".format(i_start, i_end, candidate_sequence))\n",
    "    align_seq1,align_seq2,align_score = SMalignment(candidate_sequence,query_seq)\n",
    "    \n",
    "    return align_seq1, align_seq2, align_score, i_start\n",
    "\n",
    "\n",
    "# compare single base\n",
    "def SingleBaseCompare(seq1,seq2,i,j):\n",
    "    if seq1[i] == seq2[j]:\n",
    "        return 2\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#\n",
    "# Smith-Waterman Alignment\n",
    "#\n",
    "def SMalignment(seq1, seq2):\n",
    "    if (False or DDebug):\n",
    "        print(\"SMalignment\")\n",
    "        print(\"Seg1: {}\".format(seq1))\n",
    "        print(\"Seg2: {}\".format(seq2))\n",
    "    \n",
    "    #ti=time()\n",
    "    m = len(seq1)\n",
    "    n = len(seq2)\n",
    "    g = -3\n",
    "    matrix = []\n",
    "    for i in range(0, m):\n",
    "        tmp = []\n",
    "        for j in range(0, n):\n",
    "            tmp.append(0)\n",
    "        matrix.append(tmp)\n",
    "    for sii in range(0, m):\n",
    "        matrix[sii][0] = sii*g\n",
    "    for sjj in range(0, n):\n",
    "        matrix[0][sjj] = sjj*g\n",
    "    for siii in range(1, m):\n",
    "        for sjjj in range(1, n):\n",
    "            matrix[siii][sjjj] = max(matrix[siii-1][sjjj] + g, matrix[siii - 1][sjjj - 1] + SingleBaseCompare(seq1,seq2,siii, sjjj), matrix[siii][sjjj-1] + g)\n",
    "    sequ1 = [seq1[m-1]]\n",
    "    sequ2 = [seq2[n-1]]\n",
    "    while m > 1 and n > 1:\n",
    "        if max(matrix[m-1][n-2], matrix[m-2][n-2], matrix[m-2][n-1]) == matrix[m-2][n-2]:\n",
    "            m -= 1\n",
    "            n -= 1\n",
    "            sequ1.append(seq1[m-1])\n",
    "            sequ2.append(seq2[n-1])\n",
    "        elif max(matrix[m-1][n-2], matrix[m-2][n-2], matrix[m-2][n-1]) == matrix[m-1][n-2]:\n",
    "            n -= 1\n",
    "            sequ1.append('-')\n",
    "            sequ2.append(seq2[n-1])\n",
    "        else:\n",
    "            m -= 1\n",
    "            sequ1.append(seq1[m-1])\n",
    "            sequ2.append('-')\n",
    "    sequ1.reverse()\n",
    "    sequ2.reverse()\n",
    "    align_seq1 = ''.join(sequ1)\n",
    "    align_seq2 = ''.join(sequ2)\n",
    "    align_score = 0.\n",
    "    for k in range(0, len(align_seq1)):\n",
    "        if align_seq1[k] == align_seq2[k]:\n",
    "            align_score += 1\n",
    "    align_score = float(align_score)/len(align_seq1)\n",
    "    #tf=time()\n",
    "    #td=round(tf-ti,3)\n",
    "    #if (td>1):\n",
    "    #    print(\"@@@@@@ {} SMalignment({}, {}).\".format(td, seq1, seq2))\n",
    "    return align_seq1, align_seq2, align_score\n",
    "\n",
    "\n",
    "# Display BlAST result\n",
    "def Display(seque1, seque2):\n",
    "    le = 40\n",
    "    while len(seque1)-le >= 0:\n",
    "        print('sequence1: ',end='')\n",
    "        for a in list(seque1)[le-40:le]:\n",
    "            print(a,end='')\n",
    "        print(\"\\n\")\n",
    "        print('           ',end='')\n",
    "        for k in range(le-40, le):\n",
    "            if seque1[k] == seque2[k]:\n",
    "                print('|',end='')\n",
    "            else:\n",
    "                print(' ',end='')\n",
    "        print(\"\\n\")\n",
    "        print('sequence2: ',end='')\n",
    "        for b in list(seque2)[le-40:le]:\n",
    "            print(b,end='')\n",
    "        print(\"\\n\")\n",
    "        le += 40\n",
    "    if len(seque1) > le-40:\n",
    "        print('sequence1: ',end='')\n",
    "        for a in list(seque1)[le-40:len(seque1)]:\n",
    "            print(a,end='')\n",
    "        print(\"\\n\")\n",
    "        print('           ',end='')\n",
    "        for k in range(le-40, len(seque1)):\n",
    "            if seque1[k] == seque2[k]:\n",
    "                print('|',end='')\n",
    "            else:\n",
    "                print(' ',end='')\n",
    "        print(\"\\n\")\n",
    "        print('sequence2: ',end='')\n",
    "        for b in list(seque2)[le-40:len(seque2)]:\n",
    "            print(b,end='')\n",
    "        print(\"\\n\")\n",
    "   \n",
    "\n",
    "## Testing \n",
    "\n",
    "if (DDoTesting):    \n",
    "       \n",
    "    # Test 1: Calculate Query's keys & desplazaments (with header line)\n",
    "    print(\"Test 1a: Calculate Query's keys & desplazaments (with header line)\")\n",
    "    queryFilename = '../Datasets/References/Query3.txt'\n",
    "    referenceName = \"example2_r100\"\n",
    "    Method = ECreate2LinesData\n",
    "    #main(sc, sqlContext, queryFilename, referenceName)  \n",
    "    \n",
    "    \n",
    "    # Test 2: Calculate Query's keys & desplazaments (with header line)\n",
    "    print(\"Test 2: Calculate Query's keys & desplazaments (with header line)\")\n",
    "    queryFilename = '../Datasets/References/Query_GRCh38.txt'\n",
    "    referenceName = \"grch38_1m\"\n",
    "    Method = ECreate2LinesData\n",
    "    #main(sc, sqlContext, queryFilename, referenceName.lower())  \n",
    "    \n",
    "\n",
    "    # Test 3: Calculate Multiple Querys \n",
    "    print(\"Test 1a: Calculate Multipe Querys\")\n",
    "    queryFilename = '../Datasets/References/MulQuery1.txt'\n",
    "    referenceName = \"example2_r100\"\n",
    "    #MultipleQuery(sc, sqlContext, queryFilename, referenceName)  \n",
    "    \n",
    "    # Test 4: Calculate Multiple Querys \n",
    "    print(\"Test 3: Calculate Multipe Querys\")\n",
    "    queryFilename = 'hdfs://babel.udl.cat//user/nando/Datasets/Sequences/GRCh38_latest_genomic_200-400.csv.gz'\n",
    "    referenceName = \"grch38F\"\n",
    "    DDebug = True\n",
    "    #MultipleQuery(sc, sqlContext, queryFilename, referenceName)  \n",
    "    \n",
    "    # Test 4N: Calculate Multiple Querys \n",
    "    print(\"Test 3: Calculate Multipe Querys\")\n",
    "    queryFilename = 'hdfs://babel.udl.cat//user/nando/Datasets/Sequences/NM_114184.csv.gz'\n",
    "    referenceName = \"grch38F_K21\"\n",
    "    DDebug = True\n",
    "    MultipleQuery(sc, sqlContext, queryFilename, referenceName)  \n",
    "    \n",
    "    error\n",
    "\n",
    "## End Testing \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"__Main__\")\n",
    "    \n",
    "    ## Process parameters.\n",
    "    ## SparkBlast_DoQuery <Query_Files> <ReferenceName> [Key_size=11]\n",
    "    if (len(sys.argv)<2):\n",
    "        print(\"Error parametes. Usage: DoQuery [--MQuery] <Query_Files> <ReferenceName> [Key_size=11] [StadisticsFile] [HashName] .\\n\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    KeySize = DKeySize\n",
    "    Method = DDefaultMethod\n",
    "  \n",
    "    if (len(sys.argv)>1 and sys.argv[1].upper()==\"--MQUERY\"):\n",
    "        DoMultipleQuery = True\n",
    "        args=2\n",
    "    else: \n",
    "        DoMultipleQuery = False\n",
    "        args=1\n",
    "       \n",
    "    QueryFilename = sys.argv[args]\n",
    "    ReferenceName = sys.argv[args+1].lower()\n",
    "    HashName=None\n",
    "    global NumberPartitions\n",
    "    NumberPartitions = DNumberPartitions\n",
    "    if (len(sys.argv)>(args+2)):\n",
    "        KeySize = int(sys.argv[args+2])\n",
    "    if (len(sys.argv)>args+3):\n",
    "        StatisticsFileName = sys.argv[args+3]\n",
    "    if (len(sys.argv)>(args+4)):\n",
    "        NumberPartitions = int(sys.argv[args+4])\n",
    "    if (len(sys.argv)>(args+5)):\n",
    "        HashName = sys.argv[args+5].lower()\n",
    "    \n",
    "\n",
    "    ## Configure Spark\n",
    "    conf = SparkConf().setAppName(APP_NAME+ReferenceName)\n",
    "    conf.set(\"spark.sql.shuffle.partitions\", NumberPartitions)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "    sqlContext = SQLContext(sc)\n",
    "    random.seed()\n",
    "    \n",
    "    t0 = time()\n",
    "    gt0_bc = sc.broadcast(t0)\n",
    "        \n",
    "    # Execute Main functionality\n",
    "    print(\"{}({}, {}, {}, {}).\".format(sys.argv[0], MultipleQuery, QueryFilename, ReferenceName, KeySize, HashName))\n",
    "    main(sc, sqlContext, QueryFilename, ReferenceName, KeySize, DoMultipleQuery, HashName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
